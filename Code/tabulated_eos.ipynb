{"cells":[{"cell_type":"code","execution_count":1,"id":"0bb9f224","metadata":{"id":"0bb9f224","executionInfo":{"status":"ok","timestamp":1681398656896,"user_tz":-120,"elapsed":8764,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["# Standard libraries\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.rcParams['figure.dpi'] = 300\n","import random\n","import csv\n","import pandas as pd\n","import h5py\n","# Scikit learn libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","# PyTorch libraries\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import ToTensor, Normalize \n","# Get dirs\n","import os\n","cwd = os.getcwd()# \"Code\" folder\n","master_dir = os.path.abspath(os.path.join(cwd, \"..\"))"]},{"cell_type":"markdown","id":"4c465f7c","metadata":{"id":"4c465f7c"},"source":["When using __Google Colab__, run the following cell"]},{"cell_type":"code","execution_count":2,"id":"bbeb496d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbeb496d","executionInfo":{"status":"ok","timestamp":1681398681220,"user_tz":-120,"elapsed":19202,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"30fa916e-e8c0-4fbe-de12-cd51790fc6a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/KUL/MAI thesis/Code\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","code_dir = \"/content/drive/MyDrive/KUL/MAI thesis/Code\"\n","master_dir = os.path.join(code_dir, \"..\")\n","os.chdir(code_dir)\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":3,"id":"36bbffc0","metadata":{"id":"36bbffc0","executionInfo":{"status":"ok","timestamp":1681398682653,"user_tz":-120,"elapsed":1462,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["# Load own scripts:\n","%load_ext autoreload\n","%autoreload 2\n","import physics\n","import data\n","import nnc2p"]},{"cell_type":"markdown","id":"c3393c80","metadata":{"id":"c3393c80"},"source":["Point towards the folder where we store the eos tables (__Note:__ they are not in the Github as these are very large files)"]},{"cell_type":"code","execution_count":4,"id":"efbc8d2b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efbc8d2b","executionInfo":{"status":"ok","timestamp":1681398682653,"user_tz":-120,"elapsed":30,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"49af00ba-c706-4dd9-fd2c-ee828683afe1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Going to look for EOS tables at /content/drive/MyDrive/KUL/MAI thesis/Code/../Data\n"]}],"source":["# eos_tables_dir = os.path.join(\"D:/Coding/Datasets/eos_tables\")  # offline\n","eos_tables_dir = os.path.join(master_dir, \"Data\")  # in Google Colab\n","print(f\"Going to look for EOS tables at {eos_tables_dir}\")"]},{"cell_type":"markdown","source":["For the training, check if GPU is available:"],"metadata":{"id":"vqCsP2sKAeCm"},"id":"vqCsP2sKAeCm"},{"cell_type":"code","source":["if torch.cuda.is_available(): \n"," DEVICE = \"cuda:0\" \n"," torch.set_default_device('cuda')\n","else: \n"," DEVICE = \"cpu\" \n","print(f\"Device for training: {DEVICE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8YoVE15AhRC","executionInfo":{"status":"ok","timestamp":1681398682654,"user_tz":-120,"elapsed":29,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"4f6e373c-7e27-4461-f5cf-51b91d99d440"},"id":"P8YoVE15AhRC","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Device for training: cpu\n"]}]},{"cell_type":"markdown","id":"02fb3c8c","metadata":{"id":"02fb3c8c"},"source":["# Introduction"]},{"cell_type":"markdown","id":"96ab265e","metadata":{"id":"96ab265e"},"source":["Here, we try to find a way to generalize the NN approach from the first semester to the situation of tabular EOS. More work coming soon!"]},{"cell_type":"markdown","id":"cf08ad4a","metadata":{"id":"cf08ad4a"},"source":["# Exploring EOS tables"]},{"cell_type":"code","execution_count":6,"id":"4d5e6fdb","metadata":{"id":"4d5e6fdb","executionInfo":{"status":"ok","timestamp":1681398683009,"user_tz":-120,"elapsed":380,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["# Put the downloaded EOS tables here\n","# first_table_filename       = \"LS180_234r_136t_50y_analmu_20091212_SVNr26.h5\"\n","# second_table_filename = \"GShen_NL3EOS_rho280_temp180_ye52_version_1.1_20120817.h5\"\n","third_table_filename      = \"SLy4_0000_rho391_temp163_ye66.h5\"\n","# Then specify which we are going to use here\n","eos_table_filename = third_table_filename"]},{"cell_type":"markdown","id":"78d8f465","metadata":{"id":"78d8f465"},"source":["Read in the SLy4 EOS table using our py script:"]},{"cell_type":"code","execution_count":7,"id":"913d9582","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"913d9582","executionInfo":{"status":"ok","timestamp":1681398683667,"user_tz":-120,"elapsed":660,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"677bba7a-6c4a-4190-a9fa-8f0bbca82bfd"},"outputs":[{"output_type":"stream","name":"stdout","text":["This EOS table has dimensions 66 x 163 x 391\n"]}],"source":["eos_table = physics.read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n","dim_ye, dim_temp, dim_rho = eos_table[\"pointsye\"][()][0], eos_table[\"pointstemp\"][()][0], eos_table[\"pointsrho\"][()][0]\n","print(f\"This EOS table has dimensions {dim_ye} x {dim_temp} x {dim_rho}\")"]},{"cell_type":"code","execution_count":8,"id":"d74b0643","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d74b0643","executionInfo":{"status":"ok","timestamp":1681398686859,"user_tz":-120,"elapsed":1493,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"e6c43f6c-41c0-4031-f547-b37a1c47d150"},"outputs":[{"output_type":"stream","name":"stdout","text":["For (3.0239960056064277, -3.0, 0.005) we have (log eps, log p): (19.2791052025363, 17.99956975587081).\n"]}],"source":["# Small test to see the output of the EOS table\n","test_ye = eos_table[\"ye\"][()][0]\n","test_temp = eos_table[\"logtemp\"][()][0]\n","test_rho = eos_table[\"logrho\"][()][0]\n","test_press, test_eps = eos_table[\"logpress\"][()][0, 0, 0], eos_table[\"logenergy\"][()][0, 0, 0]\n","print(f\"For ({test_rho}, {test_temp}, {test_ye}) we have (log eps, log p): ({test_eps}, {test_press}).\")"]},{"cell_type":"markdown","id":"f0167f31","metadata":{"id":"f0167f31"},"source":["See what is inside this EOS table"]},{"cell_type":"code","execution_count":9,"id":"316b6963","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"316b6963","executionInfo":{"status":"ok","timestamp":1681398689771,"user_tz":-120,"elapsed":517,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"77b6ecac-15ea-461d-f76a-b641a290fd63"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Abar', 'Albar', 'MERGE-space.in', 'MERGE-src.tar.gz', 'MERGE-tables.in', 'MERGE-transition.in', 'SNA-skyrme.in', 'SNA-space.in', 'SNA-src.tar.gz', 'Xa', 'Xh', 'Xl', 'Xn', 'Xp', 'Zbar', 'Zlbar', 'cs2', 'dedt', 'dpderho', 'dpdrhoe', 'energy_shift', 'entropy', 'gamma', 'have_rel_cs2', 'logenergy', 'logpress', 'logrho', 'logtemp', 'meffn', 'meffp', 'mu_e', 'mu_n', 'mu_p', 'muhat', 'munu', 'pointsrho', 'pointstemp', 'pointsye', 'r', 'u', 'ye']\n","41\n"]}],"source":["# Iterate over keys and save them to list for simplified viewing\n","keys = []\n","for key in eos_table:\n","    keys.append(key)\n","print(keys)\n","print(len(keys))"]},{"cell_type":"markdown","id":"cc15e58b","metadata":{"id":"cc15e58b"},"source":["# Generating training data by sampling from EOS table"]},{"cell_type":"markdown","id":"bc8fc6d3","metadata":{"id":"bc8fc6d3"},"source":["To generate new data"]},{"cell_type":"code","execution_count":10,"id":"511ae7db","metadata":{"id":"511ae7db","executionInfo":{"status":"ok","timestamp":1681398695231,"user_tz":-120,"elapsed":7,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["# dat = physics.generate_tabular_data(eos_table, number_of_points = 100000, save_name = \"SLy4_training_data\")\n","# dat = physics.generate_tabular_data(eos_table, number_of_points = 20000, save_name = \"SLy4_test_data\")"]},{"cell_type":"markdown","id":"a0a121af","metadata":{"id":"a0a121af"},"source":["Load data"]},{"cell_type":"code","execution_count":11,"id":"f929937d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"f929937d","executionInfo":{"status":"ok","timestamp":1681398695783,"user_tz":-120,"elapsed":557,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"304adfcc-4c0a-40fa-e2eb-452997407dc9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["             rho     logeps         v   logtemp     ye   logpress     logcs2  \\\n","0       4.523996  19.616338  0.691345 -0.633333  0.175  23.395574  43.696991   \n","1      12.490663  19.528674  0.481485  0.733333  0.285  31.366264  43.774950   \n","2      13.623996  19.448926  0.053539 -0.233333  0.175  32.269661  43.182871   \n","3       9.890663  19.205428  0.590829 -1.266667  0.155  27.232815  40.255750   \n","4       8.657329  19.074603  0.129825 -2.633333  0.335  25.966272  40.166319   \n","...          ...        ...       ...       ...    ...        ...        ...   \n","99995   3.057329  19.089191  0.604101 -1.800000  0.605  19.201463  37.575094   \n","99996   6.590663  20.798573  0.020398  0.200000  0.305  26.896812  46.403492   \n","99997   7.790663  19.106546  0.125179 -1.100000  0.605  25.172194  40.353432   \n","99998   9.623996  19.120506  0.593556 -1.566667  0.295  27.193780  40.754153   \n","99999   3.790663  20.324174  0.235517 -0.600000  0.275  23.540747  45.513301   \n","\n","               D             S           tau  \n","0       6.261371  3.295258e+23  2.280030e+23  \n","1      14.251357  1.456761e+31  7.014085e+30  \n","2      13.643564  9.990231e+30  5.348641e+29  \n","3      12.259173  1.551489e+27  9.166645e+26  \n","4       8.731223  1.221837e+25  1.586354e+24  \n","...          ...           ...           ...  \n","99995   3.836490  5.084013e+19  6.825593e+19  \n","99996   6.592034  1.609076e+25  3.323597e+23  \n","99997   7.852428  1.890548e+24  2.367563e+23  \n","99998  11.958352  1.431770e+27  8.498353e+26  \n","99999   3.900379  8.680609e+22  2.124390e+22  \n","\n","[100000 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-6c2edeb1-bd38-4e5e-9bef-b0f64ee9c502\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rho</th>\n","      <th>logeps</th>\n","      <th>v</th>\n","      <th>logtemp</th>\n","      <th>ye</th>\n","      <th>logpress</th>\n","      <th>logcs2</th>\n","      <th>D</th>\n","      <th>S</th>\n","      <th>tau</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.523996</td>\n","      <td>19.616338</td>\n","      <td>0.691345</td>\n","      <td>-0.633333</td>\n","      <td>0.175</td>\n","      <td>23.395574</td>\n","      <td>43.696991</td>\n","      <td>6.261371</td>\n","      <td>3.295258e+23</td>\n","      <td>2.280030e+23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12.490663</td>\n","      <td>19.528674</td>\n","      <td>0.481485</td>\n","      <td>0.733333</td>\n","      <td>0.285</td>\n","      <td>31.366264</td>\n","      <td>43.774950</td>\n","      <td>14.251357</td>\n","      <td>1.456761e+31</td>\n","      <td>7.014085e+30</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13.623996</td>\n","      <td>19.448926</td>\n","      <td>0.053539</td>\n","      <td>-0.233333</td>\n","      <td>0.175</td>\n","      <td>32.269661</td>\n","      <td>43.182871</td>\n","      <td>13.643564</td>\n","      <td>9.990231e+30</td>\n","      <td>5.348641e+29</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9.890663</td>\n","      <td>19.205428</td>\n","      <td>0.590829</td>\n","      <td>-1.266667</td>\n","      <td>0.155</td>\n","      <td>27.232815</td>\n","      <td>40.255750</td>\n","      <td>12.259173</td>\n","      <td>1.551489e+27</td>\n","      <td>9.166645e+26</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8.657329</td>\n","      <td>19.074603</td>\n","      <td>0.129825</td>\n","      <td>-2.633333</td>\n","      <td>0.335</td>\n","      <td>25.966272</td>\n","      <td>40.166319</td>\n","      <td>8.731223</td>\n","      <td>1.221837e+25</td>\n","      <td>1.586354e+24</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>99995</th>\n","      <td>3.057329</td>\n","      <td>19.089191</td>\n","      <td>0.604101</td>\n","      <td>-1.800000</td>\n","      <td>0.605</td>\n","      <td>19.201463</td>\n","      <td>37.575094</td>\n","      <td>3.836490</td>\n","      <td>5.084013e+19</td>\n","      <td>6.825593e+19</td>\n","    </tr>\n","    <tr>\n","      <th>99996</th>\n","      <td>6.590663</td>\n","      <td>20.798573</td>\n","      <td>0.020398</td>\n","      <td>0.200000</td>\n","      <td>0.305</td>\n","      <td>26.896812</td>\n","      <td>46.403492</td>\n","      <td>6.592034</td>\n","      <td>1.609076e+25</td>\n","      <td>3.323597e+23</td>\n","    </tr>\n","    <tr>\n","      <th>99997</th>\n","      <td>7.790663</td>\n","      <td>19.106546</td>\n","      <td>0.125179</td>\n","      <td>-1.100000</td>\n","      <td>0.605</td>\n","      <td>25.172194</td>\n","      <td>40.353432</td>\n","      <td>7.852428</td>\n","      <td>1.890548e+24</td>\n","      <td>2.367563e+23</td>\n","    </tr>\n","    <tr>\n","      <th>99998</th>\n","      <td>9.623996</td>\n","      <td>19.120506</td>\n","      <td>0.593556</td>\n","      <td>-1.566667</td>\n","      <td>0.295</td>\n","      <td>27.193780</td>\n","      <td>40.754153</td>\n","      <td>11.958352</td>\n","      <td>1.431770e+27</td>\n","      <td>8.498353e+26</td>\n","    </tr>\n","    <tr>\n","      <th>99999</th>\n","      <td>3.790663</td>\n","      <td>20.324174</td>\n","      <td>0.235517</td>\n","      <td>-0.600000</td>\n","      <td>0.275</td>\n","      <td>23.540747</td>\n","      <td>45.513301</td>\n","      <td>3.900379</td>\n","      <td>8.680609e+22</td>\n","      <td>2.124390e+22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100000 rows × 10 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c2edeb1-bd38-4e5e-9bef-b0f64ee9c502')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6c2edeb1-bd38-4e5e-9bef-b0f64ee9c502 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6c2edeb1-bd38-4e5e-9bef-b0f64ee9c502');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n","df"]},{"cell_type":"markdown","id":"5f2fce3d","metadata":{"id":"5f2fce3d"},"source":["The network architecture we will use:"]},{"cell_type":"code","execution_count":12,"id":"270fd513","metadata":{"id":"270fd513","executionInfo":{"status":"ok","timestamp":1681398697956,"user_tz":-120,"elapsed":215,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["class Net(nn.Module):\n","    \"\"\"\n","    Implements a simple feedforward neural network.\n","    \"\"\"\n","    def __init__(self, nb_of_inputs: int = 3, nb_of_outputs: int = 1, h: list = [600, 200], reg: bool = False, activation_function = torch.nn.Sigmoid) -> None:\n","        \"\"\"\n","        Initialize the neural network class.\n","        \"\"\"\n","        # Call the super constructor first\n","        super(Net, self).__init__()\n","        \n","        # For convenience, save the sizes of the hidden layers as fields as well\n","        self.h = h\n","        # Add visible layers as well: input is 3D and output is 1D\n","        self.h_augmented = [nb_of_inputs] + h + [nb_of_outputs]\n","\n","        # Add field to specify whether or not we do regularization\n","        self.regularization = reg\n","\n","        # Define the layers:\n","        for i in range(len(self.h_augmented)-1):\n","            if i == len(self.h_augmented)-2:\n","                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1], bias=False))\n","            else:\n","                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1]))\n","                setattr(self, f\"activation{i+1}\", activation_function())\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Computes a forward step given the input x.\n","        :param x: Input for the neural network.\n","        :return: x: Output neural network\n","        \"\"\"\n","\n","        for i, module in enumerate(self.modules()):\n","            # The first module is the whole NNC2P object, continue\n","            if i == 0:\n","                continue\n","            x = module(x)\n","\n","        return x"]},{"cell_type":"markdown","id":"8fd2f127","metadata":{"id":"8fd2f127"},"source":["# First goal: NNEOS"]},{"cell_type":"markdown","id":"e5850152","metadata":{"id":"e5850152"},"source":["__NNEOS__: try to replicate the EOS table (at least the core variables we are interested in) using the \"input\" variables rho, temp, ye."]},{"cell_type":"markdown","id":"12089f53","metadata":{"id":"12089f53"},"source":["For this, choose the output variables and convert the EOS table to the appropriate format, which has rows of examples"]},{"cell_type":"code","execution_count":13,"id":"7a92994d","metadata":{"id":"7a92994d","executionInfo":{"status":"ok","timestamp":1681398700618,"user_tz":-120,"elapsed":4,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["# Get the filename of converted training data\n","filename = os.path.join(eos_tables_dir, \"train_eos_table.h5\")"]},{"cell_type":"code","execution_count":14,"id":"051199cf","metadata":{"id":"051199cf","executionInfo":{"status":"ok","timestamp":1681398701905,"user_tz":-120,"elapsed":3,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["# # Create new dataset (if desired)\n","## Specify output vars as \"var_names\" argument in this function - see physics\n","# physics.convert_eos_table(eos_table, save_name=filename)"]},{"cell_type":"code","execution_count":15,"id":"ccbc983b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccbc983b","executionInfo":{"status":"ok","timestamp":1681398704133,"user_tz":-120,"elapsed":2035,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"a345b306-1aba-427e-874c-daa97b43cdca"},"outputs":[{"output_type":"stream","name":"stdout","text":["The output variables are [b'logenergy' b'logpress']. Number of examples: 4206378\n"]}],"source":["# Load the data\n","train_eos_table = h5py.File(filename, 'r')\n","features  = train_eos_table[\"features\"][:]\n","labels    = train_eos_table[\"labels\"][:]\n","var_names = train_eos_table[\"var_names\"][:]\n","print(f\"The output variables are {var_names}. Number of examples: {len(features)}\")"]},{"cell_type":"code","execution_count":16,"id":"0d8a4d14","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d8a4d14","executionInfo":{"status":"ok","timestamp":1681398706075,"user_tz":-120,"elapsed":36,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"0f2e12a5-cd17-4dcb-ac27-f09ecee5e38a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 3.02399601 -3.          0.005     ]\n","[19.2791052  17.99956976]\n"]}],"source":["# test - how does output look like?\n","print(features[0])\n","print(labels[0])"]},{"cell_type":"markdown","id":"ec6d6047","metadata":{"id":"ec6d6047"},"source":["Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."]},{"cell_type":"code","execution_count":17,"id":"ec910284","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ec910284","executionInfo":{"status":"ok","timestamp":1681398709169,"user_tz":-120,"elapsed":1126,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"4f15e095-7252-40dc-c90d-2bafae1c6cb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["3154783\n","315478\n"]}],"source":["# For normalization, use sklearn's StandardScaler\n","scaler = StandardScaler()\n","# Do train test split here\n","train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n","# \"Cutoff\": only use certain portion of the data for training and testing, to speed up training when tuning architecture \n","cutoff = 0.1\n","print(len(train_features))\n","end = int(cutoff*len(train_features))\n","train_features = train_features[:end]\n","train_labels = train_labels[:end]\n","end = int(cutoff*len(test_features))\n","test_features = test_features[:end]\n","test_labels = test_labels[:end]\n","print(len(train_features))\n","# Convert to PyTorch Datasets as we defined them\n","train_dataset = data.HDF5Dataset(train_features, train_labels, normalization_function = scaler.fit_transform) \n","test_dataset  = data.HDF5Dataset(test_features, test_labels, normalization_function = scaler.transform)\n","# Then create dataloaders, with batch size 32, from datasets\n","train_dataloader = DataLoader(train_dataset, batch_size = 32)\n","test_dataloader  = DataLoader(test_dataset, batch_size = 32)"]},{"cell_type":"code","source":["train_dataset.features.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykhi1Vr6FyWY","executionInfo":{"status":"ok","timestamp":1681398711840,"user_tz":-120,"elapsed":989,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"91e450b3-e399-4824-aa8a-6867a7112e3b"},"id":"ykhi1Vr6FyWY","execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","id":"f12e532a","metadata":{"id":"f12e532a"},"source":["Create a new instance of the Net:"]},{"cell_type":"code","execution_count":19,"id":"be7a388c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"be7a388c","executionInfo":{"status":"ok","timestamp":1681398714800,"user_tz":-120,"elapsed":31,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"c92948dd-975b-4e2f-81a0-ee0661b400ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}],"source":["model = Net(nb_of_inputs=3, nb_of_outputs=2, h=[100, 100]).double()\n","print(next(model.parameters()).is_cuda )"]},{"cell_type":"code","execution_count":20,"id":"5a0e1f63","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a0e1f63","executionInfo":{"status":"ok","timestamp":1681398716642,"user_tz":-120,"elapsed":31,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"474f33c1-c8a7-4433-9f4d-9738b0a08141"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10700"]},"metadata":{},"execution_count":20}],"source":["nnc2p.count_parameters(model)"]},{"cell_type":"markdown","id":"1e1e3745","metadata":{"id":"1e1e3745"},"source":["Create a trainer object from it:"]},{"cell_type":"code","execution_count":21,"id":"df92e917","metadata":{"id":"df92e917","executionInfo":{"status":"ok","timestamp":1681398719638,"user_tz":-120,"elapsed":705,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}}},"outputs":[],"source":["trainer = nnc2p.Trainer(model, 1e-2, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"]},{"cell_type":"code","execution_count":22,"id":"53ce8669","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53ce8669","executionInfo":{"status":"ok","timestamp":1681398892786,"user_tz":-120,"elapsed":171807,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"711c53ca-cf46-43e3-8d63-6625ebdf0880"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training the model for 10 epochs.\n","\n"," Epoch 0 \n"," --------------\n","Train loss: 4.10E-03\n","Test  loss: 4.12E-03\n","\n"," Epoch 1 \n"," --------------\n","Train loss: 1.69E-03\n","Test  loss: 1.70E-03\n","\n"," Epoch 2 \n"," --------------\n","Train loss: 7.63E-03\n","Test  loss: 7.62E-03\n","\n"," Epoch 3 \n"," --------------\n","Train loss: 2.06E-03\n","Test  loss: 2.05E-03\n","\n"," Epoch 4 \n"," --------------\n","Train loss: 3.10E-03\n","Test  loss: 3.10E-03\n","\n"," Epoch 5 \n"," --------------\n","Train loss: 1.43E-03\n","Test  loss: 1.42E-03\n","\n"," Epoch 6 \n"," --------------\n","Train loss: 1.98E-03\n","Test  loss: 1.98E-03\n","\n"," Epoch 7 \n"," --------------\n","Train loss: 4.53E-03\n","Test  loss: 4.53E-03\n","\n"," Epoch 8 \n"," --------------\n","Train loss: 2.47E-03\n","Test  loss: 2.46E-03\n","\n"," Epoch 9 \n"," --------------\n","Train loss: 2.27E-03\n","Test  loss: 2.27E-03\n","Done!\n"]}],"source":["start = time.time()\n","trainer.train(number_of_epochs=10)\n","end = time.time()"]},{"cell_type":"code","source":["print(end-start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03FZlAryKrXo","executionInfo":{"status":"ok","timestamp":1681398896574,"user_tz":-120,"elapsed":216,"user":{"displayName":"Thibeau Wouters","userId":"14702334917940433667"}},"outputId":"3c2e3dae-4866-4463-9e9e-cf214e956639"},"id":"03FZlAryKrXo","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["171.4996280670166\n"]}]},{"cell_type":"code","execution_count":null,"id":"2f3a92f7","metadata":{"id":"2f3a92f7"},"outputs":[],"source":["# trainer.report_training(\"NNEOS_tab_experiments.csv\", comment = \"Output now also has cs2, previous did not.\")"]},{"cell_type":"markdown","id":"296ab01e","metadata":{"id":"296ab01e"},"source":["Create a quick sketch of training"]},{"cell_type":"code","execution_count":null,"id":"f8056faf","metadata":{"id":"f8056faf"},"outputs":[],"source":["# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n","# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n","\n","# plt.grid()\n","# plt.legend()\n","# for ind in trainer.adaptation_indices:\n","#     plt.axvline(ind, ls = '--', color='grey')\n","# plt.yscale('log')\n","# plt.xlabel(\"Epochs\")\n","# plt.xlabel(\"MSE Loss\")\n","# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n","# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n","# plt.show()"]},{"cell_type":"markdown","id":"377b0cbf","metadata":{"id":"377b0cbf"},"source":["# Second goal: NNC2P"]},{"cell_type":"markdown","id":"bf95fa58","metadata":{"id":"bf95fa58"},"source":["__TO DO__ think about design of architecture AND fix the conserved variable values -- I think they were not computed correctly before! "]},{"cell_type":"markdown","id":"2b454181","metadata":{"id":"2b454181"},"source":["__NNC2P__: try to replicate the full C2P conversion."]},{"cell_type":"markdown","id":"9c10571f","metadata":{"id":"9c10571f"},"source":["Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."]},{"cell_type":"code","execution_count":null,"id":"852d22c5","metadata":{"id":"852d22c5"},"outputs":[],"source":["# Give the names of the input vars (features) and output vars (labels)\n","in_vars = [\"rho\", \"eps\", \"ye\"]\n","out_vars = [\"temp\"]\n","# For normalization, use sklearn's StandardScaler\n","scaler = StandardScaler()\n","# Read the sampled data as pandas dataframes\n","train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n","test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n","# Convert to PyTorch Datasets as we defined them\n","train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n","test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n","# Then create dataloaders, with batch size 32, from datasets\n","train_dataloader = DataLoader(train_dataset, batch_size=32)\n","test_dataloader  = DataLoader(test_dataset, batch_size=32)"]},{"cell_type":"markdown","id":"24e96de6","metadata":{"id":"24e96de6"},"source":["Create a new instance of the Net:"]},{"cell_type":"code","execution_count":null,"id":"d08aa592","metadata":{"id":"d08aa592","outputId":"1a3ec4f4-f574-496b-d3a8-25cfc9c30361"},"outputs":[{"data":{"text/plain":["Net(\n","  (linear1): Linear(in_features=3, out_features=50, bias=True)\n","  (activation1): Sigmoid()\n","  (linear2): Linear(in_features=50, out_features=50, bias=True)\n","  (activation2): Sigmoid()\n","  (linear3): Linear(in_features=50, out_features=1, bias=False)\n",")"]},"metadata":{},"output_type":"display_data"}],"source":["model = Net(nb_of_inputs = 3, nb_of_outputs = 1, h=[50, 50])\n","model"]},{"cell_type":"code","execution_count":null,"id":"504399cb","metadata":{"id":"504399cb","outputId":"aa1b5560-9e9b-461b-a9e5-3eb90573d2c6"},"outputs":[{"data":{"text/plain":["2800"]},"metadata":{},"output_type":"display_data"}],"source":["nnc2p.count_parameters(model)"]},{"cell_type":"markdown","id":"da83cead","metadata":{"id":"da83cead"},"source":["Create a trainer object from it:"]},{"cell_type":"code","execution_count":null,"id":"7fffc1e4","metadata":{"id":"7fffc1e4"},"outputs":[],"source":["trainer = nnc2p.Trainer(model, 1e-1, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"]},{"cell_type":"code","execution_count":null,"id":"e0369872","metadata":{"id":"e0369872","outputId":"324b4965-3166-4d96-dac7-ff9fa27aae78"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training the model for 500 epochs.\n","\n"," Epoch 0 \n"," --------------\n","Train loss: 7.59E-01\n","Test  loss: 7.62E-01\n","\n"," Epoch 1 \n"," --------------\n","Train loss: 9.92E-01\n","Test  loss: 9.98E-01\n","\n"," Epoch 2 \n"," --------------\n","Train loss: 9.45E-01\n","Test  loss: 9.61E-01\n","\n"," Epoch 3 \n"," --------------\n","Train loss: 8.24E-01\n","Test  loss: 8.29E-01\n","\n"," Epoch 4 \n"," --------------\n","Train loss: 9.10E-01\n","Test  loss: 9.16E-01\n","\n"," Epoch 5 \n"," --------------\n","Train loss: 8.80E-01\n","Test  loss: 8.91E-01\n","\n"," Epoch 6 \n"," --------------\n","Train loss: 8.62E-01\n","Test  loss: 8.67E-01\n","\n"," Epoch 7 \n"," --------------\n","Train loss: 8.20E-01\n","Test  loss: 8.27E-01\n","\n"," Epoch 8 \n"," --------------\n","Train loss: 8.80E-01\n","Test  loss: 8.94E-01\n","\n"," Epoch 9 \n"," --------------\n","Train loss: 9.20E-01\n","Test  loss: 9.26E-01\n","\n"," Epoch 10 \n"," --------------\n","Train loss: 7.34E-01\n","Test  loss: 7.39E-01\n","\n"," Epoch 11 \n"," --------------\n","Train loss: 7.48E-01\n","Test  loss: 7.56E-01\n","\n"," Epoch 12 \n"," --------------\n","Train loss: 7.28E-01\n","Test  loss: 7.31E-01\n","\n"," Epoch 13 \n"," --------------\n","Train loss: 8.87E-01\n","Test  loss: 8.99E-01\n","\n"," Epoch 14 \n"," --------------\n","Train loss: 8.02E-01\n","Test  loss: 8.09E-01\n","\n"," Epoch 15 \n"," --------------\n","Train loss: 8.11E-01\n","Test  loss: 8.20E-01\n","\n"," Epoch 16 \n"," --------------\n","Train loss: 8.02E-01\n","Test  loss: 8.08E-01\n","\n"," Epoch 17 \n"," --------------\n","Adapting learning rate to 0.05\n","Train loss: 8.70E-01\n","Test  loss: 8.79E-01\n","\n"," Epoch 18 \n"," --------------\n","Train loss: 6.88E-01\n","Test  loss: 6.92E-01\n","\n"," Epoch 19 \n"," --------------\n","Train loss: 6.24E-01\n","Test  loss: 6.25E-01\n","\n"," Epoch 20 \n"," --------------\n","Train loss: 6.42E-01\n","Test  loss: 6.46E-01\n","\n"," Epoch 21 \n"," --------------\n","Train loss: 6.21E-01\n","Test  loss: 6.25E-01\n","\n"," Epoch 22 \n"," --------------\n","Train loss: 6.29E-01\n","Test  loss: 6.32E-01\n","\n"," Epoch 23 \n"," --------------\n","Train loss: 6.30E-01\n","Test  loss: 6.33E-01\n","\n"," Epoch 24 \n"," --------------\n","Train loss: 6.13E-01\n","Test  loss: 6.16E-01\n","\n"," Epoch 25 \n"," --------------\n","Train loss: 6.25E-01\n","Test  loss: 6.27E-01\n","\n"," Epoch 26 \n"," --------------\n","Train loss: 6.75E-01\n","Test  loss: 6.81E-01\n","\n"," Epoch 27 \n"," --------------\n","Train loss: 6.22E-01\n","Test  loss: 6.23E-01\n","\n"," Epoch 28 \n"," --------------\n","Train loss: 6.77E-01\n","Test  loss: 6.81E-01\n","\n"," Epoch 29 \n"," --------------\n","Adapting learning rate to 0.025\n","Train loss: 6.98E-01\n","Test  loss: 6.99E-01\n","\n"," Epoch 30 \n"," --------------\n","Train loss: 5.71E-01\n","Test  loss: 5.74E-01\n","\n"," Epoch 31 \n"," --------------\n","Train loss: 5.77E-01\n","Test  loss: 5.81E-01\n","\n"," Epoch 32 \n"," --------------\n","Train loss: 5.69E-01\n","Test  loss: 5.74E-01\n","\n"," Epoch 33 \n"," --------------\n","Train loss: 5.65E-01\n","Test  loss: 5.69E-01\n","\n"," Epoch 34 \n"," --------------\n","Train loss: 5.63E-01\n","Test  loss: 5.67E-01\n","\n"," Epoch 35 \n"," --------------\n","Train loss: 5.60E-01\n","Test  loss: 5.65E-01\n","\n"," Epoch 36 \n"," --------------\n","Train loss: 5.67E-01\n","Test  loss: 5.70E-01\n","\n"," Epoch 37 \n"," --------------\n","Train loss: 5.58E-01\n","Test  loss: 5.60E-01\n","\n"," Epoch 38 \n"," --------------\n","Train loss: 5.62E-01\n","Test  loss: 5.66E-01\n","\n"," Epoch 39 \n"," --------------\n","Train loss: 5.62E-01\n","Test  loss: 5.66E-01\n","\n"," Epoch 40 \n"," --------------\n","Train loss: 5.62E-01\n","Test  loss: 5.64E-01\n","\n"," Epoch 41 \n"," --------------\n","Train loss: 5.73E-01\n","Test  loss: 5.75E-01\n","\n"," Epoch 42 \n"," --------------\n","Adapting learning rate to 0.0125\n","Train loss: 5.69E-01\n","Test  loss: 5.73E-01\n","\n"," Epoch 43 \n"," --------------\n","Train loss: 4.89E-01\n","Test  loss: 4.92E-01\n","\n"," Epoch 44 \n"," --------------\n","Train loss: 4.87E-01\n","Test  loss: 4.91E-01\n","\n"," Epoch 45 \n"," --------------\n","Train loss: 4.86E-01\n","Test  loss: 4.91E-01\n","\n"," Epoch 46 \n"," --------------\n","Train loss: 4.83E-01\n","Test  loss: 4.88E-01\n","\n"," Epoch 47 \n"," --------------\n","Train loss: 4.80E-01\n","Test  loss: 4.85E-01\n","\n"," Epoch 48 \n"," --------------\n","Train loss: 4.78E-01\n","Test  loss: 4.83E-01\n","\n"," Epoch 49 \n"," --------------\n","Train loss: 4.77E-01\n","Test  loss: 4.81E-01\n","\n"," Epoch 50 \n"," --------------\n","Train loss: 4.75E-01\n","Test  loss: 4.79E-01\n","\n"," Epoch 51 \n"," --------------\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n","Cell \u001b[1;32mIn[118], line 1\u001b[0m\n","\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","\n","File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n","\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n","\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n","\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n","\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n","\n","File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:277\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n","\u001b[0;32m    274\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n","\u001b[0;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n","\u001b[0;32m    276\u001b[0m     \u001b[39m# Compute prediction \u001b[39;00m\n","\u001b[1;32m--> 277\u001b[0m     prediction \u001b[39m=\u001b[39m model(X)\n","\u001b[0;32m    278\u001b[0m     \u001b[39m# In case we use C2P loss function, have to provide conserved variables for the loss computation\u001b[39;00m\n","\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m use_c2p_loss:\n","\n","File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n","\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n","\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n","\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n","\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n","\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n","\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\n","Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n","\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n","\u001b[0;32m     38\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","\u001b[1;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n","\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","\n","File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n","\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n","\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n","\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n","\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n","\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n","\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\n","File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n","\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n","\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"markdown","id":"1b8a5288","metadata":{"id":"1b8a5288"},"source":["Create a quick sketch of training"]},{"cell_type":"code","execution_count":null,"id":"f07eb521","metadata":{"id":"f07eb521"},"outputs":[],"source":["# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n","# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n","\n","# plt.grid()\n","# plt.legend()\n","# for ind in trainer.adaptation_indices:\n","#     plt.axvline(ind, ls = '--', color='grey')\n","# plt.yscale('log')\n","# plt.xlabel(\"Epochs\")\n","# plt.xlabel(\"MSE Loss\")\n","# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n","# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n","# plt.show()"]},{"cell_type":"markdown","id":"8ad41d2a","metadata":{"id":"8ad41d2a"},"source":["# Archive: NNE2T"]},{"cell_type":"markdown","id":"391a25b5","metadata":{"id":"391a25b5"},"source":["__NNE2T__: try to replicate the conversion from energy to temperature, which is currently done by rootfinding approximations & lookups in the EOS table (see Gmunu code). It seemed harder than I initially thought to model and train this; and in the end, I'm not sure how useful it'll be, so I'm archiving this."]},{"cell_type":"markdown","id":"7f48e387","metadata":{"id":"7f48e387"},"source":["Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."]},{"cell_type":"code","execution_count":null,"id":"b88f2675","metadata":{"id":"b88f2675"},"outputs":[],"source":["# Give the names of the input vars (features) and output vars (labels)\n","in_vars = [\"rho\", \"eps\", \"ye\"]\n","out_vars = [\"temp\"]\n","# For normalization, use sklearn's StandardScaler\n","scaler = StandardScaler()\n","# Read the sampled data as pandas dataframes\n","train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n","test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n","# Convert to PyTorch Datasets as we defined them\n","train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n","test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n","# Then create dataloaders, with batch size 32, from datasets\n","train_dataloader = DataLoader(train_dataset, batch_size=32)\n","test_dataloader  = DataLoader(test_dataset, batch_size=32)"]},{"cell_type":"markdown","id":"abee61f0","metadata":{"id":"abee61f0"},"source":["Create a new instance of the Net:"]},{"cell_type":"code","execution_count":null,"id":"b990a7ff","metadata":{"id":"b990a7ff","outputId":"76d4c28e-ca6a-423d-e905-27d55d54b42e"},"outputs":[{"data":{"text/plain":["Net(\n","  (linear1): Linear(in_features=3, out_features=50, bias=True)\n","  (activation1): Sigmoid()\n","  (linear2): Linear(in_features=50, out_features=50, bias=True)\n","  (activation2): Sigmoid()\n","  (linear3): Linear(in_features=50, out_features=1, bias=False)\n",")"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["model = Net(nb_of_inputs = 3, nb_of_outputs = 1, h=[50, 50])\n","model"]},{"cell_type":"code","execution_count":null,"id":"312052a9","metadata":{"id":"312052a9","outputId":"18d37e9e-a86e-48e2-87bd-c82646596aeb"},"outputs":[{"data":{"text/plain":["2800"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["nnc2p.count_parameters(model)"]},{"cell_type":"markdown","id":"1f23d022","metadata":{"id":"1f23d022"},"source":["Create a trainer object from it:"]},{"cell_type":"code","execution_count":null,"id":"06becda4","metadata":{"id":"06becda4"},"outputs":[],"source":["trainer = nnc2p.Trainer(model, 1e-1, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"]},{"cell_type":"code","execution_count":null,"id":"93ce3bda","metadata":{"id":"93ce3bda","outputId":"f4a25ecd-bfdf-4140-b06d-6c414eaf8736"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training the model for 500 epochs.\n","\n"," Epoch 0 \n"," --------------\n","Train loss: 7.59E-01\n","Test  loss: 7.62E-01\n","\n"," Epoch 1 \n"," --------------\n","Train loss: 9.92E-01\n","Test  loss: 9.98E-01\n","\n"," Epoch 2 \n"," --------------\n","Train loss: 9.45E-01\n","Test  loss: 9.61E-01\n","\n"," Epoch 3 \n"," --------------\n","Train loss: 8.24E-01\n","Test  loss: 8.29E-01\n","\n"," Epoch 4 \n"," --------------\n","Train loss: 9.10E-01\n","Test  loss: 9.16E-01\n","\n"," Epoch 5 \n"," --------------\n","Train loss: 8.80E-01\n","Test  loss: 8.91E-01\n","\n"," Epoch 6 \n"," --------------\n","Train loss: 8.62E-01\n","Test  loss: 8.67E-01\n","\n"," Epoch 7 \n"," --------------\n","Train loss: 8.20E-01\n","Test  loss: 8.27E-01\n","\n"," Epoch 8 \n"," --------------\n","Train loss: 8.80E-01\n","Test  loss: 8.94E-01\n","\n"," Epoch 9 \n"," --------------\n","Train loss: 9.20E-01\n","Test  loss: 9.26E-01\n","\n"," Epoch 10 \n"," --------------\n","Train loss: 7.34E-01\n","Test  loss: 7.39E-01\n","\n"," Epoch 11 \n"," --------------\n","Train loss: 7.48E-01\n","Test  loss: 7.56E-01\n","\n"," Epoch 12 \n"," --------------\n","Train loss: 7.28E-01\n","Test  loss: 7.31E-01\n","\n"," Epoch 13 \n"," --------------\n","Train loss: 8.87E-01\n","Test  loss: 8.99E-01\n","\n"," Epoch 14 \n"," --------------\n","Train loss: 8.02E-01\n","Test  loss: 8.09E-01\n","\n"," Epoch 15 \n"," --------------\n","Train loss: 8.11E-01\n","Test  loss: 8.20E-01\n","\n"," Epoch 16 \n"," --------------\n","Train loss: 8.02E-01\n","Test  loss: 8.08E-01\n","\n"," Epoch 17 \n"," --------------\n","Adapting learning rate to 0.05\n","Train loss: 8.70E-01\n","Test  loss: 8.79E-01\n","\n"," Epoch 18 \n"," --------------\n","Train loss: 6.88E-01\n","Test  loss: 6.92E-01\n","\n"," Epoch 19 \n"," --------------\n","Train loss: 6.24E-01\n","Test  loss: 6.25E-01\n","\n"," Epoch 20 \n"," --------------\n","Train loss: 6.42E-01\n","Test  loss: 6.46E-01\n","\n"," Epoch 21 \n"," --------------\n","Train loss: 6.21E-01\n","Test  loss: 6.25E-01\n","\n"," Epoch 22 \n"," --------------\n","Train loss: 6.29E-01\n","Test  loss: 6.32E-01\n","\n"," Epoch 23 \n"," --------------\n","Train loss: 6.30E-01\n","Test  loss: 6.33E-01\n","\n"," Epoch 24 \n"," --------------\n","Train loss: 6.13E-01\n","Test  loss: 6.16E-01\n","\n"," Epoch 25 \n"," --------------\n","Train loss: 6.25E-01\n","Test  loss: 6.27E-01\n","\n"," Epoch 26 \n"," --------------\n","Train loss: 6.75E-01\n","Test  loss: 6.81E-01\n","\n"," Epoch 27 \n"," --------------\n","Train loss: 6.22E-01\n","Test  loss: 6.23E-01\n","\n"," Epoch 28 \n"," --------------\n","Train loss: 6.77E-01\n","Test  loss: 6.81E-01\n","\n"," Epoch 29 \n"," --------------\n","Adapting learning rate to 0.025\n","Train loss: 6.98E-01\n","Test  loss: 6.99E-01\n","\n"," Epoch 30 \n"," --------------\n","Train loss: 5.71E-01\n","Test  loss: 5.74E-01\n","\n"," Epoch 31 \n"," --------------\n","Train loss: 5.77E-01\n","Test  loss: 5.81E-01\n","\n"," Epoch 32 \n"," --------------\n","Train loss: 5.69E-01\n","Test  loss: 5.74E-01\n","\n"," Epoch 33 \n"," --------------\n","Train loss: 5.65E-01\n","Test  loss: 5.69E-01\n","\n"," Epoch 34 \n"," --------------\n","Train loss: 5.63E-01\n","Test  loss: 5.67E-01\n","\n"," Epoch 35 \n"," --------------\n","Train loss: 5.60E-01\n","Test  loss: 5.65E-01\n","\n"," Epoch 36 \n"," --------------\n","Train loss: 5.67E-01\n","Test  loss: 5.70E-01\n","\n"," Epoch 37 \n"," --------------\n","Train loss: 5.58E-01\n","Test  loss: 5.60E-01\n","\n"," Epoch 38 \n"," --------------\n","Train loss: 5.62E-01\n","Test  loss: 5.66E-01\n","\n"," Epoch 39 \n"," --------------\n","Train loss: 5.62E-01\n","Test  loss: 5.66E-01\n","\n"," Epoch 40 \n"," --------------\n","Train loss: 5.62E-01\n","Test  loss: 5.64E-01\n","\n"," Epoch 41 \n"," --------------\n","Train loss: 5.73E-01\n","Test  loss: 5.75E-01\n","\n"," Epoch 42 \n"," --------------\n","Adapting learning rate to 0.0125\n","Train loss: 5.69E-01\n","Test  loss: 5.73E-01\n","\n"," Epoch 43 \n"," --------------\n","Train loss: 4.89E-01\n","Test  loss: 4.92E-01\n","\n"," Epoch 44 \n"," --------------\n","Train loss: 4.87E-01\n","Test  loss: 4.91E-01\n","\n"," Epoch 45 \n"," --------------\n","Train loss: 4.86E-01\n","Test  loss: 4.91E-01\n","\n"," Epoch 46 \n"," --------------\n","Train loss: 4.83E-01\n","Test  loss: 4.88E-01\n","\n"," Epoch 47 \n"," --------------\n","Train loss: 4.80E-01\n","Test  loss: 4.85E-01\n","\n"," Epoch 48 \n"," --------------\n","Train loss: 4.78E-01\n","Test  loss: 4.83E-01\n","\n"," Epoch 49 \n"," --------------\n","Train loss: 4.77E-01\n","Test  loss: 4.81E-01\n","\n"," Epoch 50 \n"," --------------\n","Train loss: 4.75E-01\n","Test  loss: 4.79E-01\n","\n"," Epoch 51 \n"," --------------\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[118], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n","File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:277\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n\u001b[0;32m    274\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m    276\u001b[0m     \u001b[39m# Compute prediction \u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     prediction \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m    278\u001b[0m     \u001b[39m# In case we use C2P loss function, have to provide conserved variables for the loss computation\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m use_c2p_loss:\n","File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"markdown","id":"8cc2a130","metadata":{"id":"8cc2a130"},"source":["Create a quick sketch of training"]},{"cell_type":"code","execution_count":null,"id":"c5338413","metadata":{"id":"c5338413"},"outputs":[],"source":["# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n","# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n","\n","# plt.grid()\n","# plt.legend()\n","# for ind in trainer.adaptation_indices:\n","#     plt.axvline(ind, ls = '--', color='grey')\n","# plt.yscale('log')\n","# plt.xlabel(\"Epochs\")\n","# plt.xlabel(\"MSE Loss\")\n","# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n","# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n","# plt.show()"]}],"metadata":{"author":"Thibeau Wouters","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}