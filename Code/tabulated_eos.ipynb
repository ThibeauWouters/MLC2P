{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb9f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import h5py\n",
    "# Scikit learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize \n",
    "# Own scripts:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import physics\n",
    "import data\n",
    "import nnc2p\n",
    "# from nnc2p import NeuralNetwork # our own architecture\n",
    "# Get dirs\n",
    "import os\n",
    "cwd = os.getcwd()# \"Code\" folder\n",
    "master_dir = os.path.abspath(os.path.join(cwd, \"..\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3393c80",
   "metadata": {},
   "source": [
    "Point towards the folder where we store the eos tables (__Note:__ they are not in the Github as these are very large files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbc8d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to look for EOS table at D:/Coding/Datasets/eos_tables\n"
     ]
    }
   ],
   "source": [
    "eos_tables_folder = os.path.join(\"D:/Coding/Datasets/eos_tables\")\n",
    "print(f\"Going to look for EOS table at {eos_tables_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb3c8c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96ab265e",
   "metadata": {},
   "source": [
    "Here, we try to find a way to generalize the NN approach from the first semester to the situation of tabular EOS. More work coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08ad4a",
   "metadata": {},
   "source": [
    "# Exploring EOS tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5e6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the downloaded EOS tables here\n",
    "first_table_filename       = \"LS180_234r_136t_50y_analmu_20091212_SVNr26.h5\"\n",
    "second_table_filename = \"GShen_NL3EOS_rho280_temp180_ye52_version_1.1_20120817.h5\"\n",
    "third_table_filename      = \"SLy4_0000_rho391_temp163_ye66.h5\"\n",
    "# Then specify which we are going to use here\n",
    "eos_table_filename = third_table_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8f465",
   "metadata": {},
   "source": [
    "Read in the SLy4 EOS table using our py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913d9582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This EOS table has dimensions 66 x 163 x 391\n"
     ]
    }
   ],
   "source": [
    "eos_table = physics.read_eos_table(os.path.join(eos_tables_folder, eos_table_filename))\n",
    "dim_ye, dim_temp, dim_rho = eos_table[\"pointsye\"][()][0], eos_table[\"pointstemp\"][()][0], eos_table[\"pointsrho\"][()][0]\n",
    "print(f\"This EOS table has dimensions {dim_ye} x {dim_temp} x {dim_rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d38366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 163, 391)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(eos_table[\"logenergy\"][()])  # same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74b0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ye 0.005, log temp -3.0, log rho 3.0239960056064277, we have log p: 17.99956975587081.\n"
     ]
    }
   ],
   "source": [
    "# Small test to see the output of the EOS table\n",
    "test_ye = eos_table[\"ye\"][()][0]\n",
    "test_temp = eos_table[\"logtemp\"][()][0]\n",
    "test_rho = eos_table[\"logrho\"][()][0]\n",
    "test_press = eos_table[\"logpress\"][()][0, 0, 0]\n",
    "print(f\"For ye {test_ye}, log temp {test_temp}, log rho {test_rho}, we have log p: {test_press}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0167f31",
   "metadata": {},
   "source": [
    "See what is inside this EOS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316b6963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abar', 'Albar', 'MERGE-space.in', 'MERGE-src.tar.gz', 'MERGE-tables.in', 'MERGE-transition.in', 'SNA-skyrme.in', 'SNA-space.in', 'SNA-src.tar.gz', 'Xa', 'Xh', 'Xl', 'Xn', 'Xp', 'Zbar', 'Zlbar', 'cs2', 'dedt', 'dpderho', 'dpdrhoe', 'energy_shift', 'entropy', 'gamma', 'have_rel_cs2', 'logenergy', 'logpress', 'logrho', 'logtemp', 'meffn', 'meffp', 'mu_e', 'mu_n', 'mu_p', 'muhat', 'munu', 'pointsrho', 'pointstemp', 'pointsye', 'r', 'u', 'ye']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# Iterate over keys and save them to list for simplified viewing\n",
    "keys = []\n",
    "for key in eos_table:\n",
    "    keys.append(key)\n",
    "print(keys)\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15e58b",
   "metadata": {},
   "source": [
    "# Generating training data by sampling from EOS table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc8fc6d3",
   "metadata": {},
   "source": [
    "To generate new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "511ae7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = physics.generate_tabular_data(eos_table, number_of_points = 100000, save_name = \"SLy4_training_data\")\n",
    "# dat = physics.generate_tabular_data(eos_table, number_of_points = 20000, save_name = \"SLy4_test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a121af",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f929937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>logeps</th>\n",
       "      <th>v</th>\n",
       "      <th>logtemp</th>\n",
       "      <th>ye</th>\n",
       "      <th>logpress</th>\n",
       "      <th>logcs2</th>\n",
       "      <th>D</th>\n",
       "      <th>S</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.523996</td>\n",
       "      <td>19.616338</td>\n",
       "      <td>0.691345</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td>0.175</td>\n",
       "      <td>23.395574</td>\n",
       "      <td>43.696991</td>\n",
       "      <td>6.261371</td>\n",
       "      <td>3.295258e+23</td>\n",
       "      <td>2.280030e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.490663</td>\n",
       "      <td>19.528674</td>\n",
       "      <td>0.481485</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.285</td>\n",
       "      <td>31.366264</td>\n",
       "      <td>43.774950</td>\n",
       "      <td>14.251357</td>\n",
       "      <td>1.456761e+31</td>\n",
       "      <td>7.014085e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.623996</td>\n",
       "      <td>19.448926</td>\n",
       "      <td>0.053539</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>0.175</td>\n",
       "      <td>32.269661</td>\n",
       "      <td>43.182871</td>\n",
       "      <td>13.643564</td>\n",
       "      <td>9.990231e+30</td>\n",
       "      <td>5.348641e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.890663</td>\n",
       "      <td>19.205428</td>\n",
       "      <td>0.590829</td>\n",
       "      <td>-1.266667</td>\n",
       "      <td>0.155</td>\n",
       "      <td>27.232815</td>\n",
       "      <td>40.255750</td>\n",
       "      <td>12.259173</td>\n",
       "      <td>1.551489e+27</td>\n",
       "      <td>9.166645e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.657329</td>\n",
       "      <td>19.074603</td>\n",
       "      <td>0.129825</td>\n",
       "      <td>-2.633333</td>\n",
       "      <td>0.335</td>\n",
       "      <td>25.966272</td>\n",
       "      <td>40.166319</td>\n",
       "      <td>8.731223</td>\n",
       "      <td>1.221837e+25</td>\n",
       "      <td>1.586354e+24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>3.057329</td>\n",
       "      <td>19.089191</td>\n",
       "      <td>0.604101</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>0.605</td>\n",
       "      <td>19.201463</td>\n",
       "      <td>37.575094</td>\n",
       "      <td>3.836490</td>\n",
       "      <td>5.084013e+19</td>\n",
       "      <td>6.825593e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>6.590663</td>\n",
       "      <td>20.798573</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.305</td>\n",
       "      <td>26.896812</td>\n",
       "      <td>46.403492</td>\n",
       "      <td>6.592034</td>\n",
       "      <td>1.609076e+25</td>\n",
       "      <td>3.323597e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>7.790663</td>\n",
       "      <td>19.106546</td>\n",
       "      <td>0.125179</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>0.605</td>\n",
       "      <td>25.172194</td>\n",
       "      <td>40.353432</td>\n",
       "      <td>7.852428</td>\n",
       "      <td>1.890548e+24</td>\n",
       "      <td>2.367563e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>9.623996</td>\n",
       "      <td>19.120506</td>\n",
       "      <td>0.593556</td>\n",
       "      <td>-1.566667</td>\n",
       "      <td>0.295</td>\n",
       "      <td>27.193780</td>\n",
       "      <td>40.754153</td>\n",
       "      <td>11.958352</td>\n",
       "      <td>1.431770e+27</td>\n",
       "      <td>8.498353e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3.790663</td>\n",
       "      <td>20.324174</td>\n",
       "      <td>0.235517</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.275</td>\n",
       "      <td>23.540747</td>\n",
       "      <td>45.513301</td>\n",
       "      <td>3.900379</td>\n",
       "      <td>8.680609e+22</td>\n",
       "      <td>2.124390e+22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rho     logeps         v   logtemp     ye   logpress     logcs2  \\\n",
       "0       4.523996  19.616338  0.691345 -0.633333  0.175  23.395574  43.696991   \n",
       "1      12.490663  19.528674  0.481485  0.733333  0.285  31.366264  43.774950   \n",
       "2      13.623996  19.448926  0.053539 -0.233333  0.175  32.269661  43.182871   \n",
       "3       9.890663  19.205428  0.590829 -1.266667  0.155  27.232815  40.255750   \n",
       "4       8.657329  19.074603  0.129825 -2.633333  0.335  25.966272  40.166319   \n",
       "...          ...        ...       ...       ...    ...        ...        ...   \n",
       "99995   3.057329  19.089191  0.604101 -1.800000  0.605  19.201463  37.575094   \n",
       "99996   6.590663  20.798573  0.020398  0.200000  0.305  26.896812  46.403492   \n",
       "99997   7.790663  19.106546  0.125179 -1.100000  0.605  25.172194  40.353432   \n",
       "99998   9.623996  19.120506  0.593556 -1.566667  0.295  27.193780  40.754153   \n",
       "99999   3.790663  20.324174  0.235517 -0.600000  0.275  23.540747  45.513301   \n",
       "\n",
       "               D             S           tau  \n",
       "0       6.261371  3.295258e+23  2.280030e+23  \n",
       "1      14.251357  1.456761e+31  7.014085e+30  \n",
       "2      13.643564  9.990231e+30  5.348641e+29  \n",
       "3      12.259173  1.551489e+27  9.166645e+26  \n",
       "4       8.731223  1.221837e+25  1.586354e+24  \n",
       "...          ...           ...           ...  \n",
       "99995   3.836490  5.084013e+19  6.825593e+19  \n",
       "99996   6.592034  1.609076e+25  3.323597e+23  \n",
       "99997   7.852428  1.890548e+24  2.367563e+23  \n",
       "99998  11.958352  1.431770e+27  8.498353e+26  \n",
       "99999   3.900379  8.680609e+22  2.124390e+22  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f2fce3d",
   "metadata": {},
   "source": [
    "The network architecture we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270fd513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple feedforward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_of_inputs: int = 3, nb_of_outputs: int = 1, h: list = [600, 200], reg: bool = False, activation_function = torch.nn.Sigmoid) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network class.\n",
    "        \"\"\"\n",
    "        # Call the super constructor first\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # For convenience, save the sizes of the hidden layers as fields as well\n",
    "        self.h = h\n",
    "        # Add visible layers as well: input is 3D and output is 1D\n",
    "        self.h_augmented = [nb_of_inputs] + h + [nb_of_outputs]\n",
    "\n",
    "        # Add field to specify whether or not we do regularization\n",
    "        self.regularization = reg\n",
    "\n",
    "        # Define the layers:\n",
    "        for i in range(len(self.h_augmented)-1):\n",
    "            if i == len(self.h_augmented)-2:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1], bias=False))\n",
    "            else:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1]))\n",
    "                setattr(self, f\"activation{i+1}\", activation_function())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward step given the input x.\n",
    "        :param x: Input for the neural network.\n",
    "        :return: x: Output neural network\n",
    "        \"\"\"\n",
    "\n",
    "        for i, module in enumerate(self.modules()):\n",
    "            # The first module is the whole NNC2P object, continue\n",
    "            if i == 0:\n",
    "                continue\n",
    "            x = module(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fd2f127",
   "metadata": {},
   "source": [
    "# First goal: NNEOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5850152",
   "metadata": {},
   "source": [
    "__NNEOS__: try to replicate the EOS table (at least the core variables we are interested in) using the \"input\" variables rho, temp, ye."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec6d6047",
   "metadata": {},
   "source": [
    "Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a91d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"rho\", \"logtemp\", \"ye\"]\n",
    "out_vars = [\"logeps\", \"logpress\", \"logcs2\"]\n",
    "# For normalization, use sklearn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f12e532a",
   "metadata": {},
   "source": [
    "Create a new instance of the Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7a388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=100, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=100, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(nb_of_inputs = len(in_vars), nb_of_outputs = len(out_vars), h=[100, 100])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a0e1f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10800"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e3745",
   "metadata": {},
   "source": [
    "Create a trainer object from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df92e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-2, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ce8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 3.66E-02\n",
      "Test  loss: 3.69E-02\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 1.13E-02\n",
      "Test  loss: 1.14E-02\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 8.54E-03\n",
      "Test  loss: 8.78E-03\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 2.23E-02\n",
      "Test  loss: 2.24E-02\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 2.61E-02\n",
      "Test  loss: 2.61E-02\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 2.15E-02\n",
      "Test  loss: 2.15E-02\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 9.52E-03\n",
      "Test  loss: 9.62E-03\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 7.20E-03\n",
      "Test  loss: 7.33E-03\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 6.29E-03\n",
      "Test  loss: 6.44E-03\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 4.74E-03\n",
      "Test  loss: 4.90E-03\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 7.73E-03\n",
      "Test  loss: 7.87E-03\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 1.21E-02\n",
      "Test  loss: 1.23E-02\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Train loss: 1.11E-02\n",
      "Test  loss: 1.13E-02\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 6.60E-03\n",
      "Test  loss: 6.79E-03\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 3.76E-03\n",
      "Test  loss: 3.93E-03\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 4.97E-03\n",
      "Test  loss: 5.15E-03\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 3.49E-03\n",
      "Test  loss: 3.65E-03\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Train loss: 3.20E-03\n",
      "Test  loss: 3.36E-03\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 3.34E-03\n",
      "Test  loss: 3.50E-03\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 3.20E-03\n",
      "Test  loss: 3.36E-03\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 2.85E-03\n",
      "Test  loss: 3.02E-03\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Train loss: 2.94E-03\n",
      "Test  loss: 3.11E-03\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 4.06E-03\n",
      "Test  loss: 4.22E-03\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Train loss: 3.62E-03\n",
      "Test  loss: 3.78E-03\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 6.18E-03\n",
      "Test  loss: 6.33E-03\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Adapting learning rate to 0.005\n",
      "Train loss: 5.28E-03\n",
      "Test  loss: 5.43E-03\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 1.50E-03\n",
      "Test  loss: 1.69E-03\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 1.34E-03\n",
      "Test  loss: 1.52E-03\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 1.22E-03\n",
      "Test  loss: 1.40E-03\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Train loss: 1.16E-03\n",
      "Test  loss: 1.33E-03\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 1.12E-03\n",
      "Test  loss: 1.29E-03\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 1.10E-03\n",
      "Test  loss: 1.27E-03\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 1.09E-03\n",
      "Test  loss: 1.26E-03\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 1.07E-03\n",
      "Test  loss: 1.24E-03\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Train loss: 1.07E-03\n",
      "Test  loss: 1.24E-03\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 1.07E-03\n",
      "Test  loss: 1.24E-03\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 1.07E-03\n",
      "Test  loss: 1.24E-03\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 1.06E-03\n",
      "Test  loss: 1.23E-03\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 1.07E-03\n",
      "Test  loss: 1.24E-03\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 1.08E-03\n",
      "Test  loss: 1.24E-03\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 1.09E-03\n",
      "Test  loss: 1.25E-03\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 1.10E-03\n",
      "Test  loss: 1.27E-03\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Adapting learning rate to 0.0025\n",
      "Train loss: 1.12E-03\n",
      "Test  loss: 1.29E-03\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 7.92E-04\n",
      "Test  loss: 9.65E-04\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 7.84E-04\n",
      "Test  loss: 9.57E-04\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 7.80E-04\n",
      "Test  loss: 9.53E-04\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 7.79E-04\n",
      "Test  loss: 9.52E-04\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 7.81E-04\n",
      "Test  loss: 9.53E-04\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 7.82E-04\n",
      "Test  loss: 9.55E-04\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 7.85E-04\n",
      "Test  loss: 9.57E-04\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 7.88E-04\n",
      "Test  loss: 9.60E-04\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n",
      "Train loss: 7.91E-04\n",
      "Test  loss: 9.62E-04\n",
      "\n",
      " Epoch 52 \n",
      " --------------\n",
      "Train loss: 7.93E-04\n",
      "Test  loss: 9.64E-04\n",
      "\n",
      " Epoch 53 \n",
      " --------------\n",
      "Adapting learning rate to 0.00125\n",
      "Train loss: 7.94E-04\n",
      "Test  loss: 9.66E-04\n",
      "\n",
      " Epoch 54 \n",
      " --------------\n",
      "Train loss: 7.06E-04\n",
      "Test  loss: 8.66E-04\n",
      "\n",
      " Epoch 55 \n",
      " --------------\n",
      "Train loss: 6.98E-04\n",
      "Test  loss: 8.58E-04\n",
      "\n",
      " Epoch 56 \n",
      " --------------\n",
      "Train loss: 6.90E-04\n",
      "Test  loss: 8.50E-04\n",
      "\n",
      " Epoch 57 \n",
      " --------------\n",
      "Train loss: 6.83E-04\n",
      "Test  loss: 8.43E-04\n",
      "\n",
      " Epoch 58 \n",
      " --------------\n",
      "Train loss: 6.78E-04\n",
      "Test  loss: 8.38E-04\n",
      "\n",
      " Epoch 59 \n",
      " --------------\n",
      "Train loss: 6.74E-04\n",
      "Test  loss: 8.33E-04\n",
      "\n",
      " Epoch 60 \n",
      " --------------\n",
      "Train loss: 6.70E-04\n",
      "Test  loss: 8.29E-04\n",
      "\n",
      " Epoch 61 \n",
      " --------------\n",
      "Train loss: 6.66E-04\n",
      "Test  loss: 8.25E-04\n",
      "\n",
      " Epoch 62 \n",
      " --------------\n",
      "Train loss: 6.63E-04\n",
      "Test  loss: 8.21E-04\n",
      "\n",
      " Epoch 63 \n",
      " --------------\n",
      "Train loss: 6.59E-04\n",
      "Test  loss: 8.18E-04\n",
      "\n",
      " Epoch 64 \n",
      " --------------\n",
      "Train loss: 6.56E-04\n",
      "Test  loss: 8.14E-04\n",
      "\n",
      " Epoch 65 \n",
      " --------------\n",
      "Train loss: 6.53E-04\n",
      "Test  loss: 8.11E-04\n",
      "\n",
      " Epoch 66 \n",
      " --------------\n",
      "Train loss: 6.50E-04\n",
      "Test  loss: 8.08E-04\n",
      "\n",
      " Epoch 67 \n",
      " --------------\n",
      "Train loss: 6.48E-04\n",
      "Test  loss: 8.05E-04\n",
      "\n",
      " Epoch 68 \n",
      " --------------\n",
      "Train loss: 6.45E-04\n",
      "Test  loss: 8.03E-04\n",
      "\n",
      " Epoch 69 \n",
      " --------------\n",
      "Train loss: 6.43E-04\n",
      "Test  loss: 8.00E-04\n",
      "\n",
      " Epoch 70 \n",
      " --------------\n",
      "Train loss: 6.41E-04\n",
      "Test  loss: 7.98E-04\n",
      "\n",
      " Epoch 71 \n",
      " --------------\n",
      "Train loss: 6.38E-04\n",
      "Test  loss: 7.95E-04\n",
      "\n",
      " Epoch 72 \n",
      " --------------\n",
      "Train loss: 6.36E-04\n",
      "Test  loss: 7.93E-04\n",
      "\n",
      " Epoch 73 \n",
      " --------------\n",
      "Train loss: 6.34E-04\n",
      "Test  loss: 7.91E-04\n",
      "\n",
      " Epoch 74 \n",
      " --------------\n",
      "Train loss: 6.32E-04\n",
      "Test  loss: 7.89E-04\n",
      "\n",
      " Epoch 75 \n",
      " --------------\n",
      "Train loss: 6.31E-04\n",
      "Test  loss: 7.87E-04\n",
      "\n",
      " Epoch 76 \n",
      " --------------\n",
      "Train loss: 6.29E-04\n",
      "Test  loss: 7.85E-04\n",
      "\n",
      " Epoch 77 \n",
      " --------------\n",
      "Train loss: 6.27E-04\n",
      "Test  loss: 7.83E-04\n",
      "\n",
      " Epoch 78 \n",
      " --------------\n",
      "Train loss: 6.25E-04\n",
      "Test  loss: 7.81E-04\n",
      "\n",
      " Epoch 79 \n",
      " --------------\n",
      "Train loss: 6.24E-04\n",
      "Test  loss: 7.79E-04\n",
      "\n",
      " Epoch 80 \n",
      " --------------\n",
      "Train loss: 6.22E-04\n",
      "Test  loss: 7.77E-04\n",
      "\n",
      " Epoch 81 \n",
      " --------------\n",
      "Train loss: 6.20E-04\n",
      "Test  loss: 7.75E-04\n",
      "\n",
      " Epoch 82 \n",
      " --------------\n",
      "Train loss: 6.19E-04\n",
      "Test  loss: 7.73E-04\n",
      "\n",
      " Epoch 83 \n",
      " --------------\n",
      "Train loss: 6.17E-04\n",
      "Test  loss: 7.72E-04\n",
      "\n",
      " Epoch 84 \n",
      " --------------\n",
      "Train loss: 6.16E-04\n",
      "Test  loss: 7.70E-04\n",
      "\n",
      " Epoch 85 \n",
      " --------------\n",
      "Train loss: 6.14E-04\n",
      "Test  loss: 7.68E-04\n",
      "\n",
      " Epoch 86 \n",
      " --------------\n",
      "Train loss: 6.13E-04\n",
      "Test  loss: 7.67E-04\n",
      "\n",
      " Epoch 87 \n",
      " --------------\n",
      "Train loss: 6.11E-04\n",
      "Test  loss: 7.65E-04\n",
      "\n",
      " Epoch 88 \n",
      " --------------\n",
      "Train loss: 6.10E-04\n",
      "Test  loss: 7.63E-04\n",
      "\n",
      " Epoch 89 \n",
      " --------------\n",
      "Train loss: 6.08E-04\n",
      "Test  loss: 7.61E-04\n",
      "\n",
      " Epoch 90 \n",
      " --------------\n",
      "Train loss: 6.07E-04\n",
      "Test  loss: 7.59E-04\n",
      "\n",
      " Epoch 91 \n",
      " --------------\n",
      "Train loss: 6.05E-04\n",
      "Test  loss: 7.58E-04\n",
      "\n",
      " Epoch 92 \n",
      " --------------\n",
      "Train loss: 6.04E-04\n",
      "Test  loss: 7.56E-04\n",
      "\n",
      " Epoch 93 \n",
      " --------------\n",
      "Train loss: 6.02E-04\n",
      "Test  loss: 7.54E-04\n",
      "\n",
      " Epoch 94 \n",
      " --------------\n",
      "Train loss: 6.00E-04\n",
      "Test  loss: 7.52E-04\n",
      "\n",
      " Epoch 95 \n",
      " --------------\n",
      "Train loss: 5.99E-04\n",
      "Test  loss: 7.50E-04\n",
      "\n",
      " Epoch 96 \n",
      " --------------\n",
      "Train loss: 5.97E-04\n",
      "Test  loss: 7.48E-04\n",
      "\n",
      " Epoch 97 \n",
      " --------------\n",
      "Train loss: 5.95E-04\n",
      "Test  loss: 7.46E-04\n",
      "\n",
      " Epoch 98 \n",
      " --------------\n",
      "Train loss: 5.94E-04\n",
      "Test  loss: 7.44E-04\n",
      "\n",
      " Epoch 99 \n",
      " --------------\n",
      "Train loss: 5.92E-04\n",
      "Test  loss: 7.42E-04\n",
      "\n",
      " Epoch 100 \n",
      " --------------\n",
      "Train loss: 5.90E-04\n",
      "Test  loss: 7.40E-04\n",
      "\n",
      " Epoch 101 \n",
      " --------------\n",
      "Train loss: 5.88E-04\n",
      "Test  loss: 7.38E-04\n",
      "\n",
      " Epoch 102 \n",
      " --------------\n",
      "Train loss: 5.86E-04\n",
      "Test  loss: 7.36E-04\n",
      "\n",
      " Epoch 103 \n",
      " --------------\n",
      "Train loss: 5.85E-04\n",
      "Test  loss: 7.34E-04\n",
      "\n",
      " Epoch 104 \n",
      " --------------\n",
      "Train loss: 5.83E-04\n",
      "Test  loss: 7.32E-04\n",
      "\n",
      " Epoch 105 \n",
      " --------------\n",
      "Train loss: 5.81E-04\n",
      "Test  loss: 7.30E-04\n",
      "\n",
      " Epoch 106 \n",
      " --------------\n",
      "Train loss: 5.79E-04\n",
      "Test  loss: 7.28E-04\n",
      "\n",
      " Epoch 107 \n",
      " --------------\n",
      "Train loss: 5.77E-04\n",
      "Test  loss: 7.26E-04\n",
      "\n",
      " Epoch 108 \n",
      " --------------\n",
      "Train loss: 5.75E-04\n",
      "Test  loss: 7.23E-04\n",
      "\n",
      " Epoch 109 \n",
      " --------------\n",
      "Train loss: 5.74E-04\n",
      "Test  loss: 7.22E-04\n",
      "\n",
      " Epoch 110 \n",
      " --------------\n",
      "Train loss: 5.72E-04\n",
      "Test  loss: 7.19E-04\n",
      "\n",
      " Epoch 111 \n",
      " --------------\n",
      "Train loss: 5.70E-04\n",
      "Test  loss: 7.17E-04\n",
      "\n",
      " Epoch 112 \n",
      " --------------\n",
      "Train loss: 5.68E-04\n",
      "Test  loss: 7.15E-04\n",
      "\n",
      " Epoch 113 \n",
      " --------------\n",
      "Train loss: 5.66E-04\n",
      "Test  loss: 7.13E-04\n",
      "\n",
      " Epoch 114 \n",
      " --------------\n",
      "Train loss: 5.64E-04\n",
      "Test  loss: 7.11E-04\n",
      "\n",
      " Epoch 115 \n",
      " --------------\n",
      "Train loss: 5.62E-04\n",
      "Test  loss: 7.09E-04\n",
      "\n",
      " Epoch 116 \n",
      " --------------\n",
      "Train loss: 5.60E-04\n",
      "Test  loss: 7.07E-04\n",
      "\n",
      " Epoch 117 \n",
      " --------------\n",
      "Train loss: 5.59E-04\n",
      "Test  loss: 7.05E-04\n",
      "\n",
      " Epoch 118 \n",
      " --------------\n",
      "Train loss: 5.57E-04\n",
      "Test  loss: 7.03E-04\n",
      "\n",
      " Epoch 119 \n",
      " --------------\n",
      "Train loss: 5.55E-04\n",
      "Test  loss: 7.01E-04\n",
      "\n",
      " Epoch 120 \n",
      " --------------\n",
      "Train loss: 5.53E-04\n",
      "Test  loss: 6.99E-04\n",
      "\n",
      " Epoch 121 \n",
      " --------------\n",
      "Train loss: 5.51E-04\n",
      "Test  loss: 6.97E-04\n",
      "\n",
      " Epoch 122 \n",
      " --------------\n",
      "Train loss: 5.49E-04\n",
      "Test  loss: 6.95E-04\n",
      "\n",
      " Epoch 123 \n",
      " --------------\n",
      "Train loss: 5.47E-04\n",
      "Test  loss: 6.93E-04\n",
      "\n",
      " Epoch 124 \n",
      " --------------\n",
      "Train loss: 5.46E-04\n",
      "Test  loss: 6.91E-04\n",
      "\n",
      " Epoch 125 \n",
      " --------------\n",
      "Train loss: 5.44E-04\n",
      "Test  loss: 6.89E-04\n",
      "\n",
      " Epoch 126 \n",
      " --------------\n",
      "Train loss: 5.42E-04\n",
      "Test  loss: 6.87E-04\n",
      "\n",
      " Epoch 127 \n",
      " --------------\n",
      "Train loss: 5.40E-04\n",
      "Test  loss: 6.84E-04\n",
      "\n",
      " Epoch 128 \n",
      " --------------\n",
      "Train loss: 5.38E-04\n",
      "Test  loss: 6.83E-04\n",
      "\n",
      " Epoch 129 \n",
      " --------------\n",
      "Train loss: 5.36E-04\n",
      "Test  loss: 6.81E-04\n",
      "\n",
      " Epoch 130 \n",
      " --------------\n",
      "Train loss: 5.35E-04\n",
      "Test  loss: 6.79E-04\n",
      "\n",
      " Epoch 131 \n",
      " --------------\n",
      "Train loss: 5.33E-04\n",
      "Test  loss: 6.77E-04\n",
      "\n",
      " Epoch 132 \n",
      " --------------\n",
      "Train loss: 5.31E-04\n",
      "Test  loss: 6.75E-04\n",
      "\n",
      " Epoch 133 \n",
      " --------------\n",
      "Train loss: 5.29E-04\n",
      "Test  loss: 6.73E-04\n",
      "\n",
      " Epoch 134 \n",
      " --------------\n",
      "Train loss: 5.27E-04\n",
      "Test  loss: 6.71E-04\n",
      "\n",
      " Epoch 135 \n",
      " --------------\n",
      "Train loss: 5.26E-04\n",
      "Test  loss: 6.69E-04\n",
      "\n",
      " Epoch 136 \n",
      " --------------\n",
      "Train loss: 5.24E-04\n",
      "Test  loss: 6.67E-04\n",
      "\n",
      " Epoch 137 \n",
      " --------------\n",
      "Train loss: 5.22E-04\n",
      "Test  loss: 6.65E-04\n",
      "\n",
      " Epoch 138 \n",
      " --------------\n",
      "Train loss: 5.20E-04\n",
      "Test  loss: 6.63E-04\n",
      "\n",
      " Epoch 139 \n",
      " --------------\n",
      "Train loss: 5.19E-04\n",
      "Test  loss: 6.62E-04\n",
      "\n",
      " Epoch 140 \n",
      " --------------\n",
      "Train loss: 5.17E-04\n",
      "Test  loss: 6.60E-04\n",
      "\n",
      " Epoch 141 \n",
      " --------------\n",
      "Train loss: 5.15E-04\n",
      "Test  loss: 6.58E-04\n",
      "\n",
      " Epoch 142 \n",
      " --------------\n",
      "Train loss: 5.14E-04\n",
      "Test  loss: 6.56E-04\n",
      "\n",
      " Epoch 143 \n",
      " --------------\n",
      "Train loss: 5.12E-04\n",
      "Test  loss: 6.55E-04\n",
      "\n",
      " Epoch 144 \n",
      " --------------\n",
      "Train loss: 5.10E-04\n",
      "Test  loss: 6.53E-04\n",
      "\n",
      " Epoch 145 \n",
      " --------------\n",
      "Train loss: 5.09E-04\n",
      "Test  loss: 6.51E-04\n",
      "\n",
      " Epoch 146 \n",
      " --------------\n",
      "Train loss: 5.07E-04\n",
      "Test  loss: 6.49E-04\n",
      "\n",
      " Epoch 147 \n",
      " --------------\n",
      "Train loss: 5.06E-04\n",
      "Test  loss: 6.48E-04\n",
      "\n",
      " Epoch 148 \n",
      " --------------\n",
      "Train loss: 5.04E-04\n",
      "Test  loss: 6.46E-04\n",
      "\n",
      " Epoch 149 \n",
      " --------------\n",
      "Train loss: 5.03E-04\n",
      "Test  loss: 6.45E-04\n",
      "\n",
      " Epoch 150 \n",
      " --------------\n",
      "Train loss: 5.01E-04\n",
      "Test  loss: 6.43E-04\n",
      "\n",
      " Epoch 151 \n",
      " --------------\n",
      "Train loss: 5.00E-04\n",
      "Test  loss: 6.41E-04\n",
      "\n",
      " Epoch 152 \n",
      " --------------\n",
      "Train loss: 4.98E-04\n",
      "Test  loss: 6.40E-04\n",
      "\n",
      " Epoch 153 \n",
      " --------------\n",
      "Train loss: 4.97E-04\n",
      "Test  loss: 6.38E-04\n",
      "\n",
      " Epoch 154 \n",
      " --------------\n",
      "Train loss: 4.95E-04\n",
      "Test  loss: 6.37E-04\n",
      "\n",
      " Epoch 155 \n",
      " --------------\n",
      "Train loss: 4.94E-04\n",
      "Test  loss: 6.35E-04\n",
      "\n",
      " Epoch 156 \n",
      " --------------\n",
      "Train loss: 4.92E-04\n",
      "Test  loss: 6.34E-04\n",
      "\n",
      " Epoch 157 \n",
      " --------------\n",
      "Train loss: 4.91E-04\n",
      "Test  loss: 6.32E-04\n",
      "\n",
      " Epoch 158 \n",
      " --------------\n",
      "Train loss: 4.90E-04\n",
      "Test  loss: 6.31E-04\n",
      "\n",
      " Epoch 159 \n",
      " --------------\n",
      "Train loss: 4.88E-04\n",
      "Test  loss: 6.29E-04\n",
      "\n",
      " Epoch 160 \n",
      " --------------\n",
      "Train loss: 4.87E-04\n",
      "Test  loss: 6.28E-04\n",
      "\n",
      " Epoch 161 \n",
      " --------------\n",
      "Train loss: 4.85E-04\n",
      "Test  loss: 6.27E-04\n",
      "\n",
      " Epoch 162 \n",
      " --------------\n",
      "Train loss: 4.84E-04\n",
      "Test  loss: 6.25E-04\n",
      "\n",
      " Epoch 163 \n",
      " --------------\n",
      "Train loss: 4.83E-04\n",
      "Test  loss: 6.24E-04\n",
      "\n",
      " Epoch 164 \n",
      " --------------\n",
      "Train loss: 4.81E-04\n",
      "Test  loss: 6.22E-04\n",
      "\n",
      " Epoch 165 \n",
      " --------------\n",
      "Train loss: 4.80E-04\n",
      "Test  loss: 6.21E-04\n",
      "\n",
      " Epoch 166 \n",
      " --------------\n",
      "Train loss: 4.79E-04\n",
      "Test  loss: 6.20E-04\n",
      "\n",
      " Epoch 167 \n",
      " --------------\n",
      "Train loss: 4.78E-04\n",
      "Test  loss: 6.18E-04\n",
      "\n",
      " Epoch 168 \n",
      " --------------\n",
      "Train loss: 4.76E-04\n",
      "Test  loss: 6.17E-04\n",
      "\n",
      " Epoch 169 \n",
      " --------------\n",
      "Train loss: 4.75E-04\n",
      "Test  loss: 6.16E-04\n",
      "\n",
      " Epoch 170 \n",
      " --------------\n",
      "Train loss: 4.74E-04\n",
      "Test  loss: 6.14E-04\n",
      "\n",
      " Epoch 171 \n",
      " --------------\n",
      "Train loss: 4.73E-04\n",
      "Test  loss: 6.13E-04\n",
      "\n",
      " Epoch 172 \n",
      " --------------\n",
      "Train loss: 4.72E-04\n",
      "Test  loss: 6.12E-04\n",
      "\n",
      " Epoch 173 \n",
      " --------------\n",
      "Train loss: 4.70E-04\n",
      "Test  loss: 6.11E-04\n",
      "\n",
      " Epoch 174 \n",
      " --------------\n",
      "Train loss: 4.69E-04\n",
      "Test  loss: 6.09E-04\n",
      "\n",
      " Epoch 175 \n",
      " --------------\n",
      "Train loss: 4.68E-04\n",
      "Test  loss: 6.08E-04\n",
      "\n",
      " Epoch 176 \n",
      " --------------\n",
      "Train loss: 4.67E-04\n",
      "Test  loss: 6.07E-04\n",
      "\n",
      " Epoch 177 \n",
      " --------------\n",
      "Train loss: 4.66E-04\n",
      "Test  loss: 6.06E-04\n",
      "\n",
      " Epoch 178 \n",
      " --------------\n",
      "Train loss: 4.65E-04\n",
      "Test  loss: 6.05E-04\n",
      "\n",
      " Epoch 179 \n",
      " --------------\n",
      "Train loss: 4.64E-04\n",
      "Test  loss: 6.04E-04\n",
      "\n",
      " Epoch 180 \n",
      " --------------\n",
      "Train loss: 4.63E-04\n",
      "Test  loss: 6.03E-04\n",
      "\n",
      " Epoch 181 \n",
      " --------------\n",
      "Train loss: 4.62E-04\n",
      "Test  loss: 6.02E-04\n",
      "\n",
      " Epoch 182 \n",
      " --------------\n",
      "Train loss: 4.61E-04\n",
      "Test  loss: 6.00E-04\n",
      "\n",
      " Epoch 183 \n",
      " --------------\n",
      "Train loss: 4.60E-04\n",
      "Test  loss: 5.99E-04\n",
      "\n",
      " Epoch 184 \n",
      " --------------\n",
      "Train loss: 4.59E-04\n",
      "Test  loss: 5.98E-04\n",
      "\n",
      " Epoch 185 \n",
      " --------------\n",
      "Train loss: 4.58E-04\n",
      "Test  loss: 5.97E-04\n",
      "\n",
      " Epoch 186 \n",
      " --------------\n",
      "Train loss: 4.57E-04\n",
      "Test  loss: 5.96E-04\n",
      "\n",
      " Epoch 187 \n",
      " --------------\n",
      "Train loss: 4.56E-04\n",
      "Test  loss: 5.96E-04\n",
      "\n",
      " Epoch 188 \n",
      " --------------\n",
      "Train loss: 4.55E-04\n",
      "Test  loss: 5.95E-04\n",
      "\n",
      " Epoch 189 \n",
      " --------------\n",
      "Train loss: 4.54E-04\n",
      "Test  loss: 5.94E-04\n",
      "\n",
      " Epoch 190 \n",
      " --------------\n",
      "Train loss: 4.53E-04\n",
      "Test  loss: 5.93E-04\n",
      "\n",
      " Epoch 191 \n",
      " --------------\n",
      "Train loss: 4.52E-04\n",
      "Test  loss: 5.92E-04\n",
      "\n",
      " Epoch 192 \n",
      " --------------\n",
      "Train loss: 4.52E-04\n",
      "Test  loss: 5.91E-04\n",
      "\n",
      " Epoch 193 \n",
      " --------------\n",
      "Train loss: 4.51E-04\n",
      "Test  loss: 5.90E-04\n",
      "\n",
      " Epoch 194 \n",
      " --------------\n",
      "Train loss: 4.50E-04\n",
      "Test  loss: 5.89E-04\n",
      "\n",
      " Epoch 195 \n",
      " --------------\n",
      "Train loss: 4.49E-04\n",
      "Test  loss: 5.89E-04\n",
      "\n",
      " Epoch 196 \n",
      " --------------\n",
      "Train loss: 4.48E-04\n",
      "Test  loss: 5.88E-04\n",
      "\n",
      " Epoch 197 \n",
      " --------------\n",
      "Train loss: 4.48E-04\n",
      "Test  loss: 5.87E-04\n",
      "\n",
      " Epoch 198 \n",
      " --------------\n",
      "Train loss: 4.47E-04\n",
      "Test  loss: 5.86E-04\n",
      "\n",
      " Epoch 199 \n",
      " --------------\n",
      "Train loss: 4.46E-04\n",
      "Test  loss: 5.85E-04\n",
      "\n",
      " Epoch 200 \n",
      " --------------\n",
      "Train loss: 4.46E-04\n",
      "Test  loss: 5.85E-04\n",
      "\n",
      " Epoch 201 \n",
      " --------------\n",
      "Train loss: 4.45E-04\n",
      "Test  loss: 5.84E-04\n",
      "\n",
      " Epoch 202 \n",
      " --------------\n",
      "Train loss: 4.44E-04\n",
      "Test  loss: 5.83E-04\n",
      "\n",
      " Epoch 203 \n",
      " --------------\n",
      "Train loss: 4.44E-04\n",
      "Test  loss: 5.83E-04\n",
      "\n",
      " Epoch 204 \n",
      " --------------\n",
      "Train loss: 4.43E-04\n",
      "Test  loss: 5.82E-04\n",
      "\n",
      " Epoch 205 \n",
      " --------------\n",
      "Train loss: 4.43E-04\n",
      "Test  loss: 5.81E-04\n",
      "\n",
      " Epoch 206 \n",
      " --------------\n",
      "Train loss: 4.42E-04\n",
      "Test  loss: 5.81E-04\n",
      "\n",
      " Epoch 207 \n",
      " --------------\n",
      "Train loss: 4.41E-04\n",
      "Test  loss: 5.80E-04\n",
      "\n",
      " Epoch 208 \n",
      " --------------\n",
      "Train loss: 4.41E-04\n",
      "Test  loss: 5.80E-04\n",
      "\n",
      " Epoch 209 \n",
      " --------------\n",
      "Train loss: 4.40E-04\n",
      "Test  loss: 5.79E-04\n",
      "\n",
      " Epoch 210 \n",
      " --------------\n",
      "Train loss: 4.40E-04\n",
      "Test  loss: 5.79E-04\n",
      "\n",
      " Epoch 211 \n",
      " --------------\n",
      "Train loss: 4.39E-04\n",
      "Test  loss: 5.78E-04\n",
      "\n",
      " Epoch 212 \n",
      " --------------\n",
      "Train loss: 4.39E-04\n",
      "Test  loss: 5.78E-04\n",
      "\n",
      " Epoch 213 \n",
      " --------------\n",
      "Train loss: 4.39E-04\n",
      "Test  loss: 5.77E-04\n",
      "\n",
      " Epoch 214 \n",
      " --------------\n",
      "Train loss: 4.38E-04\n",
      "Test  loss: 5.77E-04\n",
      "\n",
      " Epoch 215 \n",
      " --------------\n",
      "Train loss: 4.38E-04\n",
      "Test  loss: 5.76E-04\n",
      "\n",
      " Epoch 216 \n",
      " --------------\n",
      "Train loss: 4.37E-04\n",
      "Test  loss: 5.76E-04\n",
      "\n",
      " Epoch 217 \n",
      " --------------\n",
      "Train loss: 4.37E-04\n",
      "Test  loss: 5.75E-04\n",
      "\n",
      " Epoch 218 \n",
      " --------------\n",
      "Train loss: 4.36E-04\n",
      "Test  loss: 5.75E-04\n",
      "\n",
      " Epoch 219 \n",
      " --------------\n",
      "Train loss: 4.36E-04\n",
      "Test  loss: 5.75E-04\n",
      "\n",
      " Epoch 220 \n",
      " --------------\n",
      "Train loss: 4.36E-04\n",
      "Test  loss: 5.74E-04\n",
      "\n",
      " Epoch 221 \n",
      " --------------\n",
      "Train loss: 4.35E-04\n",
      "Test  loss: 5.74E-04\n",
      "\n",
      " Epoch 222 \n",
      " --------------\n",
      "Train loss: 4.35E-04\n",
      "Test  loss: 5.73E-04\n",
      "\n",
      " Epoch 223 \n",
      " --------------\n",
      "Train loss: 4.35E-04\n",
      "Test  loss: 5.73E-04\n",
      "\n",
      " Epoch 224 \n",
      " --------------\n",
      "Train loss: 4.34E-04\n",
      "Test  loss: 5.73E-04\n",
      "\n",
      " Epoch 225 \n",
      " --------------\n",
      "Train loss: 4.34E-04\n",
      "Test  loss: 5.72E-04\n",
      "\n",
      " Epoch 226 \n",
      " --------------\n",
      "Train loss: 4.34E-04\n",
      "Test  loss: 5.72E-04\n",
      "\n",
      " Epoch 227 \n",
      " --------------\n",
      "Train loss: 4.33E-04\n",
      "Test  loss: 5.72E-04\n",
      "\n",
      " Epoch 228 \n",
      " --------------\n",
      "Train loss: 4.33E-04\n",
      "Test  loss: 5.71E-04\n",
      "\n",
      " Epoch 229 \n",
      " --------------\n",
      "Train loss: 4.33E-04\n",
      "Test  loss: 5.71E-04\n",
      "\n",
      " Epoch 230 \n",
      " --------------\n",
      "Train loss: 4.33E-04\n",
      "Test  loss: 5.71E-04\n",
      "\n",
      " Epoch 231 \n",
      " --------------\n",
      "Train loss: 4.32E-04\n",
      "Test  loss: 5.71E-04\n",
      "\n",
      " Epoch 232 \n",
      " --------------\n",
      "Train loss: 4.32E-04\n",
      "Test  loss: 5.70E-04\n",
      "\n",
      " Epoch 233 \n",
      " --------------\n",
      "Train loss: 4.32E-04\n",
      "Test  loss: 5.70E-04\n",
      "\n",
      " Epoch 234 \n",
      " --------------\n",
      "Train loss: 4.32E-04\n",
      "Test  loss: 5.70E-04\n",
      "\n",
      " Epoch 235 \n",
      " --------------\n",
      "Train loss: 4.31E-04\n",
      "Test  loss: 5.69E-04\n",
      "\n",
      " Epoch 236 \n",
      " --------------\n",
      "Train loss: 4.31E-04\n",
      "Test  loss: 5.69E-04\n",
      "\n",
      " Epoch 237 \n",
      " --------------\n",
      "Train loss: 4.31E-04\n",
      "Test  loss: 5.69E-04\n",
      "\n",
      " Epoch 238 \n",
      " --------------\n",
      "Train loss: 4.31E-04\n",
      "Test  loss: 5.69E-04\n",
      "\n",
      " Epoch 239 \n",
      " --------------\n",
      "Train loss: 4.31E-04\n",
      "Test  loss: 5.69E-04\n",
      "\n",
      " Epoch 240 \n",
      " --------------\n",
      "Train loss: 4.30E-04\n",
      "Test  loss: 5.68E-04\n",
      "\n",
      " Epoch 241 \n",
      " --------------\n",
      "Train loss: 4.30E-04\n",
      "Test  loss: 5.68E-04\n",
      "\n",
      " Epoch 242 \n",
      " --------------\n",
      "Train loss: 4.30E-04\n",
      "Test  loss: 5.68E-04\n",
      "\n",
      " Epoch 243 \n",
      " --------------\n",
      "Train loss: 4.30E-04\n",
      "Test  loss: 5.68E-04\n",
      "\n",
      " Epoch 244 \n",
      " --------------\n",
      "Train loss: 4.30E-04\n",
      "Test  loss: 5.68E-04\n",
      "\n",
      " Epoch 245 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.67E-04\n",
      "\n",
      " Epoch 246 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.67E-04\n",
      "\n",
      " Epoch 247 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.67E-04\n",
      "\n",
      " Epoch 248 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.67E-04\n",
      "\n",
      " Epoch 249 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.67E-04\n",
      "\n",
      " Epoch 250 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.66E-04\n",
      "\n",
      " Epoch 251 \n",
      " --------------\n",
      "Train loss: 4.29E-04\n",
      "Test  loss: 5.66E-04\n",
      "\n",
      " Epoch 252 \n",
      " --------------\n",
      "Train loss: 4.28E-04\n",
      "Test  loss: 5.66E-04\n",
      "\n",
      " Epoch 253 \n",
      " --------------\n",
      "Train loss: 4.28E-04\n",
      "Test  loss: 5.66E-04\n",
      "\n",
      " Epoch 254 \n",
      " --------------\n",
      "Train loss: 4.28E-04\n",
      "Test  loss: 5.66E-04\n",
      "\n",
      " Epoch 255 \n",
      " --------------\n",
      "Train loss: 4.28E-04\n",
      "Test  loss: 5.65E-04\n",
      "\n",
      " Epoch 256 \n",
      " --------------\n",
      "Train loss: 4.28E-04\n",
      "Test  loss: 5.65E-04\n",
      "\n",
      " Epoch 257 \n",
      " --------------\n",
      "Train loss: 4.28E-04\n",
      "Test  loss: 5.65E-04\n",
      "\n",
      " Epoch 258 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.65E-04\n",
      "\n",
      " Epoch 259 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.65E-04\n",
      "\n",
      " Epoch 260 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.65E-04\n",
      "\n",
      " Epoch 261 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.64E-04\n",
      "\n",
      " Epoch 262 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.64E-04\n",
      "\n",
      " Epoch 263 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.64E-04\n",
      "\n",
      " Epoch 264 \n",
      " --------------\n",
      "Train loss: 4.27E-04\n",
      "Test  loss: 5.64E-04\n",
      "\n",
      " Epoch 265 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.64E-04\n",
      "\n",
      " Epoch 266 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.64E-04\n",
      "\n",
      " Epoch 267 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 268 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 269 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 270 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 271 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 272 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 273 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 274 \n",
      " --------------\n",
      "Train loss: 4.26E-04\n",
      "Test  loss: 5.63E-04\n",
      "\n",
      " Epoch 275 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 276 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 277 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 278 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 279 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 280 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 281 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 282 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 283 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.62E-04\n",
      "\n",
      " Epoch 284 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 285 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 286 \n",
      " --------------\n",
      "Train loss: 4.25E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 287 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 288 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 289 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 290 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 291 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 292 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.61E-04\n",
      "\n",
      " Epoch 293 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 294 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 295 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 296 \n",
      " --------------\n",
      "Train loss: 4.24E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 297 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 298 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 299 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 300 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.60E-04\n",
      "\n",
      " Epoch 301 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 302 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 303 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 304 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 305 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 306 \n",
      " --------------\n",
      "Train loss: 4.23E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 307 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.59E-04\n",
      "\n",
      " Epoch 308 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 309 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 310 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 311 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 312 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 313 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 314 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 315 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.58E-04\n",
      "\n",
      " Epoch 316 \n",
      " --------------\n",
      "Train loss: 4.22E-04\n",
      "Test  loss: 5.57E-04\n",
      "\n",
      " Epoch 317 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.57E-04\n",
      "\n",
      " Epoch 318 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.57E-04\n",
      "\n",
      " Epoch 319 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.57E-04\n",
      "\n",
      " Epoch 320 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.57E-04\n",
      "\n",
      " Epoch 321 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.57E-04\n",
      "\n",
      " Epoch 322 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 323 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 324 \n",
      " --------------\n",
      "Train loss: 4.21E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 325 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 326 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 327 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 328 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 329 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.56E-04\n",
      "\n",
      " Epoch 330 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.55E-04\n",
      "\n",
      " Epoch 331 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.55E-04\n",
      "\n",
      " Epoch 332 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.55E-04\n",
      "\n",
      " Epoch 333 \n",
      " --------------\n",
      "Train loss: 4.20E-04\n",
      "Test  loss: 5.55E-04\n",
      "\n",
      " Epoch 334 \n",
      " --------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:287\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n\u001b[0;32m    285\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    286\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 287\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    289\u001b[0m \u001b[39m# If we want to report progress during training (not recommended - obstructs view)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m report_progress:\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:351\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    354\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f3a92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.report_training(\"NNEOS_tab_experiments.csv\", comment = \"Output now also has cs2, previous did not.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "296ab01e",
   "metadata": {},
   "source": [
    "Create a quick sketch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8056faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n",
    "# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n",
    "\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# for ind in trainer.adaptation_indices:\n",
    "#     plt.axvline(ind, ls = '--', color='grey')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"MSE Loss\")\n",
    "# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n",
    "# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "377b0cbf",
   "metadata": {},
   "source": [
    "# Second goal: NNC2P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf95fa58",
   "metadata": {},
   "source": [
    "__TO DO__ think about design of architecture AND fix the conserved variable values -- I think they were not computed correctly before! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b454181",
   "metadata": {},
   "source": [
    "__NNC2P__: try to replicate the full C2P conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10571f",
   "metadata": {},
   "source": [
    "Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"rho\", \"eps\", \"ye\"]\n",
    "out_vars = [\"temp\"]\n",
    "# For normalization, use sklearn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e96de6",
   "metadata": {},
   "source": [
    "Create a new instance of the Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08aa592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=50, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Net(nb_of_inputs = 3, nb_of_outputs = 1, h=[50, 50])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504399cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83cead",
   "metadata": {},
   "source": [
    "Create a trainer object from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-1, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0369872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 7.59E-01\n",
      "Test  loss: 7.62E-01\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 9.92E-01\n",
      "Test  loss: 9.98E-01\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 9.45E-01\n",
      "Test  loss: 9.61E-01\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 8.24E-01\n",
      "Test  loss: 8.29E-01\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 9.10E-01\n",
      "Test  loss: 9.16E-01\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.91E-01\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 8.62E-01\n",
      "Test  loss: 8.67E-01\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 8.20E-01\n",
      "Test  loss: 8.27E-01\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.94E-01\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 9.20E-01\n",
      "Test  loss: 9.26E-01\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 7.34E-01\n",
      "Test  loss: 7.39E-01\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 7.48E-01\n",
      "Test  loss: 7.56E-01\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Train loss: 7.28E-01\n",
      "Test  loss: 7.31E-01\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 8.87E-01\n",
      "Test  loss: 8.99E-01\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.09E-01\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 8.11E-01\n",
      "Test  loss: 8.20E-01\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.08E-01\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Adapting learning rate to 0.05\n",
      "Train loss: 8.70E-01\n",
      "Test  loss: 8.79E-01\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 6.88E-01\n",
      "Test  loss: 6.92E-01\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 6.24E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 6.42E-01\n",
      "Test  loss: 6.46E-01\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Train loss: 6.21E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 6.29E-01\n",
      "Test  loss: 6.32E-01\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Train loss: 6.30E-01\n",
      "Test  loss: 6.33E-01\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 6.13E-01\n",
      "Test  loss: 6.16E-01\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Train loss: 6.25E-01\n",
      "Test  loss: 6.27E-01\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 6.75E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 6.22E-01\n",
      "Test  loss: 6.23E-01\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 6.77E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Adapting learning rate to 0.025\n",
      "Train loss: 6.98E-01\n",
      "Test  loss: 6.99E-01\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 5.71E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 5.77E-01\n",
      "Test  loss: 5.81E-01\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 5.65E-01\n",
      "Test  loss: 5.69E-01\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Train loss: 5.63E-01\n",
      "Test  loss: 5.67E-01\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 5.60E-01\n",
      "Test  loss: 5.65E-01\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 5.67E-01\n",
      "Test  loss: 5.70E-01\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 5.58E-01\n",
      "Test  loss: 5.60E-01\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.64E-01\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 5.73E-01\n",
      "Test  loss: 5.75E-01\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Adapting learning rate to 0.0125\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.73E-01\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 4.89E-01\n",
      "Test  loss: 4.92E-01\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 4.87E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 4.86E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 4.83E-01\n",
      "Test  loss: 4.88E-01\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 4.80E-01\n",
      "Test  loss: 4.85E-01\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 4.78E-01\n",
      "Test  loss: 4.83E-01\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 4.77E-01\n",
      "Test  loss: 4.81E-01\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 4.75E-01\n",
      "Test  loss: 4.79E-01\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n",
      "\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n",
      "\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n",
      "\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:277\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n",
      "\u001b[0;32m    274\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n",
      "\u001b[0;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n",
      "\u001b[0;32m    276\u001b[0m     \u001b[39m# Compute prediction \u001b[39;00m\n",
      "\u001b[1;32m--> 277\u001b[0m     prediction \u001b[39m=\u001b[39m model(X)\n",
      "\u001b[0;32m    278\u001b[0m     \u001b[39m# In case we use C2P loss function, have to provide conserved variables for the loss computation\u001b[39;00m\n",
      "\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m use_c2p_loss:\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m     38\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n",
      "\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a5288",
   "metadata": {},
   "source": [
    "Create a quick sketch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07eb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n",
    "# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n",
    "\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# for ind in trainer.adaptation_indices:\n",
    "#     plt.axvline(ind, ls = '--', color='grey')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"MSE Loss\")\n",
    "# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n",
    "# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ad41d2a",
   "metadata": {},
   "source": [
    "# Archive: NNE2T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391a25b5",
   "metadata": {},
   "source": [
    "__NNE2T__: try to replicate the conversion from energy to temperature, which is currently done by rootfinding approximations & lookups in the EOS table (see Gmunu code). It seemed harder than I initially thought to model and train this; and in the end, I'm not sure how useful it'll be, so I'm archiving this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48e387",
   "metadata": {},
   "source": [
    "Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b88f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"rho\", \"eps\", \"ye\"]\n",
    "out_vars = [\"temp\"]\n",
    "# For normalization, use sklearn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abee61f0",
   "metadata": {},
   "source": [
    "Create a new instance of the Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b990a7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=50, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(nb_of_inputs = 3, nb_of_outputs = 1, h=[50, 50])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "312052a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23d022",
   "metadata": {},
   "source": [
    "Create a trainer object from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06becda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-1, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "93ce3bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 7.59E-01\n",
      "Test  loss: 7.62E-01\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 9.92E-01\n",
      "Test  loss: 9.98E-01\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 9.45E-01\n",
      "Test  loss: 9.61E-01\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 8.24E-01\n",
      "Test  loss: 8.29E-01\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 9.10E-01\n",
      "Test  loss: 9.16E-01\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.91E-01\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 8.62E-01\n",
      "Test  loss: 8.67E-01\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 8.20E-01\n",
      "Test  loss: 8.27E-01\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.94E-01\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 9.20E-01\n",
      "Test  loss: 9.26E-01\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 7.34E-01\n",
      "Test  loss: 7.39E-01\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 7.48E-01\n",
      "Test  loss: 7.56E-01\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Train loss: 7.28E-01\n",
      "Test  loss: 7.31E-01\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 8.87E-01\n",
      "Test  loss: 8.99E-01\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.09E-01\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 8.11E-01\n",
      "Test  loss: 8.20E-01\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.08E-01\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Adapting learning rate to 0.05\n",
      "Train loss: 8.70E-01\n",
      "Test  loss: 8.79E-01\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 6.88E-01\n",
      "Test  loss: 6.92E-01\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 6.24E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 6.42E-01\n",
      "Test  loss: 6.46E-01\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Train loss: 6.21E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 6.29E-01\n",
      "Test  loss: 6.32E-01\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Train loss: 6.30E-01\n",
      "Test  loss: 6.33E-01\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 6.13E-01\n",
      "Test  loss: 6.16E-01\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Train loss: 6.25E-01\n",
      "Test  loss: 6.27E-01\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 6.75E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 6.22E-01\n",
      "Test  loss: 6.23E-01\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 6.77E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Adapting learning rate to 0.025\n",
      "Train loss: 6.98E-01\n",
      "Test  loss: 6.99E-01\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 5.71E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 5.77E-01\n",
      "Test  loss: 5.81E-01\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 5.65E-01\n",
      "Test  loss: 5.69E-01\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Train loss: 5.63E-01\n",
      "Test  loss: 5.67E-01\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 5.60E-01\n",
      "Test  loss: 5.65E-01\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 5.67E-01\n",
      "Test  loss: 5.70E-01\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 5.58E-01\n",
      "Test  loss: 5.60E-01\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.64E-01\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 5.73E-01\n",
      "Test  loss: 5.75E-01\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Adapting learning rate to 0.0125\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.73E-01\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 4.89E-01\n",
      "Test  loss: 4.92E-01\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 4.87E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 4.86E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 4.83E-01\n",
      "Test  loss: 4.88E-01\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 4.80E-01\n",
      "Test  loss: 4.85E-01\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 4.78E-01\n",
      "Test  loss: 4.83E-01\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 4.77E-01\n",
      "Test  loss: 4.81E-01\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 4.75E-01\n",
      "Test  loss: 4.79E-01\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:277\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n\u001b[0;32m    274\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m    276\u001b[0m     \u001b[39m# Compute prediction \u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     prediction \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m    278\u001b[0m     \u001b[39m# In case we use C2P loss function, have to provide conserved variables for the loss computation\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m use_c2p_loss:\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2a130",
   "metadata": {},
   "source": [
    "Create a quick sketch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5338413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n",
    "# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n",
    "\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# for ind in trainer.adaptation_indices:\n",
    "#     plt.axvline(ind, ls = '--', color='grey')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"MSE Loss\")\n",
    "# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n",
    "# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "author": "Thibeau Wouters",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
