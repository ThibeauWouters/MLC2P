{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bb9f224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import h5py\n",
    "# Scikit learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize \n",
    "# Own scripts:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import physics\n",
    "import data\n",
    "import nnc2p\n",
    "# from nnc2p import NeuralNetwork # our own architecture\n",
    "# Get dirs\n",
    "import os\n",
    "cwd = os.getcwd()# \"Code\" folder\n",
    "master_dir = os.path.abspath(os.path.join(cwd, \"..\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3393c80",
   "metadata": {},
   "source": [
    "Point towards the folder where we store the eos tables (__Note:__ they are not in the Github as these are very large files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbc8d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to look for EOS table at D:/Coding/Datasets/eos_tables\n"
     ]
    }
   ],
   "source": [
    "eos_tables_folder = os.path.join(\"D:/Coding/Datasets/eos_tables\")\n",
    "print(f\"Going to look for EOS table at {eos_tables_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb3c8c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96ab265e",
   "metadata": {},
   "source": [
    "Here, we try to find a way to generalize the NN approach from the first semester to the situation of tabular EOS. More work coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08ad4a",
   "metadata": {},
   "source": [
    "# Exploring EOS tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5e6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the downloaded EOS tables here\n",
    "first_table_filename       = \"LS180_234r_136t_50y_analmu_20091212_SVNr26.h5\"\n",
    "second_table_filename = \"GShen_NL3EOS_rho280_temp180_ye52_version_1.1_20120817.h5\"\n",
    "third_table_filename      = \"SLy4_0000_rho391_temp163_ye66.h5\"\n",
    "# Then specify which we are going to use here\n",
    "eos_table_filename = third_table_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8f465",
   "metadata": {},
   "source": [
    "Read in the SLy4 EOS table using our py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913d9582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This EOS table has dimensions 66 x 163 x 391\n"
     ]
    }
   ],
   "source": [
    "eos_table = physics.read_eos_table(os.path.join(eos_tables_folder, eos_table_filename))\n",
    "dim_ye, dim_temp, dim_rho = eos_table[\"pointsye\"][()][0], eos_table[\"pointstemp\"][()][0], eos_table[\"pointsrho\"][()][0]\n",
    "print(f\"This EOS table has dimensions {dim_ye} x {dim_temp} x {dim_rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d38366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 163, 391)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(eos_table[\"logenergy\"][()])  # same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74b0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ye 0.005, log temp -3.0, log rho 3.0239960056064277, we have log p: 17.99956975587081.\n"
     ]
    }
   ],
   "source": [
    "# Small test to see the output of the EOS table\n",
    "test_ye = eos_table[\"ye\"][()][0]\n",
    "test_temp = eos_table[\"logtemp\"][()][0]\n",
    "test_rho = eos_table[\"logrho\"][()][0]\n",
    "test_press = eos_table[\"logpress\"][()][0, 0, 0]\n",
    "print(f\"For ye {test_ye}, log temp {test_temp}, log rho {test_rho}, we have log p: {test_press}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0167f31",
   "metadata": {},
   "source": [
    "See what is inside this EOS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "316b6963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abar', 'Albar', 'MERGE-space.in', 'MERGE-src.tar.gz', 'MERGE-tables.in', 'MERGE-transition.in', 'SNA-skyrme.in', 'SNA-space.in', 'SNA-src.tar.gz', 'Xa', 'Xh', 'Xl', 'Xn', 'Xp', 'Zbar', 'Zlbar', 'cs2', 'dedt', 'dpderho', 'dpdrhoe', 'energy_shift', 'entropy', 'gamma', 'have_rel_cs2', 'logenergy', 'logpress', 'logrho', 'logtemp', 'meffn', 'meffp', 'mu_e', 'mu_n', 'mu_p', 'muhat', 'munu', 'pointsrho', 'pointstemp', 'pointsye', 'r', 'u', 'ye']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# Iterate over keys and save them to list for simplified viewing\n",
    "keys = []\n",
    "for key in eos_table:\n",
    "    keys.append(key)\n",
    "print(keys)\n",
    "print(len(keys))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15e58b",
   "metadata": {},
   "source": [
    "# Generating training data by sampling from EOS table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc8fc6d3",
   "metadata": {},
   "source": [
    "To generate new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511ae7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = physics.generate_tabular_data(eos_table, number_of_points = 100000, save_name = \"SLy4_training_data\")\n",
    "# dat = physics.generate_tabular_data(eos_table, number_of_points = 20000, save_name = \"SLy4_test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a121af",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f929937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>eps</th>\n",
       "      <th>v</th>\n",
       "      <th>temp</th>\n",
       "      <th>ye</th>\n",
       "      <th>p</th>\n",
       "      <th>D</th>\n",
       "      <th>S</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.590663</td>\n",
       "      <td>19.381445</td>\n",
       "      <td>0.683568</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.045</td>\n",
       "      <td>31.985334</td>\n",
       "      <td>18.620245</td>\n",
       "      <td>580.748110</td>\n",
       "      <td>655.358209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.457329</td>\n",
       "      <td>28.786040</td>\n",
       "      <td>0.041886</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>34.766248</td>\n",
       "      <td>6.463001</td>\n",
       "      <td>13.269992</td>\n",
       "      <td>186.431092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.323996</td>\n",
       "      <td>19.654200</td>\n",
       "      <td>0.558550</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.535</td>\n",
       "      <td>30.418591</td>\n",
       "      <td>13.652070</td>\n",
       "      <td>310.329455</td>\n",
       "      <td>393.570454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.723996</td>\n",
       "      <td>19.246855</td>\n",
       "      <td>0.009527</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>26.341137</td>\n",
       "      <td>8.724392</td>\n",
       "      <td>2.749496</td>\n",
       "      <td>167.935288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.523996</td>\n",
       "      <td>29.186040</td>\n",
       "      <td>0.334705</td>\n",
       "      <td>2.033333</td>\n",
       "      <td>0.095</td>\n",
       "      <td>34.232914</td>\n",
       "      <td>5.862105</td>\n",
       "      <td>103.365923</td>\n",
       "      <td>195.482542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>3.323996</td>\n",
       "      <td>25.652376</td>\n",
       "      <td>0.577186</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.435</td>\n",
       "      <td>28.498601</td>\n",
       "      <td>4.070468</td>\n",
       "      <td>125.881369</td>\n",
       "      <td>157.178893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>9.757329</td>\n",
       "      <td>19.124371</td>\n",
       "      <td>0.376611</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>27.371010</td>\n",
       "      <td>10.532847</td>\n",
       "      <td>140.768515</td>\n",
       "      <td>238.842241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>9.590663</td>\n",
       "      <td>19.114310</td>\n",
       "      <td>0.266570</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>0.305</td>\n",
       "      <td>27.176186</td>\n",
       "      <td>9.950723</td>\n",
       "      <td>90.427775</td>\n",
       "      <td>207.064143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>14.690663</td>\n",
       "      <td>20.976110</td>\n",
       "      <td>0.476948</td>\n",
       "      <td>2.233333</td>\n",
       "      <td>0.485</td>\n",
       "      <td>35.391058</td>\n",
       "      <td>16.714229</td>\n",
       "      <td>326.155098</td>\n",
       "      <td>461.688397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>7.590663</td>\n",
       "      <td>19.036140</td>\n",
       "      <td>0.589693</td>\n",
       "      <td>-2.133333</td>\n",
       "      <td>0.455</td>\n",
       "      <td>24.684220</td>\n",
       "      <td>9.398714</td>\n",
       "      <td>224.588620</td>\n",
       "      <td>275.127114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rho        eps         v      temp     ye          p          D  \\\n",
       "0      13.590663  19.381445  0.683568 -0.333333  0.045  31.985334  18.620245   \n",
       "1       6.457329  28.786040  0.041886  2.166667  0.545  34.766248   6.463001   \n",
       "2      11.323996  19.654200  0.558550  0.866667  0.535  30.418591  13.652070   \n",
       "3       8.723996  19.246855  0.009527 -0.300000  0.095  26.341137   8.724392   \n",
       "4       5.523996  29.186040  0.334705  2.033333  0.095  34.232914   5.862105   \n",
       "...          ...        ...       ...       ...    ...        ...        ...   \n",
       "99995   3.323996  25.652376  0.577186  0.600000  0.435  28.498601   4.070468   \n",
       "99996   9.757329  19.124371  0.376611 -2.900000  0.295  27.371010  10.532847   \n",
       "99997   9.590663  19.114310  0.266570 -0.833333  0.305  27.176186   9.950723   \n",
       "99998  14.690663  20.976110  0.476948  2.233333  0.485  35.391058  16.714229   \n",
       "99999   7.590663  19.036140  0.589693 -2.133333  0.455  24.684220   9.398714   \n",
       "\n",
       "                S         tau  \n",
       "0      580.748110  655.358209  \n",
       "1       13.269992  186.431092  \n",
       "2      310.329455  393.570454  \n",
       "3        2.749496  167.935288  \n",
       "4      103.365923  195.482542  \n",
       "...           ...         ...  \n",
       "99995  125.881369  157.178893  \n",
       "99996  140.768515  238.842241  \n",
       "99997   90.427775  207.064143  \n",
       "99998  326.155098  461.688397  \n",
       "99999  224.588620  275.127114  \n",
       "\n",
       "[100000 rows x 9 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f2fce3d",
   "metadata": {},
   "source": [
    "The network architecture we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fd513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple feedforward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_of_inputs: int = 3, nb_of_outputs: int = 1, h: list = [600, 200], reg: bool = False, activation_function = torch.nn.Sigmoid) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network class.\n",
    "        \"\"\"\n",
    "        # Call the super constructor first\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # For convenience, save the sizes of the hidden layers as fields as well\n",
    "        self.h = h\n",
    "        # Add visible layers as well: input is 3D and output is 1D\n",
    "        self.h_augmented = [nb_of_inputs] + h + [nb_of_outputs]\n",
    "\n",
    "        # Add field to specify whether or not we do regularization\n",
    "        self.regularization = reg\n",
    "\n",
    "        # Define the layers:\n",
    "        for i in range(len(self.h_augmented)-1):\n",
    "            if i == len(self.h_augmented)-2:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1], bias=False))\n",
    "            else:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1]))\n",
    "                setattr(self, f\"activation{i+1}\", activation_function())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward step given the input x.\n",
    "        :param x: Input for the neural network.\n",
    "        :return: x: Output neural network\n",
    "        \"\"\"\n",
    "\n",
    "        for i, module in enumerate(self.modules()):\n",
    "            # The first module is the whole NNC2P object, continue\n",
    "            if i == 0:\n",
    "                continue\n",
    "            x = module(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fd2f127",
   "metadata": {},
   "source": [
    "# First goal: NNEOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5850152",
   "metadata": {},
   "source": [
    "__NNEOS__: try to replicate the EOS table (at least the core variables we are interested in) using the \"input\" variables rho, temp, ye."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec6d6047",
   "metadata": {},
   "source": [
    "Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a3a91d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"rho\", \"temp\", \"ye\"]\n",
    "out_vars = [\"eps\", \"p\"]\n",
    "# For normalization, use sklearn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f12e532a",
   "metadata": {},
   "source": [
    "Create a new instance of the Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "be7a388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=100, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (activation3): Sigmoid()\n",
       "  (linear4): Linear(in_features=100, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(nb_of_inputs = len(in_vars), nb_of_outputs = len(out_vars), h=[100, 100, 100])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5a0e1f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20800"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e3745",
   "metadata": {},
   "source": [
    "Create a trainer object from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "df92e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-2, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53ce8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 4.44E-01\n",
      "Test  loss: 4.45E-01\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 7.15E-03\n",
      "Test  loss: 7.16E-03\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 1.26E-02\n",
      "Test  loss: 1.24E-02\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 9.31E-03\n",
      "Test  loss: 9.44E-03\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 1.58E-02\n",
      "Test  loss: 1.60E-02\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 1.10E-02\n",
      "Test  loss: 1.10E-02\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 7.63E-03\n",
      "Test  loss: 7.65E-03\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 2.66E-03\n",
      "Test  loss: 2.69E-03\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 8.83E-03\n",
      "Test  loss: 8.72E-03\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 4.94E-03\n",
      "Test  loss: 4.94E-03\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 4.90E-03\n",
      "Test  loss: 4.91E-03\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 5.38E-03\n",
      "Test  loss: 5.39E-03\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Adapting learning rate to 0.005\n",
      "Train loss: 2.22E-02\n",
      "Test  loss: 2.23E-02\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 1.47E-03\n",
      "Test  loss: 1.47E-03\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 1.39E-03\n",
      "Test  loss: 1.38E-03\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 1.33E-03\n",
      "Test  loss: 1.33E-03\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 1.07E-03\n",
      "Test  loss: 1.07E-03\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Train loss: 9.27E-03\n",
      "Test  loss: 9.21E-03\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 6.62E-03\n",
      "Test  loss: 6.57E-03\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 1.53E-03\n",
      "Test  loss: 1.52E-03\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 1.47E-03\n",
      "Test  loss: 1.46E-03\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Train loss: 2.04E-03\n",
      "Test  loss: 2.03E-03\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 1.45E-03\n",
      "Test  loss: 1.46E-03\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Adapting learning rate to 0.0025\n",
      "Train loss: 1.22E-03\n",
      "Test  loss: 1.22E-03\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 5.75E-04\n",
      "Test  loss: 5.81E-04\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Train loss: 4.96E-04\n",
      "Test  loss: 5.01E-04\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 5.08E-04\n",
      "Test  loss: 5.13E-04\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 6.09E-04\n",
      "Test  loss: 6.14E-04\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 7.06E-04\n",
      "Test  loss: 7.12E-04\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Train loss: 8.03E-04\n",
      "Test  loss: 8.09E-04\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 8.72E-04\n",
      "Test  loss: 8.78E-04\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 9.00E-04\n",
      "Test  loss: 9.07E-04\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 9.05E-04\n",
      "Test  loss: 9.12E-04\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 9.26E-04\n",
      "Test  loss: 9.32E-04\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Adapting learning rate to 0.00125\n",
      "Train loss: 9.07E-04\n",
      "Test  loss: 9.12E-04\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 1.98E-04\n",
      "Test  loss: 2.02E-04\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 1.97E-04\n",
      "Test  loss: 2.01E-04\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 1.93E-04\n",
      "Test  loss: 1.97E-04\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 1.90E-04\n",
      "Test  loss: 1.94E-04\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 1.86E-04\n",
      "Test  loss: 1.90E-04\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 1.83E-04\n",
      "Test  loss: 1.87E-04\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 1.80E-04\n",
      "Test  loss: 1.84E-04\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Train loss: 1.77E-04\n",
      "Test  loss: 1.80E-04\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 1.72E-04\n",
      "Test  loss: 1.76E-04\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 1.68E-04\n",
      "Test  loss: 1.72E-04\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 1.63E-04\n",
      "Test  loss: 1.67E-04\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 1.58E-04\n",
      "Test  loss: 1.62E-04\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 1.56E-04\n",
      "Test  loss: 1.60E-04\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 1.53E-04\n",
      "Test  loss: 1.57E-04\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 1.49E-04\n",
      "Test  loss: 1.53E-04\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 1.46E-04\n",
      "Test  loss: 1.50E-04\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n",
      "Train loss: 1.44E-04\n",
      "Test  loss: 1.48E-04\n",
      "\n",
      " Epoch 52 \n",
      " --------------\n",
      "Train loss: 1.43E-04\n",
      "Test  loss: 1.47E-04\n",
      "\n",
      " Epoch 53 \n",
      " --------------\n",
      "Train loss: 1.42E-04\n",
      "Test  loss: 1.47E-04\n",
      "\n",
      " Epoch 54 \n",
      " --------------\n",
      "Train loss: 1.43E-04\n",
      "Test  loss: 1.48E-04\n",
      "\n",
      " Epoch 55 \n",
      " --------------\n",
      "Train loss: 1.45E-04\n",
      "Test  loss: 1.50E-04\n",
      "\n",
      " Epoch 56 \n",
      " --------------\n",
      "Train loss: 1.50E-04\n",
      "Test  loss: 1.54E-04\n",
      "\n",
      " Epoch 57 \n",
      " --------------\n",
      "Train loss: 1.57E-04\n",
      "Test  loss: 1.61E-04\n",
      "\n",
      " Epoch 58 \n",
      " --------------\n",
      "Adapting learning rate to 0.000625\n",
      "Train loss: 1.68E-04\n",
      "Test  loss: 1.72E-04\n",
      "\n",
      " Epoch 59 \n",
      " --------------\n",
      "Train loss: 1.33E-04\n",
      "Test  loss: 1.36E-04\n",
      "\n",
      " Epoch 60 \n",
      " --------------\n",
      "Train loss: 1.32E-04\n",
      "Test  loss: 1.35E-04\n",
      "\n",
      " Epoch 61 \n",
      " --------------\n",
      "Train loss: 1.31E-04\n",
      "Test  loss: 1.34E-04\n",
      "\n",
      " Epoch 62 \n",
      " --------------\n",
      "Train loss: 1.31E-04\n",
      "Test  loss: 1.34E-04\n",
      "\n",
      " Epoch 63 \n",
      " --------------\n",
      "Train loss: 1.30E-04\n",
      "Test  loss: 1.33E-04\n",
      "\n",
      " Epoch 64 \n",
      " --------------\n",
      "Train loss: 1.29E-04\n",
      "Test  loss: 1.32E-04\n",
      "\n",
      " Epoch 65 \n",
      " --------------\n",
      "Train loss: 1.28E-04\n",
      "Test  loss: 1.31E-04\n",
      "\n",
      " Epoch 66 \n",
      " --------------\n",
      "Train loss: 1.27E-04\n",
      "Test  loss: 1.30E-04\n",
      "\n",
      " Epoch 67 \n",
      " --------------\n",
      "Train loss: 1.26E-04\n",
      "Test  loss: 1.29E-04\n",
      "\n",
      " Epoch 68 \n",
      " --------------\n",
      "Train loss: 1.25E-04\n",
      "Test  loss: 1.28E-04\n",
      "\n",
      " Epoch 69 \n",
      " --------------\n",
      "Train loss: 1.24E-04\n",
      "Test  loss: 1.27E-04\n",
      "\n",
      " Epoch 70 \n",
      " --------------\n",
      "Train loss: 1.23E-04\n",
      "Test  loss: 1.26E-04\n",
      "\n",
      " Epoch 71 \n",
      " --------------\n",
      "Train loss: 1.22E-04\n",
      "Test  loss: 1.25E-04\n",
      "\n",
      " Epoch 72 \n",
      " --------------\n",
      "Train loss: 1.21E-04\n",
      "Test  loss: 1.24E-04\n",
      "\n",
      " Epoch 73 \n",
      " --------------\n",
      "Train loss: 1.20E-04\n",
      "Test  loss: 1.23E-04\n",
      "\n",
      " Epoch 74 \n",
      " --------------\n",
      "Train loss: 1.19E-04\n",
      "Test  loss: 1.22E-04\n",
      "\n",
      " Epoch 75 \n",
      " --------------\n",
      "Train loss: 1.18E-04\n",
      "Test  loss: 1.21E-04\n",
      "\n",
      " Epoch 76 \n",
      " --------------\n",
      "Train loss: 1.17E-04\n",
      "Test  loss: 1.20E-04\n",
      "\n",
      " Epoch 77 \n",
      " --------------\n",
      "Train loss: 1.16E-04\n",
      "Test  loss: 1.19E-04\n",
      "\n",
      " Epoch 78 \n",
      " --------------\n",
      "Train loss: 1.15E-04\n",
      "Test  loss: 1.18E-04\n",
      "\n",
      " Epoch 79 \n",
      " --------------\n",
      "Train loss: 1.15E-04\n",
      "Test  loss: 1.18E-04\n",
      "\n",
      " Epoch 80 \n",
      " --------------\n",
      "Train loss: 1.14E-04\n",
      "Test  loss: 1.17E-04\n",
      "\n",
      " Epoch 81 \n",
      " --------------\n",
      "Train loss: 1.13E-04\n",
      "Test  loss: 1.16E-04\n",
      "\n",
      " Epoch 82 \n",
      " --------------\n",
      "Train loss: 1.12E-04\n",
      "Test  loss: 1.15E-04\n",
      "\n",
      " Epoch 83 \n",
      " --------------\n",
      "Train loss: 1.11E-04\n",
      "Test  loss: 1.14E-04\n",
      "\n",
      " Epoch 84 \n",
      " --------------\n",
      "Train loss: 1.10E-04\n",
      "Test  loss: 1.13E-04\n",
      "\n",
      " Epoch 85 \n",
      " --------------\n",
      "Train loss: 1.10E-04\n",
      "Test  loss: 1.13E-04\n",
      "\n",
      " Epoch 86 \n",
      " --------------\n",
      "Train loss: 1.09E-04\n",
      "Test  loss: 1.12E-04\n",
      "\n",
      " Epoch 87 \n",
      " --------------\n",
      "Train loss: 1.08E-04\n",
      "Test  loss: 1.11E-04\n",
      "\n",
      " Epoch 88 \n",
      " --------------\n",
      "Train loss: 1.07E-04\n",
      "Test  loss: 1.10E-04\n",
      "\n",
      " Epoch 89 \n",
      " --------------\n",
      "Train loss: 1.06E-04\n",
      "Test  loss: 1.09E-04\n",
      "\n",
      " Epoch 90 \n",
      " --------------\n",
      "Train loss: 1.06E-04\n",
      "Test  loss: 1.08E-04\n",
      "\n",
      " Epoch 91 \n",
      " --------------\n",
      "Train loss: 1.05E-04\n",
      "Test  loss: 1.08E-04\n",
      "\n",
      " Epoch 92 \n",
      " --------------\n",
      "Train loss: 1.04E-04\n",
      "Test  loss: 1.07E-04\n",
      "\n",
      " Epoch 93 \n",
      " --------------\n",
      "Train loss: 1.03E-04\n",
      "Test  loss: 1.06E-04\n",
      "\n",
      " Epoch 94 \n",
      " --------------\n",
      "Train loss: 1.03E-04\n",
      "Test  loss: 1.05E-04\n",
      "\n",
      " Epoch 95 \n",
      " --------------\n",
      "Train loss: 1.02E-04\n",
      "Test  loss: 1.05E-04\n",
      "\n",
      " Epoch 96 \n",
      " --------------\n",
      "Train loss: 1.01E-04\n",
      "Test  loss: 1.04E-04\n",
      "\n",
      " Epoch 97 \n",
      " --------------\n",
      "Train loss: 1.00E-04\n",
      "Test  loss: 1.03E-04\n",
      "\n",
      " Epoch 98 \n",
      " --------------\n",
      "Train loss: 9.96E-05\n",
      "Test  loss: 1.02E-04\n",
      "\n",
      " Epoch 99 \n",
      " --------------\n",
      "Train loss: 9.87E-05\n",
      "Test  loss: 1.01E-04\n",
      "\n",
      " Epoch 100 \n",
      " --------------\n",
      "Train loss: 9.79E-05\n",
      "Test  loss: 1.01E-04\n",
      "\n",
      " Epoch 101 \n",
      " --------------\n",
      "Train loss: 9.71E-05\n",
      "Test  loss: 9.97E-05\n",
      "\n",
      " Epoch 102 \n",
      " --------------\n",
      "Train loss: 9.63E-05\n",
      "Test  loss: 9.89E-05\n",
      "\n",
      " Epoch 103 \n",
      " --------------\n",
      "Train loss: 9.54E-05\n",
      "Test  loss: 9.80E-05\n",
      "\n",
      " Epoch 104 \n",
      " --------------\n",
      "Train loss: 9.46E-05\n",
      "Test  loss: 9.72E-05\n",
      "\n",
      " Epoch 105 \n",
      " --------------\n",
      "Train loss: 9.38E-05\n",
      "Test  loss: 9.64E-05\n",
      "\n",
      " Epoch 106 \n",
      " --------------\n",
      "Train loss: 9.30E-05\n",
      "Test  loss: 9.55E-05\n",
      "\n",
      " Epoch 107 \n",
      " --------------\n",
      "Train loss: 9.22E-05\n",
      "Test  loss: 9.47E-05\n",
      "\n",
      " Epoch 108 \n",
      " --------------\n",
      "Train loss: 9.15E-05\n",
      "Test  loss: 9.40E-05\n",
      "\n",
      " Epoch 109 \n",
      " --------------\n",
      "Train loss: 9.07E-05\n",
      "Test  loss: 9.32E-05\n",
      "\n",
      " Epoch 110 \n",
      " --------------\n",
      "Train loss: 9.01E-05\n",
      "Test  loss: 9.26E-05\n",
      "\n",
      " Epoch 111 \n",
      " --------------\n",
      "Train loss: 8.94E-05\n",
      "Test  loss: 9.18E-05\n",
      "\n",
      " Epoch 112 \n",
      " --------------\n",
      "Train loss: 8.88E-05\n",
      "Test  loss: 9.12E-05\n",
      "\n",
      " Epoch 113 \n",
      " --------------\n",
      "Train loss: 8.82E-05\n",
      "Test  loss: 9.06E-05\n",
      "\n",
      " Epoch 114 \n",
      " --------------\n",
      "Train loss: 8.76E-05\n",
      "Test  loss: 9.00E-05\n",
      "\n",
      " Epoch 115 \n",
      " --------------\n",
      "Train loss: 8.70E-05\n",
      "Test  loss: 8.94E-05\n",
      "\n",
      " Epoch 116 \n",
      " --------------\n",
      "Train loss: 8.64E-05\n",
      "Test  loss: 8.87E-05\n",
      "\n",
      " Epoch 117 \n",
      " --------------\n",
      "Train loss: 8.58E-05\n",
      "Test  loss: 8.81E-05\n",
      "\n",
      " Epoch 118 \n",
      " --------------\n",
      "Train loss: 8.53E-05\n",
      "Test  loss: 8.76E-05\n",
      "\n",
      " Epoch 119 \n",
      " --------------\n",
      "Train loss: 8.47E-05\n",
      "Test  loss: 8.70E-05\n",
      "\n",
      " Epoch 120 \n",
      " --------------\n",
      "Train loss: 8.42E-05\n",
      "Test  loss: 8.64E-05\n",
      "\n",
      " Epoch 121 \n",
      " --------------\n",
      "Train loss: 8.36E-05\n",
      "Test  loss: 8.59E-05\n",
      "\n",
      " Epoch 122 \n",
      " --------------\n",
      "Train loss: 8.31E-05\n",
      "Test  loss: 8.53E-05\n",
      "\n",
      " Epoch 123 \n",
      " --------------\n",
      "Train loss: 8.26E-05\n",
      "Test  loss: 8.48E-05\n",
      "\n",
      " Epoch 124 \n",
      " --------------\n",
      "Train loss: 8.21E-05\n",
      "Test  loss: 8.43E-05\n",
      "\n",
      " Epoch 125 \n",
      " --------------\n",
      "Train loss: 8.16E-05\n",
      "Test  loss: 8.37E-05\n",
      "\n",
      " Epoch 126 \n",
      " --------------\n",
      "Train loss: 8.11E-05\n",
      "Test  loss: 8.32E-05\n",
      "\n",
      " Epoch 127 \n",
      " --------------\n",
      "Train loss: 8.07E-05\n",
      "Test  loss: 8.28E-05\n",
      "\n",
      " Epoch 128 \n",
      " --------------\n",
      "Train loss: 8.01E-05\n",
      "Test  loss: 8.23E-05\n",
      "\n",
      " Epoch 129 \n",
      " --------------\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "296ab01e",
   "metadata": {},
   "source": [
    "Create a quick sketch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8056faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n",
    "# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n",
    "\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# for ind in trainer.adaptation_indices:\n",
    "#     plt.axvline(ind, ls = '--', color='grey')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"MSE Loss\")\n",
    "# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n",
    "# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "377b0cbf",
   "metadata": {},
   "source": [
    "# Second goal: NNC2P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf95fa58",
   "metadata": {},
   "source": [
    "__TO DO__ think about design of architecture AND fix the conserved variable values -- I think they were not computed correctly before! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b454181",
   "metadata": {},
   "source": [
    "__NNC2P__: try to replicate the full C2P conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10571f",
   "metadata": {},
   "source": [
    "Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"rho\", \"eps\", \"ye\"]\n",
    "out_vars = [\"temp\"]\n",
    "# For normalization, use sklearn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e96de6",
   "metadata": {},
   "source": [
    "Create a new instance of the Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08aa592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=50, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Net(nb_of_inputs = 3, nb_of_outputs = 1, h=[50, 50])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504399cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83cead",
   "metadata": {},
   "source": [
    "Create a trainer object from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-1, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0369872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 7.59E-01\n",
      "Test  loss: 7.62E-01\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 9.92E-01\n",
      "Test  loss: 9.98E-01\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 9.45E-01\n",
      "Test  loss: 9.61E-01\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 8.24E-01\n",
      "Test  loss: 8.29E-01\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 9.10E-01\n",
      "Test  loss: 9.16E-01\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.91E-01\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 8.62E-01\n",
      "Test  loss: 8.67E-01\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 8.20E-01\n",
      "Test  loss: 8.27E-01\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.94E-01\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 9.20E-01\n",
      "Test  loss: 9.26E-01\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 7.34E-01\n",
      "Test  loss: 7.39E-01\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 7.48E-01\n",
      "Test  loss: 7.56E-01\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Train loss: 7.28E-01\n",
      "Test  loss: 7.31E-01\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 8.87E-01\n",
      "Test  loss: 8.99E-01\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.09E-01\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 8.11E-01\n",
      "Test  loss: 8.20E-01\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.08E-01\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Adapting learning rate to 0.05\n",
      "Train loss: 8.70E-01\n",
      "Test  loss: 8.79E-01\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 6.88E-01\n",
      "Test  loss: 6.92E-01\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 6.24E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 6.42E-01\n",
      "Test  loss: 6.46E-01\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Train loss: 6.21E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 6.29E-01\n",
      "Test  loss: 6.32E-01\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Train loss: 6.30E-01\n",
      "Test  loss: 6.33E-01\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 6.13E-01\n",
      "Test  loss: 6.16E-01\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Train loss: 6.25E-01\n",
      "Test  loss: 6.27E-01\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 6.75E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 6.22E-01\n",
      "Test  loss: 6.23E-01\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 6.77E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Adapting learning rate to 0.025\n",
      "Train loss: 6.98E-01\n",
      "Test  loss: 6.99E-01\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 5.71E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 5.77E-01\n",
      "Test  loss: 5.81E-01\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 5.65E-01\n",
      "Test  loss: 5.69E-01\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Train loss: 5.63E-01\n",
      "Test  loss: 5.67E-01\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 5.60E-01\n",
      "Test  loss: 5.65E-01\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 5.67E-01\n",
      "Test  loss: 5.70E-01\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 5.58E-01\n",
      "Test  loss: 5.60E-01\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.64E-01\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 5.73E-01\n",
      "Test  loss: 5.75E-01\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Adapting learning rate to 0.0125\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.73E-01\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 4.89E-01\n",
      "Test  loss: 4.92E-01\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 4.87E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 4.86E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 4.83E-01\n",
      "Test  loss: 4.88E-01\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 4.80E-01\n",
      "Test  loss: 4.85E-01\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 4.78E-01\n",
      "Test  loss: 4.83E-01\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 4.77E-01\n",
      "Test  loss: 4.81E-01\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 4.75E-01\n",
      "Test  loss: 4.79E-01\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n",
      "\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n",
      "\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n",
      "\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:277\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n",
      "\u001b[0;32m    274\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n",
      "\u001b[0;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n",
      "\u001b[0;32m    276\u001b[0m     \u001b[39m# Compute prediction \u001b[39;00m\n",
      "\u001b[1;32m--> 277\u001b[0m     prediction \u001b[39m=\u001b[39m model(X)\n",
      "\u001b[0;32m    278\u001b[0m     \u001b[39m# In case we use C2P loss function, have to provide conserved variables for the loss computation\u001b[39;00m\n",
      "\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m use_c2p_loss:\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m     38\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n",
      "\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a5288",
   "metadata": {},
   "source": [
    "Create a quick sketch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07eb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n",
    "# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n",
    "\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# for ind in trainer.adaptation_indices:\n",
    "#     plt.axvline(ind, ls = '--', color='grey')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"MSE Loss\")\n",
    "# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n",
    "# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ad41d2a",
   "metadata": {},
   "source": [
    "# Archive: NNE2T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391a25b5",
   "metadata": {},
   "source": [
    "__NNE2T__: try to replicate the conversion from energy to temperature, which is currently done by rootfinding approximations & lookups in the EOS table (see Gmunu code). It seemed harder than I initially thought to model and train this; and in the end, I'm not sure how useful it'll be, so I'm archiving this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48e387",
   "metadata": {},
   "source": [
    "Get the training data as DataSet and DataLoader objects. Note on normalization: we fit transform on the training data, then use the fitted scaler object to transform (i.e. using same transformation as the training data) the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b88f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"rho\", \"eps\", \"ye\"]\n",
    "out_vars = [\"temp\"]\n",
    "# For normalization, use sklearn's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_training_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/SLy4_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.fit_transform) \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = scaler.transform)\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abee61f0",
   "metadata": {},
   "source": [
    "Create a new instance of the Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b990a7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=50, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(nb_of_inputs = 3, nb_of_outputs = 1, h=[50, 50])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "312052a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23d022",
   "metadata": {},
   "source": [
    "Create a trainer object from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06becda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-1, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "93ce3bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 7.59E-01\n",
      "Test  loss: 7.62E-01\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 9.92E-01\n",
      "Test  loss: 9.98E-01\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 9.45E-01\n",
      "Test  loss: 9.61E-01\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 8.24E-01\n",
      "Test  loss: 8.29E-01\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 9.10E-01\n",
      "Test  loss: 9.16E-01\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.91E-01\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 8.62E-01\n",
      "Test  loss: 8.67E-01\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 8.20E-01\n",
      "Test  loss: 8.27E-01\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 8.80E-01\n",
      "Test  loss: 8.94E-01\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 9.20E-01\n",
      "Test  loss: 9.26E-01\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 7.34E-01\n",
      "Test  loss: 7.39E-01\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 7.48E-01\n",
      "Test  loss: 7.56E-01\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Train loss: 7.28E-01\n",
      "Test  loss: 7.31E-01\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 8.87E-01\n",
      "Test  loss: 8.99E-01\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.09E-01\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 8.11E-01\n",
      "Test  loss: 8.20E-01\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 8.02E-01\n",
      "Test  loss: 8.08E-01\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Adapting learning rate to 0.05\n",
      "Train loss: 8.70E-01\n",
      "Test  loss: 8.79E-01\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 6.88E-01\n",
      "Test  loss: 6.92E-01\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 6.24E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 6.42E-01\n",
      "Test  loss: 6.46E-01\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Train loss: 6.21E-01\n",
      "Test  loss: 6.25E-01\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 6.29E-01\n",
      "Test  loss: 6.32E-01\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Train loss: 6.30E-01\n",
      "Test  loss: 6.33E-01\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 6.13E-01\n",
      "Test  loss: 6.16E-01\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Train loss: 6.25E-01\n",
      "Test  loss: 6.27E-01\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 6.75E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 6.22E-01\n",
      "Test  loss: 6.23E-01\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 6.77E-01\n",
      "Test  loss: 6.81E-01\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Adapting learning rate to 0.025\n",
      "Train loss: 6.98E-01\n",
      "Test  loss: 6.99E-01\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 5.71E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 5.77E-01\n",
      "Test  loss: 5.81E-01\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.74E-01\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 5.65E-01\n",
      "Test  loss: 5.69E-01\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Train loss: 5.63E-01\n",
      "Test  loss: 5.67E-01\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 5.60E-01\n",
      "Test  loss: 5.65E-01\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 5.67E-01\n",
      "Test  loss: 5.70E-01\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 5.58E-01\n",
      "Test  loss: 5.60E-01\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.66E-01\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 5.62E-01\n",
      "Test  loss: 5.64E-01\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 5.73E-01\n",
      "Test  loss: 5.75E-01\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Adapting learning rate to 0.0125\n",
      "Train loss: 5.69E-01\n",
      "Test  loss: 5.73E-01\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 4.89E-01\n",
      "Test  loss: 4.92E-01\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 4.87E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 4.86E-01\n",
      "Test  loss: 4.91E-01\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 4.83E-01\n",
      "Test  loss: 4.88E-01\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 4.80E-01\n",
      "Test  loss: 4.85E-01\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 4.78E-01\n",
      "Test  loss: 4.83E-01\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 4.77E-01\n",
      "Test  loss: 4.81E-01\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 4.75E-01\n",
      "Test  loss: 4.79E-01\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:407\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n\u001b[0;32m    405\u001b[0m write_to_txt(log_file, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_counter\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m --------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m train_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, use_c2p_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_c2p_loss)\n\u001b[0;32m    408\u001b[0m \u001b[39m# Test on the training data\u001b[39;00m\n\u001b[0;32m    409\u001b[0m average_train_loss \u001b[39m=\u001b[39m test_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n",
      "File \u001b[1;32md:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:277\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n\u001b[0;32m    274\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m    276\u001b[0m     \u001b[39m# Compute prediction \u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     prediction \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m    278\u001b[0m     \u001b[39m# In case we use C2P loss function, have to provide conserved variables for the loss computation\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m use_c2p_loss:\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2a130",
   "metadata": {},
   "source": [
    "Create a quick sketch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5338413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(trainer.train_losses, color='red', label=\"Train loss\")\n",
    "# plt.plot(trainer.test_losses, color='blue', label=\"Test loss\")\n",
    "\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# for ind in trainer.adaptation_indices:\n",
    "#     plt.axvline(ind, ls = '--', color='grey')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"MSE Loss\")\n",
    "# plt.title(\"Training (100, 100) network tabular EOS for p and eps\")\n",
    "# plt.savefig(\"testing_training_tab_eos_network_100_100.pdf\", bbox_inches = 'tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "author": "Thibeau Wouters",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
