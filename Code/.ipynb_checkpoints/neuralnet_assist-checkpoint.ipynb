{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3400623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LogNorm\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['text.usetex'] = False\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "# import h5py\n",
    "# Scikit learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision.transforms import ToTensor, Normalize \n",
    "# Own scripts:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import physics\n",
    "import data\n",
    "import nnc2p\n",
    "from collections import OrderedDict\n",
    "# from nnc2p import NeuralNetwork # our own architecture\n",
    "# Get dirs\n",
    "import os\n",
    "cwd = os.getcwd()# \"Code\" folder\n",
    "master_dir = os.path.abspath(os.path.join(cwd, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b8b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple feedforward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_of_inputs: int = 3, nb_of_outputs: int = 1, h: list = [600, 200], \n",
    "                 reg: bool = False, activation_function = torch.nn.Sigmoid, output_bias=True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network class.\n",
    "        \"\"\"\n",
    "        # Call the super constructor first\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # For convenience, save the sizes of the hidden layers as fields as well\n",
    "        self.h = h\n",
    "        # Add visible layers as well: input is 3D and output is 1D\n",
    "        self.h_augmented = [nb_of_inputs] + h + [nb_of_outputs]\n",
    "\n",
    "        # Add field to specify whether or not we do regularization\n",
    "        self.regularization = reg\n",
    "\n",
    "        # Define the layers:\n",
    "        for i in range(len(self.h_augmented)-1):\n",
    "            # For the final layer, do/do not use a bias term (user choice)\n",
    "            if i == len(self.h_augmented)-2:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1], bias=output_bias))\n",
    "            else:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1]))\n",
    "                setattr(self, f\"activation{i+1}\", activation_function())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward step given the input x.\n",
    "        :param x: Input for the neural network.\n",
    "        :return: x: Output neural network\n",
    "        \"\"\"\n",
    "\n",
    "        for i, module in enumerate(self.modules()):\n",
    "            # The first module is the whole NNC2P object, continue\n",
    "            if i == 0:\n",
    "                continue\n",
    "            x = module(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb22ec9",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779e9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"D\", \"S\", \"tau\"]\n",
    "out_vars = [\"p\"]\n",
    "# For normalization, use sklearn's StandardScaler -- give None for no normalization\n",
    "scaler = StandardScaler()\n",
    "# Read the sampled data as pandas dataframes\n",
    "train_df = pd.read_csv(os.path.join(master_dir, \"Data/ideal_gas_c2p_train_data.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(master_dir, \"Data/ideal_gas_c2p_test_data.csv\"))\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = None)  # scaler.fit_transform \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = None)  # scaler.transform\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2d4db",
   "metadata": {},
   "source": [
    "Get the mu for output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d2d629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    W = np.sqrt(1/(1-df[\"v\"]**2))\n",
    "    h = 1 + df[\"eps\"] + (df[\"p\"]/df[\"rho\"])\n",
    "    \n",
    "    df[\"mu\"] = 1/(W*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ed2327e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>eps</th>\n",
       "      <th>v</th>\n",
       "      <th>p</th>\n",
       "      <th>D</th>\n",
       "      <th>S</th>\n",
       "      <th>tau</th>\n",
       "      <th>mu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.836323</td>\n",
       "      <td>1.962039</td>\n",
       "      <td>0.266066</td>\n",
       "      <td>12.866164</td>\n",
       "      <td>10.204131</td>\n",
       "      <td>12.026585</td>\n",
       "      <td>22.131297</td>\n",
       "      <td>0.225747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.203042</td>\n",
       "      <td>1.522893</td>\n",
       "      <td>0.669504</td>\n",
       "      <td>5.282451</td>\n",
       "      <td>7.004551</td>\n",
       "      <td>22.337426</td>\n",
       "      <td>21.077156</td>\n",
       "      <td>0.209942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.591172</td>\n",
       "      <td>0.739712</td>\n",
       "      <td>0.441473</td>\n",
       "      <td>4.236664</td>\n",
       "      <td>9.574739</td>\n",
       "      <td>10.518784</td>\n",
       "      <td>10.015183</td>\n",
       "      <td>0.401851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.626353</td>\n",
       "      <td>1.392560</td>\n",
       "      <td>0.585310</td>\n",
       "      <td>0.581489</td>\n",
       "      <td>0.772503</td>\n",
       "      <td>1.851943</td>\n",
       "      <td>1.810045</td>\n",
       "      <td>0.244151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.087245</td>\n",
       "      <td>0.222690</td>\n",
       "      <td>0.637219</td>\n",
       "      <td>1.497551</td>\n",
       "      <td>13.088716</td>\n",
       "      <td>14.838680</td>\n",
       "      <td>8.700349</td>\n",
       "      <td>0.562070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2.354663</td>\n",
       "      <td>1.431949</td>\n",
       "      <td>0.609409</td>\n",
       "      <td>2.247838</td>\n",
       "      <td>2.969850</td>\n",
       "      <td>7.730559</td>\n",
       "      <td>7.467644</td>\n",
       "      <td>0.234117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.646230</td>\n",
       "      <td>0.874169</td>\n",
       "      <td>0.072389</td>\n",
       "      <td>0.376610</td>\n",
       "      <td>0.647930</td>\n",
       "      <td>0.115541</td>\n",
       "      <td>0.571578</td>\n",
       "      <td>0.405941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.854028</td>\n",
       "      <td>1.505251</td>\n",
       "      <td>0.331675</td>\n",
       "      <td>0.857018</td>\n",
       "      <td>0.905272</td>\n",
       "      <td>1.116739</td>\n",
       "      <td>1.604678</td>\n",
       "      <td>0.268869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>6.153600</td>\n",
       "      <td>1.333735</td>\n",
       "      <td>0.350154</td>\n",
       "      <td>5.471517</td>\n",
       "      <td>6.569503</td>\n",
       "      <td>7.914822</td>\n",
       "      <td>10.562783</td>\n",
       "      <td>0.290637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6.660209</td>\n",
       "      <td>1.069716</td>\n",
       "      <td>0.373827</td>\n",
       "      <td>4.749686</td>\n",
       "      <td>7.180830</td>\n",
       "      <td>8.054216</td>\n",
       "      <td>9.614791</td>\n",
       "      <td>0.333290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            rho       eps         v          p          D          S  \\\n",
       "0      9.836323  1.962039  0.266066  12.866164  10.204131  12.026585   \n",
       "1      5.203042  1.522893  0.669504   5.282451   7.004551  22.337426   \n",
       "2      8.591172  0.739712  0.441473   4.236664   9.574739  10.518784   \n",
       "3      0.626353  1.392560  0.585310   0.581489   0.772503   1.851943   \n",
       "4     10.087245  0.222690  0.637219   1.497551  13.088716  14.838680   \n",
       "...         ...       ...       ...        ...        ...        ...   \n",
       "9995   2.354663  1.431949  0.609409   2.247838   2.969850   7.730559   \n",
       "9996   0.646230  0.874169  0.072389   0.376610   0.647930   0.115541   \n",
       "9997   0.854028  1.505251  0.331675   0.857018   0.905272   1.116739   \n",
       "9998   6.153600  1.333735  0.350154   5.471517   6.569503   7.914822   \n",
       "9999   6.660209  1.069716  0.373827   4.749686   7.180830   8.054216   \n",
       "\n",
       "            tau        mu  \n",
       "0     22.131297  0.225747  \n",
       "1     21.077156  0.209942  \n",
       "2     10.015183  0.401851  \n",
       "3      1.810045  0.244151  \n",
       "4      8.700349  0.562070  \n",
       "...         ...       ...  \n",
       "9995   7.467644  0.234117  \n",
       "9996   0.571578  0.405941  \n",
       "9997   1.604678  0.268869  \n",
       "9998  10.562783  0.290637  \n",
       "9999   9.614791  0.333290  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19af6b0",
   "metadata": {},
   "source": [
    "Convert to appropriate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd5853ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the names of the input vars (features) and output vars (labels)\n",
    "in_vars = [\"D\", \"S\", \"tau\"]\n",
    "out_vars = [\"mu\"]\n",
    "# For normalization, use sklearn's StandardScaler -- give None for no normalization\n",
    "scaler = StandardScaler()\n",
    "# Convert to PyTorch Datasets as we defined them\n",
    "train_dataset = data.CustomDataset(train_df, feature_names = in_vars, label_names = out_vars, normalization_function = None)  # scaler.fit_transform \n",
    "test_dataset  = data.CustomDataset(test_df, feature_names = in_vars, label_names = out_vars, normalization_function = None)  # scaler.transform\n",
    "# Then create dataloaders, with batch size 32, from datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83b29e",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "002027bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=3, out_features=10, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(h=[10, 10], output_bias=True).float()\n",
    "device = \"cpu\"\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "de585d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnc2p.count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "340b793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nnc2p.Trainer(model, 1e-2, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "46aa2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 epochs.\n",
      "\n",
      " Epoch 0 \n",
      " --------------\n",
      "Train loss: 5.16E-04\n",
      "Test  loss: 4.20E-04\n",
      "\n",
      " Epoch 1 \n",
      " --------------\n",
      "Train loss: 2.99E-04\n",
      "Test  loss: 2.36E-04\n",
      "\n",
      " Epoch 2 \n",
      " --------------\n",
      "Train loss: 2.45E-04\n",
      "Test  loss: 2.01E-04\n",
      "\n",
      " Epoch 3 \n",
      " --------------\n",
      "Train loss: 1.94E-04\n",
      "Test  loss: 1.63E-04\n",
      "\n",
      " Epoch 4 \n",
      " --------------\n",
      "Train loss: 1.54E-04\n",
      "Test  loss: 1.27E-04\n",
      "\n",
      " Epoch 5 \n",
      " --------------\n",
      "Train loss: 1.29E-04\n",
      "Test  loss: 1.03E-04\n",
      "\n",
      " Epoch 6 \n",
      " --------------\n",
      "Train loss: 1.15E-04\n",
      "Test  loss: 9.12E-05\n",
      "\n",
      " Epoch 7 \n",
      " --------------\n",
      "Train loss: 1.05E-04\n",
      "Test  loss: 8.25E-05\n",
      "\n",
      " Epoch 8 \n",
      " --------------\n",
      "Train loss: 9.76E-05\n",
      "Test  loss: 7.65E-05\n",
      "\n",
      " Epoch 9 \n",
      " --------------\n",
      "Train loss: 9.32E-05\n",
      "Test  loss: 7.30E-05\n",
      "\n",
      " Epoch 10 \n",
      " --------------\n",
      "Train loss: 8.75E-05\n",
      "Test  loss: 6.81E-05\n",
      "\n",
      " Epoch 11 \n",
      " --------------\n",
      "Train loss: 8.12E-05\n",
      "Test  loss: 6.16E-05\n",
      "\n",
      " Epoch 12 \n",
      " --------------\n",
      "Train loss: 7.73E-05\n",
      "Test  loss: 5.76E-05\n",
      "\n",
      " Epoch 13 \n",
      " --------------\n",
      "Train loss: 7.55E-05\n",
      "Test  loss: 5.60E-05\n",
      "\n",
      " Epoch 14 \n",
      " --------------\n",
      "Train loss: 7.42E-05\n",
      "Test  loss: 5.47E-05\n",
      "\n",
      " Epoch 15 \n",
      " --------------\n",
      "Train loss: 7.34E-05\n",
      "Test  loss: 5.40E-05\n",
      "\n",
      " Epoch 16 \n",
      " --------------\n",
      "Train loss: 7.33E-05\n",
      "Test  loss: 5.39E-05\n",
      "\n",
      " Epoch 17 \n",
      " --------------\n",
      "Train loss: 7.41E-05\n",
      "Test  loss: 5.46E-05\n",
      "\n",
      " Epoch 18 \n",
      " --------------\n",
      "Train loss: 7.58E-05\n",
      "Test  loss: 5.61E-05\n",
      "\n",
      " Epoch 19 \n",
      " --------------\n",
      "Train loss: 7.77E-05\n",
      "Test  loss: 5.79E-05\n",
      "\n",
      " Epoch 20 \n",
      " --------------\n",
      "Train loss: 8.01E-05\n",
      "Test  loss: 6.01E-05\n",
      "\n",
      " Epoch 21 \n",
      " --------------\n",
      "Adapting learning rate to 0.005\n",
      "Train loss: 8.23E-05\n",
      "Test  loss: 6.20E-05\n",
      "\n",
      " Epoch 22 \n",
      " --------------\n",
      "Train loss: 6.58E-05\n",
      "Test  loss: 5.48E-05\n",
      "\n",
      " Epoch 23 \n",
      " --------------\n",
      "Train loss: 6.57E-05\n",
      "Test  loss: 5.49E-05\n",
      "\n",
      " Epoch 24 \n",
      " --------------\n",
      "Train loss: 6.51E-05\n",
      "Test  loss: 5.43E-05\n",
      "\n",
      " Epoch 25 \n",
      " --------------\n",
      "Train loss: 6.39E-05\n",
      "Test  loss: 5.32E-05\n",
      "\n",
      " Epoch 26 \n",
      " --------------\n",
      "Train loss: 6.25E-05\n",
      "Test  loss: 5.20E-05\n",
      "\n",
      " Epoch 27 \n",
      " --------------\n",
      "Train loss: 6.25E-05\n",
      "Test  loss: 5.23E-05\n",
      "\n",
      " Epoch 28 \n",
      " --------------\n",
      "Train loss: 6.22E-05\n",
      "Test  loss: 5.20E-05\n",
      "\n",
      " Epoch 29 \n",
      " --------------\n",
      "Train loss: 6.14E-05\n",
      "Test  loss: 5.12E-05\n",
      "\n",
      " Epoch 30 \n",
      " --------------\n",
      "Train loss: 6.05E-05\n",
      "Test  loss: 5.02E-05\n",
      "\n",
      " Epoch 31 \n",
      " --------------\n",
      "Train loss: 5.94E-05\n",
      "Test  loss: 4.90E-05\n",
      "\n",
      " Epoch 32 \n",
      " --------------\n",
      "Train loss: 5.82E-05\n",
      "Test  loss: 4.78E-05\n",
      "\n",
      " Epoch 33 \n",
      " --------------\n",
      "Train loss: 5.68E-05\n",
      "Test  loss: 4.65E-05\n",
      "\n",
      " Epoch 34 \n",
      " --------------\n",
      "Train loss: 5.54E-05\n",
      "Test  loss: 4.50E-05\n",
      "\n",
      " Epoch 35 \n",
      " --------------\n",
      "Train loss: 5.38E-05\n",
      "Test  loss: 4.36E-05\n",
      "\n",
      " Epoch 36 \n",
      " --------------\n",
      "Train loss: 5.23E-05\n",
      "Test  loss: 4.21E-05\n",
      "\n",
      " Epoch 37 \n",
      " --------------\n",
      "Train loss: 5.08E-05\n",
      "Test  loss: 4.07E-05\n",
      "\n",
      " Epoch 38 \n",
      " --------------\n",
      "Train loss: 4.96E-05\n",
      "Test  loss: 3.95E-05\n",
      "\n",
      " Epoch 39 \n",
      " --------------\n",
      "Train loss: 4.85E-05\n",
      "Test  loss: 3.85E-05\n",
      "\n",
      " Epoch 40 \n",
      " --------------\n",
      "Train loss: 4.75E-05\n",
      "Test  loss: 3.76E-05\n",
      "\n",
      " Epoch 41 \n",
      " --------------\n",
      "Train loss: 4.65E-05\n",
      "Test  loss: 3.67E-05\n",
      "\n",
      " Epoch 42 \n",
      " --------------\n",
      "Train loss: 4.55E-05\n",
      "Test  loss: 3.58E-05\n",
      "\n",
      " Epoch 43 \n",
      " --------------\n",
      "Train loss: 4.46E-05\n",
      "Test  loss: 3.51E-05\n",
      "\n",
      " Epoch 44 \n",
      " --------------\n",
      "Train loss: 4.38E-05\n",
      "Test  loss: 3.45E-05\n",
      "\n",
      " Epoch 45 \n",
      " --------------\n",
      "Train loss: 4.30E-05\n",
      "Test  loss: 3.39E-05\n",
      "\n",
      " Epoch 46 \n",
      " --------------\n",
      "Train loss: 4.24E-05\n",
      "Test  loss: 3.34E-05\n",
      "\n",
      " Epoch 47 \n",
      " --------------\n",
      "Train loss: 4.18E-05\n",
      "Test  loss: 3.30E-05\n",
      "\n",
      " Epoch 48 \n",
      " --------------\n",
      "Train loss: 4.13E-05\n",
      "Test  loss: 3.27E-05\n",
      "\n",
      " Epoch 49 \n",
      " --------------\n",
      "Train loss: 4.79E-05\n",
      "Test  loss: 4.14E-05\n",
      "\n",
      " Epoch 50 \n",
      " --------------\n",
      "Train loss: 4.17E-05\n",
      "Test  loss: 3.34E-05\n",
      "\n",
      " Epoch 51 \n",
      " --------------\n",
      "Train loss: 4.06E-05\n",
      "Test  loss: 3.24E-05\n",
      "\n",
      " Epoch 52 \n",
      " --------------\n",
      "Train loss: 4.02E-05\n",
      "Test  loss: 3.22E-05\n",
      "\n",
      " Epoch 53 \n",
      " --------------\n",
      "Train loss: 3.99E-05\n",
      "Test  loss: 3.20E-05\n",
      "\n",
      " Epoch 54 \n",
      " --------------\n",
      "Train loss: 3.95E-05\n",
      "Test  loss: 3.17E-05\n",
      "\n",
      " Epoch 55 \n",
      " --------------\n",
      "Train loss: 3.90E-05\n",
      "Test  loss: 3.12E-05\n",
      "\n",
      " Epoch 56 \n",
      " --------------\n",
      "Train loss: 3.84E-05\n",
      "Test  loss: 3.08E-05\n",
      "\n",
      " Epoch 57 \n",
      " --------------\n",
      "Train loss: 4.52E-05\n",
      "Test  loss: 3.89E-05\n",
      "\n",
      " Epoch 58 \n",
      " --------------\n",
      "Train loss: 3.88E-05\n",
      "Test  loss: 3.15E-05\n",
      "\n",
      " Epoch 59 \n",
      " --------------\n",
      "Train loss: 3.80E-05\n",
      "Test  loss: 3.08E-05\n",
      "\n",
      " Epoch 60 \n",
      " --------------\n",
      "Train loss: 3.77E-05\n",
      "Test  loss: 3.07E-05\n",
      "\n",
      " Epoch 61 \n",
      " --------------\n",
      "Train loss: 3.74E-05\n",
      "Test  loss: 3.05E-05\n",
      "\n",
      " Epoch 62 \n",
      " --------------\n",
      "Train loss: 3.70E-05\n",
      "Test  loss: 3.01E-05\n",
      "\n",
      " Epoch 63 \n",
      " --------------\n",
      "Train loss: 3.65E-05\n",
      "Test  loss: 2.97E-05\n",
      "\n",
      " Epoch 64 \n",
      " --------------\n",
      "Train loss: 4.06E-05\n",
      "Test  loss: 3.53E-05\n",
      "\n",
      " Epoch 65 \n",
      " --------------\n",
      "Train loss: 3.69E-05\n",
      "Test  loss: 3.04E-05\n",
      "\n",
      " Epoch 66 \n",
      " --------------\n",
      "Train loss: 3.62E-05\n",
      "Test  loss: 2.99E-05\n",
      "\n",
      " Epoch 67 \n",
      " --------------\n",
      "Train loss: 3.59E-05\n",
      "Test  loss: 2.97E-05\n",
      "\n",
      " Epoch 68 \n",
      " --------------\n",
      "Train loss: 3.57E-05\n",
      "Test  loss: 2.96E-05\n",
      "\n",
      " Epoch 69 \n",
      " --------------\n",
      "Train loss: 3.53E-05\n",
      "Test  loss: 2.93E-05\n",
      "\n",
      " Epoch 70 \n",
      " --------------\n",
      "Train loss: 3.44E-05\n",
      "Test  loss: 2.85E-05\n",
      "\n",
      " Epoch 71 \n",
      " --------------\n",
      "Train loss: 3.33E-05\n",
      "Test  loss: 2.75E-05\n",
      "\n",
      " Epoch 72 \n",
      " --------------\n",
      "Train loss: 3.37E-05\n",
      "Test  loss: 2.89E-05\n",
      "\n",
      " Epoch 73 \n",
      " --------------\n",
      "Train loss: 3.28E-05\n",
      "Test  loss: 2.75E-05\n",
      "\n",
      " Epoch 74 \n",
      " --------------\n",
      "Train loss: 3.25E-05\n",
      "Test  loss: 2.73E-05\n",
      "\n",
      " Epoch 75 \n",
      " --------------\n",
      "Train loss: 3.23E-05\n",
      "Test  loss: 2.72E-05\n",
      "\n",
      " Epoch 76 \n",
      " --------------\n",
      "Train loss: 3.22E-05\n",
      "Test  loss: 2.72E-05\n",
      "\n",
      " Epoch 77 \n",
      " --------------\n",
      "Train loss: 3.21E-05\n",
      "Test  loss: 2.71E-05\n",
      "\n",
      " Epoch 78 \n",
      " --------------\n",
      "Train loss: 3.20E-05\n",
      "Test  loss: 2.70E-05\n",
      "\n",
      " Epoch 79 \n",
      " --------------\n",
      "Train loss: 3.31E-05\n",
      "Test  loss: 2.91E-05\n",
      "\n",
      " Epoch 80 \n",
      " --------------\n",
      "Train loss: 3.18E-05\n",
      "Test  loss: 2.70E-05\n",
      "\n",
      " Epoch 81 \n",
      " --------------\n",
      "Train loss: 3.15E-05\n",
      "Test  loss: 2.69E-05\n",
      "\n",
      " Epoch 82 \n",
      " --------------\n",
      "Train loss: 3.14E-05\n",
      "Test  loss: 2.68E-05\n",
      "\n",
      " Epoch 83 \n",
      " --------------\n",
      "Train loss: 3.13E-05\n",
      "Test  loss: 2.68E-05\n",
      "\n",
      " Epoch 84 \n",
      " --------------\n",
      "Train loss: 3.13E-05\n",
      "Test  loss: 2.68E-05\n",
      "\n",
      " Epoch 85 \n",
      " --------------\n",
      "Train loss: 3.15E-05\n",
      "Test  loss: 2.70E-05\n",
      "\n",
      " Epoch 86 \n",
      " --------------\n",
      "Train loss: 3.16E-05\n",
      "Test  loss: 2.71E-05\n",
      "\n",
      " Epoch 87 \n",
      " --------------\n",
      "Train loss: 3.13E-05\n",
      "Test  loss: 2.72E-05\n",
      "\n",
      " Epoch 88 \n",
      " --------------\n",
      "Train loss: 3.15E-05\n",
      "Test  loss: 2.73E-05\n",
      "\n",
      " Epoch 89 \n",
      " --------------\n",
      "Train loss: 3.16E-05\n",
      "Test  loss: 2.74E-05\n",
      "\n",
      " Epoch 90 \n",
      " --------------\n",
      "Train loss: 3.17E-05\n",
      "Test  loss: 2.76E-05\n",
      "\n",
      " Epoch 91 \n",
      " --------------\n",
      "Train loss: 3.19E-05\n",
      "Test  loss: 2.78E-05\n",
      "\n",
      " Epoch 92 \n",
      " --------------\n",
      "Adapting learning rate to 0.0025\n",
      "Train loss: 3.21E-05\n",
      "Test  loss: 2.81E-05\n",
      "\n",
      " Epoch 93 \n",
      " --------------\n",
      "Train loss: 2.76E-05\n",
      "Test  loss: 2.72E-05\n",
      "\n",
      " Epoch 94 \n",
      " --------------\n",
      "Train loss: 2.71E-05\n",
      "Test  loss: 2.67E-05\n",
      "\n",
      " Epoch 95 \n",
      " --------------\n",
      "Train loss: 2.68E-05\n",
      "Test  loss: 2.65E-05\n",
      "\n",
      " Epoch 96 \n",
      " --------------\n",
      "Train loss: 2.66E-05\n",
      "Test  loss: 2.63E-05\n",
      "\n",
      " Epoch 97 \n",
      " --------------\n",
      "Train loss: 2.65E-05\n",
      "Test  loss: 2.62E-05\n",
      "\n",
      " Epoch 98 \n",
      " --------------\n",
      "Train loss: 2.63E-05\n",
      "Test  loss: 2.61E-05\n",
      "\n",
      " Epoch 99 \n",
      " --------------\n",
      "Train loss: 2.62E-05\n",
      "Test  loss: 2.59E-05\n",
      "\n",
      " Epoch 100 \n",
      " --------------\n",
      "Train loss: 2.61E-05\n",
      "Test  loss: 2.59E-05\n",
      "\n",
      " Epoch 101 \n",
      " --------------\n",
      "Train loss: 2.60E-05\n",
      "Test  loss: 2.58E-05\n",
      "\n",
      " Epoch 102 \n",
      " --------------\n",
      "Train loss: 2.59E-05\n",
      "Test  loss: 2.57E-05\n",
      "\n",
      " Epoch 103 \n",
      " --------------\n",
      "Train loss: 2.58E-05\n",
      "Test  loss: 2.56E-05\n",
      "\n",
      " Epoch 104 \n",
      " --------------\n",
      "Train loss: 2.58E-05\n",
      "Test  loss: 2.55E-05\n",
      "\n",
      " Epoch 105 \n",
      " --------------\n",
      "Train loss: 2.57E-05\n",
      "Test  loss: 2.55E-05\n",
      "\n",
      " Epoch 106 \n",
      " --------------\n",
      "Train loss: 2.56E-05\n",
      "Test  loss: 2.54E-05\n",
      "\n",
      " Epoch 107 \n",
      " --------------\n",
      "Train loss: 2.55E-05\n",
      "Test  loss: 2.54E-05\n",
      "\n",
      " Epoch 108 \n",
      " --------------\n",
      "Train loss: 2.55E-05\n",
      "Test  loss: 2.53E-05\n",
      "\n",
      " Epoch 109 \n",
      " --------------\n",
      "Train loss: 2.54E-05\n",
      "Test  loss: 2.52E-05\n",
      "\n",
      " Epoch 110 \n",
      " --------------\n",
      "Train loss: 2.53E-05\n",
      "Test  loss: 2.52E-05\n",
      "\n",
      " Epoch 111 \n",
      " --------------\n",
      "Train loss: 2.53E-05\n",
      "Test  loss: 2.51E-05\n",
      "\n",
      " Epoch 112 \n",
      " --------------\n",
      "Train loss: 2.52E-05\n",
      "Test  loss: 2.51E-05\n",
      "\n",
      " Epoch 113 \n",
      " --------------\n",
      "Train loss: 2.52E-05\n",
      "Test  loss: 2.50E-05\n",
      "\n",
      " Epoch 114 \n",
      " --------------\n",
      "Train loss: 2.51E-05\n",
      "Test  loss: 2.50E-05\n",
      "\n",
      " Epoch 115 \n",
      " --------------\n",
      "Train loss: 2.50E-05\n",
      "Test  loss: 2.49E-05\n",
      "\n",
      " Epoch 116 \n",
      " --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.50E-05\n",
      "Test  loss: 2.49E-05\n",
      "\n",
      " Epoch 117 \n",
      " --------------\n",
      "Train loss: 2.49E-05\n",
      "Test  loss: 2.48E-05\n",
      "\n",
      " Epoch 118 \n",
      " --------------\n",
      "Train loss: 2.49E-05\n",
      "Test  loss: 2.48E-05\n",
      "\n",
      " Epoch 119 \n",
      " --------------\n",
      "Train loss: 2.48E-05\n",
      "Test  loss: 2.47E-05\n",
      "\n",
      " Epoch 120 \n",
      " --------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:460\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, adaptation_threshold, adaptation_multiplier, number_of_epochs, log_file, csv_file)\u001b[0m\n\u001b[0;32m    458\u001b[0m write_to_txt(log_file, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m --------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_c2p_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_c2p_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# Test on the training data\u001b[39;00m\n\u001b[0;32m    462\u001b[0m average_train_loss \u001b[38;5;241m=\u001b[39m test_loop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn)\n",
      "File \u001b[1;32mD:\\Coding\\master-thesis-AI\\Code\\nnc2p.py:340\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, report_progress, use_c2p_loss)\u001b[0m\n\u001b[0;32m    338\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    339\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 340\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# If we want to report progress during training (not recommended - obstructs view)\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report_progress:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "addb9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../Models/nn_assist_10_10_sigmoid.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f052b9",
   "metadata": {},
   "source": [
    "## Test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0de80507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../Models/nn_assist_10_10_sigmoid.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5abbeaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.transpose(np.array([train_df[\"D\"].values, train_df[\"S\"].values, train_df[\"tau\"].values]))\n",
    "y = train_df[\"mu\"].values\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    x       = torch.from_numpy(x).float()\n",
    "    yhat = model(x)\n",
    "    yhat = yhat.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85db73",
   "metadata": {},
   "source": [
    "## Take for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c1af99bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2722380757331848\n",
      "[0.24501426815986635, 0.2994618833065033]\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([0.90437025614154176, 0.70543713884138814, 1.6228154380654374]).float()\n",
    "x = torch.Tensor([1.1142615696244609, 0.52933694956096999, 1.7985517610787460]).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "    y = y.item()\n",
    "print(y)\n",
    "print([0.9*y, 1.1*y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b47fa72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41238653659820557\n",
      "[0.371147882938385, 0.45362519025802617]\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1, 1, 1]).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "    y = y.item()\n",
    "print(y)\n",
    "print([0.9*y, 1.1*y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "91500179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1158])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[\"linear3.bias\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004db53",
   "metadata": {},
   "source": [
    "# Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "788ff68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\master-thesis-AI\\Data\\NN assist\\Shocktube\n"
     ]
    }
   ],
   "source": [
    "simulation = \"Shocktube\"\n",
    "simulation_dir = os.path.join(\"../Data/NN assist\", simulation)\n",
    "print(os.path.abspath(simulation_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "94ccda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(simulation_dir, \"counts_old.dat\")\n",
    "counts_old = np.loadtxt(path)\n",
    "path = os.path.join(simulation_dir, \"counts_new.dat\")\n",
    "counts_new = np.loadtxt(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "863d1052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without assist: 3.8004432965140773 +- 1.9823067885472485\n",
      "With       assist: 3.171163744939642 +- 1.579099339667568\n"
     ]
    }
   ],
   "source": [
    "print(f\"Without assist: {np.mean(counts_old)} +- {np.std(counts_old)}\")\n",
    "print(f\"With       assist: {np.mean(counts_new)} +- {np.std(counts_new)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f84476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
