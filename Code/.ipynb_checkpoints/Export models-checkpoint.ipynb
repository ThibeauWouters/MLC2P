{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af563537",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb9f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "import random\n",
    "import csv\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import os\n",
    "# Own modules:\n",
    "import physics\n",
    "import data\n",
    "from data import CustomDataset\n",
    "import nnc2p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88888cdd",
   "metadata": {},
   "source": [
    "# Update: using the ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e57225",
   "metadata": {},
   "source": [
    "(Semester 2) I'm exporting a model again, now in ONNX format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd101c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear1): Linear(in_features=3, out_features=504, bias=True)\n",
       "  (linear2): Linear(in_features=504, out_features=127, bias=True)\n",
       "  (linear3): Linear(in_features=127, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model\n",
    "state_dict = torch.load(\"finetuned_most_pruned.pth\")\n",
    "model = nnc2p.create_nn(state_dict)\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85507d2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf5442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, torch.randn(3).float(), \"pruned_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164f66c",
   "metadata": {},
   "source": [
    "Testing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a4801a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>eps</th>\n",
       "      <th>v</th>\n",
       "      <th>p</th>\n",
       "      <th>D</th>\n",
       "      <th>S</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.836323</td>\n",
       "      <td>1.962039</td>\n",
       "      <td>0.266066</td>\n",
       "      <td>12.866164</td>\n",
       "      <td>10.204131</td>\n",
       "      <td>12.026585</td>\n",
       "      <td>22.131297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rho       eps         v          p          D          S        tau\n",
       "0  9.836323  1.962039  0.266066  12.866164  10.204131  12.026585  22.131297"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/ideal_gas_c2p_test_data.csv\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28854fc9",
   "metadata": {},
   "source": [
    "Get a first input for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374dec08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.20413115, 12.02658484, 22.13129693])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([df[\"D\"][0], df[\"S\"][0], df[\"tau\"][0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4770318",
   "metadata": {},
   "source": [
    "Check inference on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f48643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.866582\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(torch.from_numpy(x).float()).numpy()\n",
    "    print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39a0486d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[-0.3666,  0.4540, -0.4352],\n",
       "                      [ 0.0356,  0.9699,  0.4002],\n",
       "                      [ 0.1087, -0.0912,  0.1072],\n",
       "                      ...,\n",
       "                      [ 0.5475, -0.5256, -0.2971],\n",
       "                      [-0.4310, -0.1393,  0.2783],\n",
       "                      [ 0.6337, -0.3134,  0.0312]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([ 5.6061e-01,  2.4333e-01, -7.7467e-01, -3.2593e-01,  4.9073e-02,\n",
       "                       7.0926e-03,  2.2542e-01, -5.3934e-01,  2.5813e-01, -3.1620e-01,\n",
       "                      -4.9130e-01, -3.7876e-01, -2.5505e-01,  7.9304e-01,  7.3299e-01,\n",
       "                       1.0455e-01,  8.8842e-01,  1.4683e-01, -4.3929e-01, -3.4603e-01,\n",
       "                      -6.0459e-01, -7.6528e-01, -4.8161e-01, -4.2301e-01, -1.1967e-01,\n",
       "                      -7.1304e-01,  2.7122e-01,  1.1723e-01,  5.4620e-01, -1.7614e-01,\n",
       "                      -1.1986e-02, -3.2823e-01, -1.2451e-01, -2.4551e-01,  5.5276e-01,\n",
       "                      -1.3098e-01,  3.2348e-01, -5.8645e-01,  5.6136e-02, -2.5985e-01,\n",
       "                       3.8681e-01,  8.3229e-01, -2.3454e-01,  5.3122e-01,  4.7994e-03,\n",
       "                       1.4047e-01, -3.4001e-01,  2.0402e-01,  6.8717e-02, -7.6738e-01,\n",
       "                       7.2325e-01,  4.5350e-01,  1.4957e-01, -7.9855e-01, -6.0024e-01,\n",
       "                       5.3511e-01, -4.5407e-01, -5.5787e-01,  2.6786e-01, -5.6681e-01,\n",
       "                      -1.8479e-01,  6.8825e-01, -1.0905e-01, -2.3895e-01,  5.6507e-01,\n",
       "                      -2.4706e-01, -8.0426e-02, -2.7791e-01,  1.4884e-01, -3.7208e-01,\n",
       "                      -5.0654e-01,  1.5907e-01,  9.4968e-02, -2.5627e-01,  2.5145e-01,\n",
       "                       9.9624e-02,  6.1657e-01, -1.6875e-01,  4.0218e-01, -7.7376e-01,\n",
       "                       9.7056e-02,  7.6158e-03,  4.0075e-01,  3.5677e-04, -5.5413e-01,\n",
       "                      -6.7252e-01, -1.5020e-01, -5.0897e-01, -4.4934e-01, -6.6589e-01,\n",
       "                      -8.1114e-01,  8.5806e-01, -4.1454e-01, -9.7778e-02, -9.7389e-02,\n",
       "                       9.8068e-02, -3.0615e-01, -2.3949e-02, -1.6122e-01,  4.6998e-01,\n",
       "                      -5.5137e-01, -6.1980e-01, -3.9145e-01,  8.6247e-02,  5.5046e-01,\n",
       "                       1.5668e-01, -2.7502e-01, -1.8127e-01, -4.9198e-01, -2.0787e-01,\n",
       "                       1.6453e-01, -2.4607e-01,  5.3280e-01, -7.8030e-01,  7.3768e-01,\n",
       "                      -4.4428e-02,  4.1386e-01,  2.6073e-01, -5.3616e-01, -6.7525e-01,\n",
       "                      -6.2794e-01, -3.0024e-02,  1.9229e-01, -4.0597e-03,  5.8490e-01,\n",
       "                       1.2414e-01, -6.2282e-01, -3.3803e-01, -4.7853e-01,  1.5441e-01,\n",
       "                       6.4502e-01,  6.5766e-01, -5.5193e-01, -7.5390e-01,  1.3894e-01,\n",
       "                       9.8502e-01,  1.8964e-01,  5.1192e-01,  1.9887e-01,  2.5534e-01,\n",
       "                       6.9651e-01,  2.3346e-01,  7.9083e-01,  2.2313e-01,  6.2242e-01,\n",
       "                       3.0691e-01,  4.4167e-02, -5.8303e-01, -6.7877e-01, -3.1687e-01,\n",
       "                      -2.7313e-01, -5.5454e-01, -5.9223e-01,  6.7415e-01, -5.3458e-02,\n",
       "                      -3.2087e-01, -2.8056e-01,  1.4163e-01,  8.2622e-01, -6.9740e-02,\n",
       "                      -1.1789e-01, -3.5249e-01,  2.2896e-01,  8.9731e-02, -3.4253e-01,\n",
       "                      -5.8954e-01, -2.2534e-01,  3.8568e-01, -5.1930e-01,  1.9953e-01,\n",
       "                       6.1900e-03,  2.6773e-01, -6.8539e-01, -2.8038e-01, -3.7443e-01,\n",
       "                       3.5219e-01, -6.6597e-02, -7.2470e-02,  3.8389e-01, -2.4917e-01,\n",
       "                       2.9217e-01, -8.1484e-01, -4.2813e-02, -1.1196e-01, -6.4715e-01,\n",
       "                      -4.2079e-01,  5.5912e-02,  4.5545e-01,  2.4642e-01, -5.7082e-02,\n",
       "                       4.4132e-01,  3.0203e-01,  3.8295e-01, -8.3331e-01, -7.9781e-02,\n",
       "                      -3.6571e-01,  2.6059e-01,  2.8052e-01,  2.1758e-01, -7.8758e-01,\n",
       "                      -1.4521e-01,  1.0382e-01, -4.5079e-01, -4.8731e-01,  3.5570e-02,\n",
       "                      -6.6347e-01,  8.1051e-02,  4.9484e-01,  3.5027e-01, -3.8311e-01,\n",
       "                      -5.3328e-01, -3.4040e-01,  1.6403e-01, -7.8798e-01, -4.3605e-01,\n",
       "                      -2.1721e-01,  4.9041e-01, -2.2180e-01, -1.7955e-01,  3.9273e-01,\n",
       "                      -1.5853e-01, -7.3876e-02,  3.8864e-02, -6.2598e-02, -8.1087e-02,\n",
       "                      -1.9798e-01, -6.5552e-01, -5.6502e-01,  3.7701e-01, -3.4392e-01,\n",
       "                       8.2957e-02, -4.8004e-01, -4.6550e-02, -5.9087e-01,  3.6662e-02,\n",
       "                       3.4451e-01, -8.8631e-02, -5.3218e-01, -7.9140e-02,  3.0321e-02,\n",
       "                       1.8166e-02, -1.5071e-01,  2.6164e-01, -6.4562e-02, -7.7570e-01,\n",
       "                       7.2120e-01, -8.5811e-01,  6.0622e-02, -4.8881e-02,  2.1444e-01,\n",
       "                      -5.9472e-01,  3.2926e-01,  7.5049e-01, -2.9735e-01, -4.2053e-01,\n",
       "                      -4.7628e-01,  7.4014e-01, -2.4838e-01, -2.9999e-01, -7.9381e-01,\n",
       "                      -5.7938e-01,  6.5448e-01,  5.6605e-01, -1.5443e-02, -7.7997e-01,\n",
       "                      -4.0222e-01, -1.9633e-01,  8.7221e-01, -2.4332e-01, -3.8246e-01,\n",
       "                       2.6872e-02, -1.7040e-01, -1.9700e-01, -5.1869e-01, -8.3258e-01,\n",
       "                      -5.2976e-01, -2.3812e-01,  4.4821e-02, -2.0142e-02, -5.2488e-02,\n",
       "                      -6.7306e-01, -7.7793e-02,  8.1652e-03,  4.9168e-02,  3.8233e-01,\n",
       "                       1.7576e-01, -6.6489e-01, -6.6914e-02,  1.8089e-01, -3.5073e-01,\n",
       "                       6.0601e-01,  6.7418e-01, -5.1596e-01, -4.4838e-02, -1.4989e-02,\n",
       "                       5.6407e-01, -5.0503e-01,  1.0544e-01,  1.3190e-02,  5.4493e-01,\n",
       "                      -1.4568e-01, -1.6339e-04,  8.3324e-02, -2.6391e-01,  1.2711e-01,\n",
       "                       1.7127e-01,  5.0796e-02, -4.1261e-01,  3.2490e-01,  2.7839e-01,\n",
       "                      -3.8546e-01,  1.4934e-01,  3.3574e-01, -4.5479e-01, -3.8480e-01,\n",
       "                       2.7638e-01,  4.5449e-01,  4.5720e-01,  2.5101e-01,  4.7465e-01,\n",
       "                      -3.1147e-01, -6.8385e-02,  8.2914e-01,  2.4961e-02, -4.9627e-01,\n",
       "                       2.0639e-01, -1.5687e-01,  4.2896e-01, -7.5359e-01,  3.9584e-01,\n",
       "                      -1.3844e-01,  5.7653e-01, -6.2221e-01, -1.2231e-01, -8.9451e-01,\n",
       "                      -1.9282e-01,  4.3169e-01,  6.8659e-01,  5.0441e-01, -8.1701e-02,\n",
       "                       2.0244e-01,  5.0619e-01, -5.5520e-01, -5.9935e-01,  8.1521e-01,\n",
       "                       6.0223e-01,  1.6424e-01,  2.3404e-01, -6.3086e-01,  4.0273e-01,\n",
       "                      -1.1020e-01, -4.9546e-01, -3.3658e-02, -3.8937e-01, -2.1455e-01,\n",
       "                      -3.4698e-01,  7.5002e-01, -1.0651e-01,  7.2249e-01, -7.4550e-01,\n",
       "                      -5.1744e-01,  5.1015e-01, -7.6862e-01,  5.4379e-02,  1.4252e-01,\n",
       "                       1.5078e-01,  2.2445e-02, -4.9472e-01,  8.2544e-01, -1.2985e-02,\n",
       "                      -1.1099e-01, -6.5773e-01,  7.8922e-01, -4.5647e-01, -2.0565e-01,\n",
       "                       1.8342e-01, -3.6585e-01,  2.3095e-01,  2.2266e-01, -6.6948e-02,\n",
       "                      -1.4292e-01,  4.6304e-01,  2.7524e-01, -2.6237e-01,  5.2672e-01,\n",
       "                       2.7830e-01, -3.2785e-02, -2.4114e-01, -6.4739e-01,  4.9275e-01,\n",
       "                      -5.5368e-01, -2.8899e-01,  7.5072e-01,  8.0340e-01, -2.1609e-01,\n",
       "                       6.1026e-01,  6.6894e-02,  7.5997e-01,  5.9304e-02,  2.0161e-01,\n",
       "                      -5.3993e-01, -1.3188e-01,  3.8199e-01,  3.9901e-01, -5.9074e-01,\n",
       "                      -6.5068e-01, -6.8939e-01, -7.4917e-01, -2.7302e-01,  1.5330e-01,\n",
       "                       1.3607e-01,  4.7892e-01, -2.3037e-01, -5.8969e-01,  6.3816e-01,\n",
       "                      -3.4101e-01,  1.8069e-01,  4.8082e-01, -3.5878e-01,  4.6800e-01,\n",
       "                       5.3187e-01,  2.2503e-01, -3.7638e-01, -2.0842e-01,  3.8425e-01,\n",
       "                      -3.3656e-01, -4.2364e-01,  2.6078e-01,  1.8838e-01,  3.7220e-01,\n",
       "                       1.4417e-01, -1.0077e-01, -1.0681e-01,  1.2787e-01,  3.7760e-01,\n",
       "                      -2.5847e-02,  2.6523e-01,  6.8195e-01, -4.9772e-01,  1.4203e-01,\n",
       "                      -7.0256e-01, -3.8349e-01,  2.2317e-01,  4.7887e-01, -4.4954e-02,\n",
       "                       5.8802e-01, -5.2608e-01, -1.8691e-01, -3.9720e-01,  5.5767e-02,\n",
       "                       2.2479e-01,  1.5030e-01, -4.8814e-01, -1.8910e-01, -2.3814e-01,\n",
       "                      -5.6490e-02,  3.1868e-01, -1.0439e-01, -2.5642e-01, -5.2558e-02,\n",
       "                      -9.7838e-03,  5.0738e-02,  4.8262e-01,  4.6405e-01, -7.9877e-01,\n",
       "                      -1.6047e-01,  8.0333e-01,  6.2192e-01, -3.2752e-01, -2.9073e-02,\n",
       "                       5.5172e-01, -7.0825e-01,  6.4921e-01, -4.7262e-01,  2.3057e-01,\n",
       "                      -1.8620e-01, -2.9797e-01,  3.4108e-01,  2.6027e-01, -1.0116e-02,\n",
       "                       1.7032e-01,  3.6078e-01,  2.2924e-03,  8.2963e-01, -8.0770e-01,\n",
       "                       1.2874e-01,  5.9890e-01,  9.3857e-02,  5.7788e-01, -3.0665e-01,\n",
       "                      -2.2959e-01,  3.9055e-01, -3.5936e-01, -3.8635e-01,  2.9675e-01,\n",
       "                       5.4247e-01,  1.4175e-01, -2.3542e-01, -4.5953e-01,  6.5254e-01,\n",
       "                      -1.8286e-01,  7.3629e-01, -6.9211e-01, -1.7368e-01])),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.1975,  0.0353,  0.0494,  ..., -0.1239,  0.0699, -0.0145],\n",
       "                      [-0.4241, -0.0561,  0.0716,  ..., -0.2802,  0.1693, -0.0044],\n",
       "                      [ 0.1471, -0.0358, -0.0640,  ...,  0.0976, -0.0556, -0.0480],\n",
       "                      ...,\n",
       "                      [-0.2535, -0.0136,  0.0662,  ..., -0.0097,  0.0494, -0.0094],\n",
       "                      [-0.2617, -0.0677,  0.0799,  ..., -0.0303,  0.0620,  0.0430],\n",
       "                      [-0.1746,  0.0117,  0.0723,  ..., -0.0221,  0.0402, -0.0008]])),\n",
       "             ('linear2.bias',\n",
       "              tensor([-0.0300, -0.0085, -0.0241,  0.0045, -0.0497,  0.0358, -0.0348,  0.0416,\n",
       "                       0.0006, -0.0533, -0.0059, -0.0452, -0.0490, -0.0718, -0.0193, -0.0061,\n",
       "                      -0.0623,  0.0067,  0.0112, -0.0616,  0.0002, -0.0232, -0.0453,  0.0438,\n",
       "                      -0.0642, -0.0277,  0.0230, -0.0351, -0.0443,  0.0093, -0.0409,  0.0191,\n",
       "                       0.0319, -0.0232,  0.0492, -0.0231,  0.0078, -0.0039, -0.0186,  0.0386,\n",
       "                       0.0195, -0.0284,  0.0273,  0.0323,  0.0024, -0.0493, -0.0524, -0.0335,\n",
       "                      -0.0273, -0.0532,  0.0267,  0.0069,  0.0181, -0.0636,  0.0055, -0.0193,\n",
       "                       0.0277,  0.0011,  0.0368, -0.0007,  0.0060,  0.0059,  0.0064, -0.0231,\n",
       "                      -0.0267, -0.0249,  0.0117, -0.0349,  0.0416, -0.0025,  0.0137, -0.0035,\n",
       "                      -0.0041, -0.0428,  0.0353,  0.0131,  0.0087, -0.0124,  0.0059,  0.0283,\n",
       "                      -0.0612,  0.0239,  0.0179,  0.0107,  0.0102,  0.0181,  0.0094,  0.0259,\n",
       "                      -0.0393,  0.0382, -0.0052,  0.0420, -0.0020, -0.0486, -0.0345, -0.0424,\n",
       "                       0.0024, -0.0560,  0.0400,  0.0057, -0.0556,  0.0243, -0.0183, -0.0325,\n",
       "                      -0.0495,  0.0488,  0.0240,  0.0243,  0.0174,  0.0286,  0.0159, -0.0456,\n",
       "                       0.0775, -0.0148,  0.0138,  0.0292,  0.0039, -0.0163,  0.0299, -0.0100,\n",
       "                       0.0749, -0.0488, -0.0056,  0.0238,  0.0060,  0.0120, -0.0412])),\n",
       "             ('linear3.weight',\n",
       "              tensor([[ 0.1659,  0.4207, -0.0723,  0.1485,  0.1768,  0.1228,  0.1697,  0.1643,\n",
       "                        0.1911,  0.2465,  0.1654,  0.1752, -0.1662,  0.2802,  0.1686,  0.1902,\n",
       "                       -0.0697, -0.0929, -0.0764,  0.2651, -0.0498,  0.1440, -0.0490,  0.0517,\n",
       "                        0.2108,  0.1801,  0.1701,  0.1790,  0.1938,  0.2001,  0.1823, -0.0895,\n",
       "                        0.1778,  0.1762,  0.1151,  0.1629,  0.1878, -0.0915,  0.1676,  0.1819,\n",
       "                        0.1876,  0.1844,  0.1468,  0.1705,  0.1832,  0.2991, -0.0671,  0.2047,\n",
       "                        0.3116,  0.1763,  0.1865,  0.2152,  0.1803,  0.1918,  0.1751,  0.1733,\n",
       "                        0.1802,  0.1476,  0.1857, -0.0661,  0.1681, -0.0706,  0.2169,  0.2045,\n",
       "                        0.1892,  0.1710,  0.1808,  0.2226,  0.1674,  0.1740,  0.2237,  0.1709,\n",
       "                       -0.0659,  0.1645,  0.1679,  0.1800, -0.0801, -0.0788,  0.1589,  0.1485,\n",
       "                        0.1957,  0.1487,  0.2049,  0.1854,  0.2007, -0.0776,  0.1780, -0.0816,\n",
       "                        0.1797,  0.1808,  0.1911,  0.1353,  0.2493,  0.1800,  0.1730,  0.2371,\n",
       "                       -0.0678,  0.1777,  0.1875,  0.1714,  0.2067,  0.2462,  0.1672,  0.1643,\n",
       "                        0.1753,  0.0615,  0.1566, -0.0823,  0.1724,  0.1716,  0.1895, -0.0603,\n",
       "                        0.2348,  0.1817,  0.1633,  0.1655, -0.0646, -0.0465,  0.1554,  0.1483,\n",
       "                        0.1100, -0.0740, -0.0917,  0.1508,  0.2362,  0.2279,  0.1881]])),\n",
       "             ('linear3.bias', tensor([0.1283]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f7562d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple feedforward neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, h: list = [3, 600, 200, 1], activation_function = nn.Sigmoid, output_bias=True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network class.\n",
    "        \"\"\"\n",
    "        # Call the super constructor first\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        # For convenience, save the sizes of the hidden layers as fields as well\n",
    "        self.h = h\n",
    "\n",
    "        # Define the layers:\n",
    "        for i in range(len(self.h)-1):\n",
    "            if i == len(self.h)-2:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h[i], self.h[i+1], bias=output_bias))\n",
    "            else:\n",
    "                setattr(self, f\"linear{i+1}\", nn.Linear(self.h[i], self.h[i+1]))\n",
    "                setattr(self, f\"activation{i+1}\", activation_function())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward step given the input x.\n",
    "        :param x: Input for the neural network.\n",
    "        :return: x: Output neural network\n",
    "        \"\"\"\n",
    "\n",
    "        for i, module in enumerate(self.modules()):\n",
    "            # The first module is the whole NNC2P object, continue\n",
    "            if i == 0:\n",
    "                continue\n",
    "            x = module(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73bb8cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNetwork(\n",
       "  (linear1): Linear(in_features=3, out_features=504, bias=True)\n",
       "  (activation1): Sigmoid()\n",
       "  (linear2): Linear(in_features=504, out_features=127, bias=True)\n",
       "  (activation2): Sigmoid()\n",
       "  (linear3): Linear(in_features=127, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc2afa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sigmoid()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.activation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ae56c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = FeedForwardNetwork(h=[3, 504, 127, 1])\n",
    "new_state_dict = new_model.state_dict()\n",
    "for key in model.state_dict():\n",
    "    new_state_dict[key] = model.state_dict()[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4485b710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[-0.3666,  0.4540, -0.4352],\n",
       "                      [ 0.0356,  0.9699,  0.4002],\n",
       "                      [ 0.1087, -0.0912,  0.1072],\n",
       "                      ...,\n",
       "                      [ 0.5475, -0.5256, -0.2971],\n",
       "                      [-0.4310, -0.1393,  0.2783],\n",
       "                      [ 0.6337, -0.3134,  0.0312]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([ 5.6061e-01,  2.4333e-01, -7.7467e-01, -3.2593e-01,  4.9073e-02,\n",
       "                       7.0926e-03,  2.2542e-01, -5.3934e-01,  2.5813e-01, -3.1620e-01,\n",
       "                      -4.9130e-01, -3.7876e-01, -2.5505e-01,  7.9304e-01,  7.3299e-01,\n",
       "                       1.0455e-01,  8.8842e-01,  1.4683e-01, -4.3929e-01, -3.4603e-01,\n",
       "                      -6.0459e-01, -7.6528e-01, -4.8161e-01, -4.2301e-01, -1.1967e-01,\n",
       "                      -7.1304e-01,  2.7122e-01,  1.1723e-01,  5.4620e-01, -1.7614e-01,\n",
       "                      -1.1986e-02, -3.2823e-01, -1.2451e-01, -2.4551e-01,  5.5276e-01,\n",
       "                      -1.3098e-01,  3.2348e-01, -5.8645e-01,  5.6136e-02, -2.5985e-01,\n",
       "                       3.8681e-01,  8.3229e-01, -2.3454e-01,  5.3122e-01,  4.7994e-03,\n",
       "                       1.4047e-01, -3.4001e-01,  2.0402e-01,  6.8717e-02, -7.6738e-01,\n",
       "                       7.2325e-01,  4.5350e-01,  1.4957e-01, -7.9855e-01, -6.0024e-01,\n",
       "                       5.3511e-01, -4.5407e-01, -5.5787e-01,  2.6786e-01, -5.6681e-01,\n",
       "                      -1.8479e-01,  6.8825e-01, -1.0905e-01, -2.3895e-01,  5.6507e-01,\n",
       "                      -2.4706e-01, -8.0426e-02, -2.7791e-01,  1.4884e-01, -3.7208e-01,\n",
       "                      -5.0654e-01,  1.5907e-01,  9.4968e-02, -2.5627e-01,  2.5145e-01,\n",
       "                       9.9624e-02,  6.1657e-01, -1.6875e-01,  4.0218e-01, -7.7376e-01,\n",
       "                       9.7056e-02,  7.6158e-03,  4.0075e-01,  3.5677e-04, -5.5413e-01,\n",
       "                      -6.7252e-01, -1.5020e-01, -5.0897e-01, -4.4934e-01, -6.6589e-01,\n",
       "                      -8.1114e-01,  8.5806e-01, -4.1454e-01, -9.7778e-02, -9.7389e-02,\n",
       "                       9.8068e-02, -3.0615e-01, -2.3949e-02, -1.6122e-01,  4.6998e-01,\n",
       "                      -5.5137e-01, -6.1980e-01, -3.9145e-01,  8.6247e-02,  5.5046e-01,\n",
       "                       1.5668e-01, -2.7502e-01, -1.8127e-01, -4.9198e-01, -2.0787e-01,\n",
       "                       1.6453e-01, -2.4607e-01,  5.3280e-01, -7.8030e-01,  7.3768e-01,\n",
       "                      -4.4428e-02,  4.1386e-01,  2.6073e-01, -5.3616e-01, -6.7525e-01,\n",
       "                      -6.2794e-01, -3.0024e-02,  1.9229e-01, -4.0597e-03,  5.8490e-01,\n",
       "                       1.2414e-01, -6.2282e-01, -3.3803e-01, -4.7853e-01,  1.5441e-01,\n",
       "                       6.4502e-01,  6.5766e-01, -5.5193e-01, -7.5390e-01,  1.3894e-01,\n",
       "                       9.8502e-01,  1.8964e-01,  5.1192e-01,  1.9887e-01,  2.5534e-01,\n",
       "                       6.9651e-01,  2.3346e-01,  7.9083e-01,  2.2313e-01,  6.2242e-01,\n",
       "                       3.0691e-01,  4.4167e-02, -5.8303e-01, -6.7877e-01, -3.1687e-01,\n",
       "                      -2.7313e-01, -5.5454e-01, -5.9223e-01,  6.7415e-01, -5.3458e-02,\n",
       "                      -3.2087e-01, -2.8056e-01,  1.4163e-01,  8.2622e-01, -6.9740e-02,\n",
       "                      -1.1789e-01, -3.5249e-01,  2.2896e-01,  8.9731e-02, -3.4253e-01,\n",
       "                      -5.8954e-01, -2.2534e-01,  3.8568e-01, -5.1930e-01,  1.9953e-01,\n",
       "                       6.1900e-03,  2.6773e-01, -6.8539e-01, -2.8038e-01, -3.7443e-01,\n",
       "                       3.5219e-01, -6.6597e-02, -7.2470e-02,  3.8389e-01, -2.4917e-01,\n",
       "                       2.9217e-01, -8.1484e-01, -4.2813e-02, -1.1196e-01, -6.4715e-01,\n",
       "                      -4.2079e-01,  5.5912e-02,  4.5545e-01,  2.4642e-01, -5.7082e-02,\n",
       "                       4.4132e-01,  3.0203e-01,  3.8295e-01, -8.3331e-01, -7.9781e-02,\n",
       "                      -3.6571e-01,  2.6059e-01,  2.8052e-01,  2.1758e-01, -7.8758e-01,\n",
       "                      -1.4521e-01,  1.0382e-01, -4.5079e-01, -4.8731e-01,  3.5570e-02,\n",
       "                      -6.6347e-01,  8.1051e-02,  4.9484e-01,  3.5027e-01, -3.8311e-01,\n",
       "                      -5.3328e-01, -3.4040e-01,  1.6403e-01, -7.8798e-01, -4.3605e-01,\n",
       "                      -2.1721e-01,  4.9041e-01, -2.2180e-01, -1.7955e-01,  3.9273e-01,\n",
       "                      -1.5853e-01, -7.3876e-02,  3.8864e-02, -6.2598e-02, -8.1087e-02,\n",
       "                      -1.9798e-01, -6.5552e-01, -5.6502e-01,  3.7701e-01, -3.4392e-01,\n",
       "                       8.2957e-02, -4.8004e-01, -4.6550e-02, -5.9087e-01,  3.6662e-02,\n",
       "                       3.4451e-01, -8.8631e-02, -5.3218e-01, -7.9140e-02,  3.0321e-02,\n",
       "                       1.8166e-02, -1.5071e-01,  2.6164e-01, -6.4562e-02, -7.7570e-01,\n",
       "                       7.2120e-01, -8.5811e-01,  6.0622e-02, -4.8881e-02,  2.1444e-01,\n",
       "                      -5.9472e-01,  3.2926e-01,  7.5049e-01, -2.9735e-01, -4.2053e-01,\n",
       "                      -4.7628e-01,  7.4014e-01, -2.4838e-01, -2.9999e-01, -7.9381e-01,\n",
       "                      -5.7938e-01,  6.5448e-01,  5.6605e-01, -1.5443e-02, -7.7997e-01,\n",
       "                      -4.0222e-01, -1.9633e-01,  8.7221e-01, -2.4332e-01, -3.8246e-01,\n",
       "                       2.6872e-02, -1.7040e-01, -1.9700e-01, -5.1869e-01, -8.3258e-01,\n",
       "                      -5.2976e-01, -2.3812e-01,  4.4821e-02, -2.0142e-02, -5.2488e-02,\n",
       "                      -6.7306e-01, -7.7793e-02,  8.1652e-03,  4.9168e-02,  3.8233e-01,\n",
       "                       1.7576e-01, -6.6489e-01, -6.6914e-02,  1.8089e-01, -3.5073e-01,\n",
       "                       6.0601e-01,  6.7418e-01, -5.1596e-01, -4.4838e-02, -1.4989e-02,\n",
       "                       5.6407e-01, -5.0503e-01,  1.0544e-01,  1.3190e-02,  5.4493e-01,\n",
       "                      -1.4568e-01, -1.6339e-04,  8.3324e-02, -2.6391e-01,  1.2711e-01,\n",
       "                       1.7127e-01,  5.0796e-02, -4.1261e-01,  3.2490e-01,  2.7839e-01,\n",
       "                      -3.8546e-01,  1.4934e-01,  3.3574e-01, -4.5479e-01, -3.8480e-01,\n",
       "                       2.7638e-01,  4.5449e-01,  4.5720e-01,  2.5101e-01,  4.7465e-01,\n",
       "                      -3.1147e-01, -6.8385e-02,  8.2914e-01,  2.4961e-02, -4.9627e-01,\n",
       "                       2.0639e-01, -1.5687e-01,  4.2896e-01, -7.5359e-01,  3.9584e-01,\n",
       "                      -1.3844e-01,  5.7653e-01, -6.2221e-01, -1.2231e-01, -8.9451e-01,\n",
       "                      -1.9282e-01,  4.3169e-01,  6.8659e-01,  5.0441e-01, -8.1701e-02,\n",
       "                       2.0244e-01,  5.0619e-01, -5.5520e-01, -5.9935e-01,  8.1521e-01,\n",
       "                       6.0223e-01,  1.6424e-01,  2.3404e-01, -6.3086e-01,  4.0273e-01,\n",
       "                      -1.1020e-01, -4.9546e-01, -3.3658e-02, -3.8937e-01, -2.1455e-01,\n",
       "                      -3.4698e-01,  7.5002e-01, -1.0651e-01,  7.2249e-01, -7.4550e-01,\n",
       "                      -5.1744e-01,  5.1015e-01, -7.6862e-01,  5.4379e-02,  1.4252e-01,\n",
       "                       1.5078e-01,  2.2445e-02, -4.9472e-01,  8.2544e-01, -1.2985e-02,\n",
       "                      -1.1099e-01, -6.5773e-01,  7.8922e-01, -4.5647e-01, -2.0565e-01,\n",
       "                       1.8342e-01, -3.6585e-01,  2.3095e-01,  2.2266e-01, -6.6948e-02,\n",
       "                      -1.4292e-01,  4.6304e-01,  2.7524e-01, -2.6237e-01,  5.2672e-01,\n",
       "                       2.7830e-01, -3.2785e-02, -2.4114e-01, -6.4739e-01,  4.9275e-01,\n",
       "                      -5.5368e-01, -2.8899e-01,  7.5072e-01,  8.0340e-01, -2.1609e-01,\n",
       "                       6.1026e-01,  6.6894e-02,  7.5997e-01,  5.9304e-02,  2.0161e-01,\n",
       "                      -5.3993e-01, -1.3188e-01,  3.8199e-01,  3.9901e-01, -5.9074e-01,\n",
       "                      -6.5068e-01, -6.8939e-01, -7.4917e-01, -2.7302e-01,  1.5330e-01,\n",
       "                       1.3607e-01,  4.7892e-01, -2.3037e-01, -5.8969e-01,  6.3816e-01,\n",
       "                      -3.4101e-01,  1.8069e-01,  4.8082e-01, -3.5878e-01,  4.6800e-01,\n",
       "                       5.3187e-01,  2.2503e-01, -3.7638e-01, -2.0842e-01,  3.8425e-01,\n",
       "                      -3.3656e-01, -4.2364e-01,  2.6078e-01,  1.8838e-01,  3.7220e-01,\n",
       "                       1.4417e-01, -1.0077e-01, -1.0681e-01,  1.2787e-01,  3.7760e-01,\n",
       "                      -2.5847e-02,  2.6523e-01,  6.8195e-01, -4.9772e-01,  1.4203e-01,\n",
       "                      -7.0256e-01, -3.8349e-01,  2.2317e-01,  4.7887e-01, -4.4954e-02,\n",
       "                       5.8802e-01, -5.2608e-01, -1.8691e-01, -3.9720e-01,  5.5767e-02,\n",
       "                       2.2479e-01,  1.5030e-01, -4.8814e-01, -1.8910e-01, -2.3814e-01,\n",
       "                      -5.6490e-02,  3.1868e-01, -1.0439e-01, -2.5642e-01, -5.2558e-02,\n",
       "                      -9.7838e-03,  5.0738e-02,  4.8262e-01,  4.6405e-01, -7.9877e-01,\n",
       "                      -1.6047e-01,  8.0333e-01,  6.2192e-01, -3.2752e-01, -2.9073e-02,\n",
       "                       5.5172e-01, -7.0825e-01,  6.4921e-01, -4.7262e-01,  2.3057e-01,\n",
       "                      -1.8620e-01, -2.9797e-01,  3.4108e-01,  2.6027e-01, -1.0116e-02,\n",
       "                       1.7032e-01,  3.6078e-01,  2.2924e-03,  8.2963e-01, -8.0770e-01,\n",
       "                       1.2874e-01,  5.9890e-01,  9.3857e-02,  5.7788e-01, -3.0665e-01,\n",
       "                      -2.2959e-01,  3.9055e-01, -3.5936e-01, -3.8635e-01,  2.9675e-01,\n",
       "                       5.4247e-01,  1.4175e-01, -2.3542e-01, -4.5953e-01,  6.5254e-01,\n",
       "                      -1.8286e-01,  7.3629e-01, -6.9211e-01, -1.7368e-01])),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.1975,  0.0353,  0.0494,  ..., -0.1239,  0.0699, -0.0145],\n",
       "                      [-0.4241, -0.0561,  0.0716,  ..., -0.2802,  0.1693, -0.0044],\n",
       "                      [ 0.1471, -0.0358, -0.0640,  ...,  0.0976, -0.0556, -0.0480],\n",
       "                      ...,\n",
       "                      [-0.2535, -0.0136,  0.0662,  ..., -0.0097,  0.0494, -0.0094],\n",
       "                      [-0.2617, -0.0677,  0.0799,  ..., -0.0303,  0.0620,  0.0430],\n",
       "                      [-0.1746,  0.0117,  0.0723,  ..., -0.0221,  0.0402, -0.0008]])),\n",
       "             ('linear2.bias',\n",
       "              tensor([-0.0300, -0.0085, -0.0241,  0.0045, -0.0497,  0.0358, -0.0348,  0.0416,\n",
       "                       0.0006, -0.0533, -0.0059, -0.0452, -0.0490, -0.0718, -0.0193, -0.0061,\n",
       "                      -0.0623,  0.0067,  0.0112, -0.0616,  0.0002, -0.0232, -0.0453,  0.0438,\n",
       "                      -0.0642, -0.0277,  0.0230, -0.0351, -0.0443,  0.0093, -0.0409,  0.0191,\n",
       "                       0.0319, -0.0232,  0.0492, -0.0231,  0.0078, -0.0039, -0.0186,  0.0386,\n",
       "                       0.0195, -0.0284,  0.0273,  0.0323,  0.0024, -0.0493, -0.0524, -0.0335,\n",
       "                      -0.0273, -0.0532,  0.0267,  0.0069,  0.0181, -0.0636,  0.0055, -0.0193,\n",
       "                       0.0277,  0.0011,  0.0368, -0.0007,  0.0060,  0.0059,  0.0064, -0.0231,\n",
       "                      -0.0267, -0.0249,  0.0117, -0.0349,  0.0416, -0.0025,  0.0137, -0.0035,\n",
       "                      -0.0041, -0.0428,  0.0353,  0.0131,  0.0087, -0.0124,  0.0059,  0.0283,\n",
       "                      -0.0612,  0.0239,  0.0179,  0.0107,  0.0102,  0.0181,  0.0094,  0.0259,\n",
       "                      -0.0393,  0.0382, -0.0052,  0.0420, -0.0020, -0.0486, -0.0345, -0.0424,\n",
       "                       0.0024, -0.0560,  0.0400,  0.0057, -0.0556,  0.0243, -0.0183, -0.0325,\n",
       "                      -0.0495,  0.0488,  0.0240,  0.0243,  0.0174,  0.0286,  0.0159, -0.0456,\n",
       "                       0.0775, -0.0148,  0.0138,  0.0292,  0.0039, -0.0163,  0.0299, -0.0100,\n",
       "                       0.0749, -0.0488, -0.0056,  0.0238,  0.0060,  0.0120, -0.0412])),\n",
       "             ('linear3.weight',\n",
       "              tensor([[ 0.1659,  0.4207, -0.0723,  0.1485,  0.1768,  0.1228,  0.1697,  0.1643,\n",
       "                        0.1911,  0.2465,  0.1654,  0.1752, -0.1662,  0.2802,  0.1686,  0.1902,\n",
       "                       -0.0697, -0.0929, -0.0764,  0.2651, -0.0498,  0.1440, -0.0490,  0.0517,\n",
       "                        0.2108,  0.1801,  0.1701,  0.1790,  0.1938,  0.2001,  0.1823, -0.0895,\n",
       "                        0.1778,  0.1762,  0.1151,  0.1629,  0.1878, -0.0915,  0.1676,  0.1819,\n",
       "                        0.1876,  0.1844,  0.1468,  0.1705,  0.1832,  0.2991, -0.0671,  0.2047,\n",
       "                        0.3116,  0.1763,  0.1865,  0.2152,  0.1803,  0.1918,  0.1751,  0.1733,\n",
       "                        0.1802,  0.1476,  0.1857, -0.0661,  0.1681, -0.0706,  0.2169,  0.2045,\n",
       "                        0.1892,  0.1710,  0.1808,  0.2226,  0.1674,  0.1740,  0.2237,  0.1709,\n",
       "                       -0.0659,  0.1645,  0.1679,  0.1800, -0.0801, -0.0788,  0.1589,  0.1485,\n",
       "                        0.1957,  0.1487,  0.2049,  0.1854,  0.2007, -0.0776,  0.1780, -0.0816,\n",
       "                        0.1797,  0.1808,  0.1911,  0.1353,  0.2493,  0.1800,  0.1730,  0.2371,\n",
       "                       -0.0678,  0.1777,  0.1875,  0.1714,  0.2067,  0.2462,  0.1672,  0.1643,\n",
       "                        0.1753,  0.0615,  0.1566, -0.0823,  0.1724,  0.1716,  0.1895, -0.0603,\n",
       "                        0.2348,  0.1817,  0.1633,  0.1655, -0.0646, -0.0465,  0.1554,  0.1483,\n",
       "                        0.1100, -0.0740, -0.0917,  0.1508,  0.2362,  0.2279,  0.1881]])),\n",
       "             ('linear3.bias', tensor([0.1283]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae00f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[-0.3666,  0.4540, -0.4352],\n",
       "                      [ 0.0356,  0.9699,  0.4002],\n",
       "                      [ 0.1087, -0.0912,  0.1072],\n",
       "                      ...,\n",
       "                      [ 0.5475, -0.5256, -0.2971],\n",
       "                      [-0.4310, -0.1393,  0.2783],\n",
       "                      [ 0.6337, -0.3134,  0.0312]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([ 5.6061e-01,  2.4333e-01, -7.7467e-01, -3.2593e-01,  4.9073e-02,\n",
       "                       7.0926e-03,  2.2542e-01, -5.3934e-01,  2.5813e-01, -3.1620e-01,\n",
       "                      -4.9130e-01, -3.7876e-01, -2.5505e-01,  7.9304e-01,  7.3299e-01,\n",
       "                       1.0455e-01,  8.8842e-01,  1.4683e-01, -4.3929e-01, -3.4603e-01,\n",
       "                      -6.0459e-01, -7.6528e-01, -4.8161e-01, -4.2301e-01, -1.1967e-01,\n",
       "                      -7.1304e-01,  2.7122e-01,  1.1723e-01,  5.4620e-01, -1.7614e-01,\n",
       "                      -1.1986e-02, -3.2823e-01, -1.2451e-01, -2.4551e-01,  5.5276e-01,\n",
       "                      -1.3098e-01,  3.2348e-01, -5.8645e-01,  5.6136e-02, -2.5985e-01,\n",
       "                       3.8681e-01,  8.3229e-01, -2.3454e-01,  5.3122e-01,  4.7994e-03,\n",
       "                       1.4047e-01, -3.4001e-01,  2.0402e-01,  6.8717e-02, -7.6738e-01,\n",
       "                       7.2325e-01,  4.5350e-01,  1.4957e-01, -7.9855e-01, -6.0024e-01,\n",
       "                       5.3511e-01, -4.5407e-01, -5.5787e-01,  2.6786e-01, -5.6681e-01,\n",
       "                      -1.8479e-01,  6.8825e-01, -1.0905e-01, -2.3895e-01,  5.6507e-01,\n",
       "                      -2.4706e-01, -8.0426e-02, -2.7791e-01,  1.4884e-01, -3.7208e-01,\n",
       "                      -5.0654e-01,  1.5907e-01,  9.4968e-02, -2.5627e-01,  2.5145e-01,\n",
       "                       9.9624e-02,  6.1657e-01, -1.6875e-01,  4.0218e-01, -7.7376e-01,\n",
       "                       9.7056e-02,  7.6158e-03,  4.0075e-01,  3.5677e-04, -5.5413e-01,\n",
       "                      -6.7252e-01, -1.5020e-01, -5.0897e-01, -4.4934e-01, -6.6589e-01,\n",
       "                      -8.1114e-01,  8.5806e-01, -4.1454e-01, -9.7778e-02, -9.7389e-02,\n",
       "                       9.8068e-02, -3.0615e-01, -2.3949e-02, -1.6122e-01,  4.6998e-01,\n",
       "                      -5.5137e-01, -6.1980e-01, -3.9145e-01,  8.6247e-02,  5.5046e-01,\n",
       "                       1.5668e-01, -2.7502e-01, -1.8127e-01, -4.9198e-01, -2.0787e-01,\n",
       "                       1.6453e-01, -2.4607e-01,  5.3280e-01, -7.8030e-01,  7.3768e-01,\n",
       "                      -4.4428e-02,  4.1386e-01,  2.6073e-01, -5.3616e-01, -6.7525e-01,\n",
       "                      -6.2794e-01, -3.0024e-02,  1.9229e-01, -4.0597e-03,  5.8490e-01,\n",
       "                       1.2414e-01, -6.2282e-01, -3.3803e-01, -4.7853e-01,  1.5441e-01,\n",
       "                       6.4502e-01,  6.5766e-01, -5.5193e-01, -7.5390e-01,  1.3894e-01,\n",
       "                       9.8502e-01,  1.8964e-01,  5.1192e-01,  1.9887e-01,  2.5534e-01,\n",
       "                       6.9651e-01,  2.3346e-01,  7.9083e-01,  2.2313e-01,  6.2242e-01,\n",
       "                       3.0691e-01,  4.4167e-02, -5.8303e-01, -6.7877e-01, -3.1687e-01,\n",
       "                      -2.7313e-01, -5.5454e-01, -5.9223e-01,  6.7415e-01, -5.3458e-02,\n",
       "                      -3.2087e-01, -2.8056e-01,  1.4163e-01,  8.2622e-01, -6.9740e-02,\n",
       "                      -1.1789e-01, -3.5249e-01,  2.2896e-01,  8.9731e-02, -3.4253e-01,\n",
       "                      -5.8954e-01, -2.2534e-01,  3.8568e-01, -5.1930e-01,  1.9953e-01,\n",
       "                       6.1900e-03,  2.6773e-01, -6.8539e-01, -2.8038e-01, -3.7443e-01,\n",
       "                       3.5219e-01, -6.6597e-02, -7.2470e-02,  3.8389e-01, -2.4917e-01,\n",
       "                       2.9217e-01, -8.1484e-01, -4.2813e-02, -1.1196e-01, -6.4715e-01,\n",
       "                      -4.2079e-01,  5.5912e-02,  4.5545e-01,  2.4642e-01, -5.7082e-02,\n",
       "                       4.4132e-01,  3.0203e-01,  3.8295e-01, -8.3331e-01, -7.9781e-02,\n",
       "                      -3.6571e-01,  2.6059e-01,  2.8052e-01,  2.1758e-01, -7.8758e-01,\n",
       "                      -1.4521e-01,  1.0382e-01, -4.5079e-01, -4.8731e-01,  3.5570e-02,\n",
       "                      -6.6347e-01,  8.1051e-02,  4.9484e-01,  3.5027e-01, -3.8311e-01,\n",
       "                      -5.3328e-01, -3.4040e-01,  1.6403e-01, -7.8798e-01, -4.3605e-01,\n",
       "                      -2.1721e-01,  4.9041e-01, -2.2180e-01, -1.7955e-01,  3.9273e-01,\n",
       "                      -1.5853e-01, -7.3876e-02,  3.8864e-02, -6.2598e-02, -8.1087e-02,\n",
       "                      -1.9798e-01, -6.5552e-01, -5.6502e-01,  3.7701e-01, -3.4392e-01,\n",
       "                       8.2957e-02, -4.8004e-01, -4.6550e-02, -5.9087e-01,  3.6662e-02,\n",
       "                       3.4451e-01, -8.8631e-02, -5.3218e-01, -7.9140e-02,  3.0321e-02,\n",
       "                       1.8166e-02, -1.5071e-01,  2.6164e-01, -6.4562e-02, -7.7570e-01,\n",
       "                       7.2120e-01, -8.5811e-01,  6.0622e-02, -4.8881e-02,  2.1444e-01,\n",
       "                      -5.9472e-01,  3.2926e-01,  7.5049e-01, -2.9735e-01, -4.2053e-01,\n",
       "                      -4.7628e-01,  7.4014e-01, -2.4838e-01, -2.9999e-01, -7.9381e-01,\n",
       "                      -5.7938e-01,  6.5448e-01,  5.6605e-01, -1.5443e-02, -7.7997e-01,\n",
       "                      -4.0222e-01, -1.9633e-01,  8.7221e-01, -2.4332e-01, -3.8246e-01,\n",
       "                       2.6872e-02, -1.7040e-01, -1.9700e-01, -5.1869e-01, -8.3258e-01,\n",
       "                      -5.2976e-01, -2.3812e-01,  4.4821e-02, -2.0142e-02, -5.2488e-02,\n",
       "                      -6.7306e-01, -7.7793e-02,  8.1652e-03,  4.9168e-02,  3.8233e-01,\n",
       "                       1.7576e-01, -6.6489e-01, -6.6914e-02,  1.8089e-01, -3.5073e-01,\n",
       "                       6.0601e-01,  6.7418e-01, -5.1596e-01, -4.4838e-02, -1.4989e-02,\n",
       "                       5.6407e-01, -5.0503e-01,  1.0544e-01,  1.3190e-02,  5.4493e-01,\n",
       "                      -1.4568e-01, -1.6339e-04,  8.3324e-02, -2.6391e-01,  1.2711e-01,\n",
       "                       1.7127e-01,  5.0796e-02, -4.1261e-01,  3.2490e-01,  2.7839e-01,\n",
       "                      -3.8546e-01,  1.4934e-01,  3.3574e-01, -4.5479e-01, -3.8480e-01,\n",
       "                       2.7638e-01,  4.5449e-01,  4.5720e-01,  2.5101e-01,  4.7465e-01,\n",
       "                      -3.1147e-01, -6.8385e-02,  8.2914e-01,  2.4961e-02, -4.9627e-01,\n",
       "                       2.0639e-01, -1.5687e-01,  4.2896e-01, -7.5359e-01,  3.9584e-01,\n",
       "                      -1.3844e-01,  5.7653e-01, -6.2221e-01, -1.2231e-01, -8.9451e-01,\n",
       "                      -1.9282e-01,  4.3169e-01,  6.8659e-01,  5.0441e-01, -8.1701e-02,\n",
       "                       2.0244e-01,  5.0619e-01, -5.5520e-01, -5.9935e-01,  8.1521e-01,\n",
       "                       6.0223e-01,  1.6424e-01,  2.3404e-01, -6.3086e-01,  4.0273e-01,\n",
       "                      -1.1020e-01, -4.9546e-01, -3.3658e-02, -3.8937e-01, -2.1455e-01,\n",
       "                      -3.4698e-01,  7.5002e-01, -1.0651e-01,  7.2249e-01, -7.4550e-01,\n",
       "                      -5.1744e-01,  5.1015e-01, -7.6862e-01,  5.4379e-02,  1.4252e-01,\n",
       "                       1.5078e-01,  2.2445e-02, -4.9472e-01,  8.2544e-01, -1.2985e-02,\n",
       "                      -1.1099e-01, -6.5773e-01,  7.8922e-01, -4.5647e-01, -2.0565e-01,\n",
       "                       1.8342e-01, -3.6585e-01,  2.3095e-01,  2.2266e-01, -6.6948e-02,\n",
       "                      -1.4292e-01,  4.6304e-01,  2.7524e-01, -2.6237e-01,  5.2672e-01,\n",
       "                       2.7830e-01, -3.2785e-02, -2.4114e-01, -6.4739e-01,  4.9275e-01,\n",
       "                      -5.5368e-01, -2.8899e-01,  7.5072e-01,  8.0340e-01, -2.1609e-01,\n",
       "                       6.1026e-01,  6.6894e-02,  7.5997e-01,  5.9304e-02,  2.0161e-01,\n",
       "                      -5.3993e-01, -1.3188e-01,  3.8199e-01,  3.9901e-01, -5.9074e-01,\n",
       "                      -6.5068e-01, -6.8939e-01, -7.4917e-01, -2.7302e-01,  1.5330e-01,\n",
       "                       1.3607e-01,  4.7892e-01, -2.3037e-01, -5.8969e-01,  6.3816e-01,\n",
       "                      -3.4101e-01,  1.8069e-01,  4.8082e-01, -3.5878e-01,  4.6800e-01,\n",
       "                       5.3187e-01,  2.2503e-01, -3.7638e-01, -2.0842e-01,  3.8425e-01,\n",
       "                      -3.3656e-01, -4.2364e-01,  2.6078e-01,  1.8838e-01,  3.7220e-01,\n",
       "                       1.4417e-01, -1.0077e-01, -1.0681e-01,  1.2787e-01,  3.7760e-01,\n",
       "                      -2.5847e-02,  2.6523e-01,  6.8195e-01, -4.9772e-01,  1.4203e-01,\n",
       "                      -7.0256e-01, -3.8349e-01,  2.2317e-01,  4.7887e-01, -4.4954e-02,\n",
       "                       5.8802e-01, -5.2608e-01, -1.8691e-01, -3.9720e-01,  5.5767e-02,\n",
       "                       2.2479e-01,  1.5030e-01, -4.8814e-01, -1.8910e-01, -2.3814e-01,\n",
       "                      -5.6490e-02,  3.1868e-01, -1.0439e-01, -2.5642e-01, -5.2558e-02,\n",
       "                      -9.7838e-03,  5.0738e-02,  4.8262e-01,  4.6405e-01, -7.9877e-01,\n",
       "                      -1.6047e-01,  8.0333e-01,  6.2192e-01, -3.2752e-01, -2.9073e-02,\n",
       "                       5.5172e-01, -7.0825e-01,  6.4921e-01, -4.7262e-01,  2.3057e-01,\n",
       "                      -1.8620e-01, -2.9797e-01,  3.4108e-01,  2.6027e-01, -1.0116e-02,\n",
       "                       1.7032e-01,  3.6078e-01,  2.2924e-03,  8.2963e-01, -8.0770e-01,\n",
       "                       1.2874e-01,  5.9890e-01,  9.3857e-02,  5.7788e-01, -3.0665e-01,\n",
       "                      -2.2959e-01,  3.9055e-01, -3.5936e-01, -3.8635e-01,  2.9675e-01,\n",
       "                       5.4247e-01,  1.4175e-01, -2.3542e-01, -4.5953e-01,  6.5254e-01,\n",
       "                      -1.8286e-01,  7.3629e-01, -6.9211e-01, -1.7368e-01])),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.1975,  0.0353,  0.0494,  ..., -0.1239,  0.0699, -0.0145],\n",
       "                      [-0.4241, -0.0561,  0.0716,  ..., -0.2802,  0.1693, -0.0044],\n",
       "                      [ 0.1471, -0.0358, -0.0640,  ...,  0.0976, -0.0556, -0.0480],\n",
       "                      ...,\n",
       "                      [-0.2535, -0.0136,  0.0662,  ..., -0.0097,  0.0494, -0.0094],\n",
       "                      [-0.2617, -0.0677,  0.0799,  ..., -0.0303,  0.0620,  0.0430],\n",
       "                      [-0.1746,  0.0117,  0.0723,  ..., -0.0221,  0.0402, -0.0008]])),\n",
       "             ('linear2.bias',\n",
       "              tensor([-0.0300, -0.0085, -0.0241,  0.0045, -0.0497,  0.0358, -0.0348,  0.0416,\n",
       "                       0.0006, -0.0533, -0.0059, -0.0452, -0.0490, -0.0718, -0.0193, -0.0061,\n",
       "                      -0.0623,  0.0067,  0.0112, -0.0616,  0.0002, -0.0232, -0.0453,  0.0438,\n",
       "                      -0.0642, -0.0277,  0.0230, -0.0351, -0.0443,  0.0093, -0.0409,  0.0191,\n",
       "                       0.0319, -0.0232,  0.0492, -0.0231,  0.0078, -0.0039, -0.0186,  0.0386,\n",
       "                       0.0195, -0.0284,  0.0273,  0.0323,  0.0024, -0.0493, -0.0524, -0.0335,\n",
       "                      -0.0273, -0.0532,  0.0267,  0.0069,  0.0181, -0.0636,  0.0055, -0.0193,\n",
       "                       0.0277,  0.0011,  0.0368, -0.0007,  0.0060,  0.0059,  0.0064, -0.0231,\n",
       "                      -0.0267, -0.0249,  0.0117, -0.0349,  0.0416, -0.0025,  0.0137, -0.0035,\n",
       "                      -0.0041, -0.0428,  0.0353,  0.0131,  0.0087, -0.0124,  0.0059,  0.0283,\n",
       "                      -0.0612,  0.0239,  0.0179,  0.0107,  0.0102,  0.0181,  0.0094,  0.0259,\n",
       "                      -0.0393,  0.0382, -0.0052,  0.0420, -0.0020, -0.0486, -0.0345, -0.0424,\n",
       "                       0.0024, -0.0560,  0.0400,  0.0057, -0.0556,  0.0243, -0.0183, -0.0325,\n",
       "                      -0.0495,  0.0488,  0.0240,  0.0243,  0.0174,  0.0286,  0.0159, -0.0456,\n",
       "                       0.0775, -0.0148,  0.0138,  0.0292,  0.0039, -0.0163,  0.0299, -0.0100,\n",
       "                       0.0749, -0.0488, -0.0056,  0.0238,  0.0060,  0.0120, -0.0412])),\n",
       "             ('linear3.weight',\n",
       "              tensor([[ 0.1659,  0.4207, -0.0723,  0.1485,  0.1768,  0.1228,  0.1697,  0.1643,\n",
       "                        0.1911,  0.2465,  0.1654,  0.1752, -0.1662,  0.2802,  0.1686,  0.1902,\n",
       "                       -0.0697, -0.0929, -0.0764,  0.2651, -0.0498,  0.1440, -0.0490,  0.0517,\n",
       "                        0.2108,  0.1801,  0.1701,  0.1790,  0.1938,  0.2001,  0.1823, -0.0895,\n",
       "                        0.1778,  0.1762,  0.1151,  0.1629,  0.1878, -0.0915,  0.1676,  0.1819,\n",
       "                        0.1876,  0.1844,  0.1468,  0.1705,  0.1832,  0.2991, -0.0671,  0.2047,\n",
       "                        0.3116,  0.1763,  0.1865,  0.2152,  0.1803,  0.1918,  0.1751,  0.1733,\n",
       "                        0.1802,  0.1476,  0.1857, -0.0661,  0.1681, -0.0706,  0.2169,  0.2045,\n",
       "                        0.1892,  0.1710,  0.1808,  0.2226,  0.1674,  0.1740,  0.2237,  0.1709,\n",
       "                       -0.0659,  0.1645,  0.1679,  0.1800, -0.0801, -0.0788,  0.1589,  0.1485,\n",
       "                        0.1957,  0.1487,  0.2049,  0.1854,  0.2007, -0.0776,  0.1780, -0.0816,\n",
       "                        0.1797,  0.1808,  0.1911,  0.1353,  0.2493,  0.1800,  0.1730,  0.2371,\n",
       "                       -0.0678,  0.1777,  0.1875,  0.1714,  0.2067,  0.2462,  0.1672,  0.1643,\n",
       "                        0.1753,  0.0615,  0.1566, -0.0823,  0.1724,  0.1716,  0.1895, -0.0603,\n",
       "                        0.2348,  0.1817,  0.1633,  0.1655, -0.0646, -0.0465,  0.1554,  0.1483,\n",
       "                        0.1100, -0.0740, -0.0917,  0.1508,  0.2362,  0.2279,  0.1881]])),\n",
       "             ('linear3.bias', tensor([0.1283]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2080095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = FeedForwardNetwork(h=[3, 504, 127, 1])\n",
    "new_model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646901a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80f117f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.20413115, 12.02658484, 22.13129693])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([df[\"D\"][0], df[\"S\"][0], df[\"tau\"][0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f5977",
   "metadata": {},
   "source": [
    "Check inference on model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93ec887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.866582\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(torch.from_numpy(x).float()).numpy()\n",
    "    print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cda5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_state_dict, \"new_finetuned_most_pruned.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1592a",
   "metadata": {},
   "source": [
    "# Use pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81069fa2",
   "metadata": {},
   "source": [
    "We use pickle to save the whole neural net object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc765b4",
   "metadata": {},
   "source": [
    "We redefine the architecture such that it can be used without nnc2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ee237b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Our network architecture\"\"\"\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a two-layered neural network for the C2P conversion. Note that hence the number of layers is fixed\n",
    "    for this NN subclass! The activation functions are sigmoids.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h1: int = 600, h2: int = 200) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network class.\n",
    "        :param name: String that names this network, in order to recognize it later on.\n",
    "        :param h1: Size (number of neurons) of the first hidden layer.\n",
    "        :param h2: Size (number of neurons) of the second hidden layer.\n",
    "        \"\"\"\n",
    "        # Call the super constructor first\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # For convenience, save the sizes of the hidden layers as fields as well\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "\n",
    "        # Define the weights:\n",
    "        self.linear1 = nn.Linear(3, h1)\n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear3 = nn.Linear(h2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward step given the input x.\n",
    "        :param x: Input for the neural network.\n",
    "        :return: x: Output neural network\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a9a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two auxiliary functions are needed to load the weights from the state dict into a fresh network\n",
    "def get_hidden_sizes_from_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Finds the sizes of the two hidden layers of our 2-layer architecture given a state dict.\n",
    "    :param state_dict: State dict of saved parameters\n",
    "    :return: h1, size of first hidden layer, and h2, size of second hidden layer\n",
    "    \"\"\"\n",
    "    h1 = np.shape(state_dict['linear1.bias'])[0]\n",
    "    h2 = np.shape(state_dict['linear2.bias'])[0]\n",
    "\n",
    "    return h1, h2\n",
    "\n",
    "def create_nn(state_dict):\n",
    "    \"\"\"\n",
    "    Create a NeuralNetwork object if given a dictionary of the weights, with correct sizes for hidden layers.\n",
    "    :param state_dict: State dictionary containing the weights of the neural network\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    h1, h2 = get_hidden_sizes_from_state_dict(state_dict)\n",
    "    model = NeuralNetwork(h1=h1, h2=h2)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b131aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear1): Linear(in_features=3, out_features=504, bias=True)\n",
       "  (linear2): Linear(in_features=504, out_features=127, bias=True)\n",
       "  (linear3): Linear(in_features=127, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"finetuned_most_pruned.pth\")\n",
    "model = create_nn(state_dict)\n",
    "model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce43033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file and use dump()\n",
    "filename = \"model.pkl\"\n",
    "with open(filename, 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229a194",
   "metadata": {},
   "source": [
    "Check: to load again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fea982a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear1): Linear(in_features=3, out_features=504, bias=True)\n",
      "  (linear2): Linear(in_features=504, out_features=127, bias=True)\n",
      "  (linear3): Linear(in_features=127, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Open the file in binary mode\n",
    "with open(filename, 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    test = pickle.load(file)\n",
    "  \n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcceeebc",
   "metadata": {},
   "source": [
    "# (Archive) Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d04d5a",
   "metadata": {},
   "source": [
    "This is the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ee3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters of the model here. Will first of all put two hidden layers\n",
    "# total of 800 neurons for the one in the paper\n",
    "device = \"cpu\"\n",
    "size_HL_1 = 600\n",
    "size_HL_2 = 200\n",
    "\n",
    "# Implement neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(3, size_HL_1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(size_HL_1, size_HL_2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(size_HL_2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # No flatten needed, as our input and output are 1D?\n",
    "        #x = self.flatten(x) \n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071780a",
   "metadata": {},
   "source": [
    "We import NNC2Pv0, which was on par with the models in the paper. The t2 version is trained a bit longer than the version of the paper. Use OS to locate the Models folder correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9341324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models folder: D:\\Coding\\master-thesis-AI\\Models\n"
     ]
    }
   ],
   "source": [
    "# # Directory of current file:\n",
    "# dir_path = os.path.abspath(\"D:\\Coding\\master-thesis-AI\\Code\\Semester 1\")\n",
    "# # Models folder\n",
    "# models_folder =os.path.abspath(\"D:\\Coding\\master-thesis-AI\\Models\")\n",
    "# print(\"Models folder: \" + models_folder)\n",
    "# # Move to the models folder\n",
    "# os.chdir(models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e1c29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to the correct folder\n",
    "file_location = os.chdir(\"D:\\Coding\\master-thesis-AI\\Models\")\n",
    "NNC2P = torch.load(\"NNC2Pv0t2.pth\")\n",
    "model = NNC2P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c9074",
   "metadata": {},
   "source": [
    "In case we want to view the variables, uncomment the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05d1fee6-36b8-4ef9-a84d-cec0dcc9456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNC2P.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fcb86b-ad79-4733-8911-d26de50082b3",
   "metadata": {},
   "source": [
    "# Save the matrices as a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad316e9b",
   "metadata": {},
   "source": [
    "Note that converting from Torch tensor to Numpy array does __not__ cause loss of information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b75e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3647063672542572021484375\n",
      "---\n",
      "-0.3647063672542572021484375\n"
     ]
    }
   ],
   "source": [
    "test_exact = NNC2P.state_dict()[\"stack.0.weight\"]\n",
    "test_exact_value = test_exact[0][0].item()\n",
    "print('%.25f' % test_exact_value)\n",
    "print(\"---\")\n",
    "test_exact_np = test_exact.numpy()\n",
    "test_exact_value_np = test_exact_np[0][0]\n",
    "print('%.25f' % test_exact_value_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80676c3",
   "metadata": {},
   "source": [
    " ## Saving and loading as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9ed1f",
   "metadata": {},
   "source": [
    "Save the values: ([refresher on Pickle](https://tech.qvread.com/python/python-list-read-write-csv/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f210e996-8996-4aef-a7e4-e759f410fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State dict contains all the variables\n",
    "state_dict = NNC2P.state_dict().items()\n",
    "# Names to save the files:\n",
    "file_names            = [\"weight0\", \"bias0\", \"weight2\", \"bias2\", \"weight4\", \"bias4\"]\n",
    "save_names         = [\"Models/paramvals/\" + name + \".csv\" for name in file_names]\n",
    "flat_save_names = [\"Models/paramvals/\" + name + \"_flat.csv\" for name in file_names]\n",
    "no_comma_flat_save_names = [\"Models/paramvals/\" + name + \"_flat_no_comma.csv\" for name in file_names]\n",
    "\n",
    "# Save each one:\n",
    "counter = 0\n",
    "for param_name, item in state_dict:\n",
    "    # Get appropriate names\n",
    "    name                   = file_names[counter]\n",
    "    save_name         = save_names[counter]\n",
    "    flat_save_name = flat_save_names[counter]\n",
    "    no_comma_flat_save_name = no_comma_flat_save_names[counter]\n",
    "    # Get the matrix and flatten it as well\n",
    "    matrix_np   = item.numpy() \n",
    "    flat_matrix_np   = matrix_np.flatten()\n",
    "    # The following save txt is only important for stuff done within this noteboo!\n",
    "    np.savetxt(no_comma_flat_save_name, flat_matrix_np, delimiter=\",\", fmt=\"%0.35f\")\n",
    "    \n",
    "    np.savetxt(save_name, matrix_np, delimiter=\",\", fmt=\"%0.35f\")\n",
    "    # Note: due to weird Fortran stuff, have to append a 0 at the start of the file\n",
    "    flat_matrix_np   = np.insert(flat_matrix_np, 0, 0)\n",
    "    np.savetxt(flat_save_name, flat_matrix_np, delimiter=\",\", newline=',\\n', fmt=\"%0.35f\")\n",
    "    \n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88386f6f-a165-4d9a-9c2d-4c14fccbfe72",
   "metadata": {},
   "source": [
    "Read the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9180ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight0 = np.loadtxt('Models/paramvals/weight0.csv', delimiter=\",\")\n",
    "bias0      = np.loadtxt('Models/paramvals/bias0.csv', delimiter=\",\")\n",
    "s = np.shape(bias0)[0]\n",
    "bias0 = np.reshape(bias0, (s, 1))\n",
    "weight2 = np.loadtxt('Models/paramvals/weight2.csv', delimiter=\",\")\n",
    "bias2      = np.loadtxt('Models/paramvals/bias2.csv', delimiter=\",\")\n",
    "s = np.shape(bias2)[0]\n",
    "bias2 = np.reshape(bias2, (s, 1))\n",
    "weight4 = np.loadtxt('Models/paramvals/weight4.csv', delimiter=\",\")\n",
    "s = np.shape(weight4)[0]\n",
    "weight4 = np.reshape(weight4, (1, s))\n",
    "bias4      = np.loadtxt('Models/paramvals/bias4.csv', delimiter=\",\")\n",
    "bias4 = np.reshape(bias4, (1, 1))\n",
    "\n",
    "weights_and_biases = [weight0, bias0, weight2, bias2, weight4, bias4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e1425",
   "metadata": {},
   "source": [
    "Same for flat: __NOTE__ for numpy (here), we load \"no_comma\" files since otherwise there's an error. For Fortran, we use the files __WITHOUT__ \"no comma\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a79742bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight0_flat = np.loadtxt('Models/paramvals/weight0_flat_no_comma.csv', delimiter=\",\")\n",
    "# bias0_flat      = np.loadtxt('Models/paramvals/bias0_flat_no_comma.csv', delimiter=\",\")\n",
    "# weight2_flat = np.loadtxt('Models/paramvals/weight2_flat_no_comma.csv', delimiter=\",\")\n",
    "# bias2_flat      = np.loadtxt('Models/paramvals/bias2_flat_no_comma.csv', delimiter=\",\")\n",
    "# weight4_flat = np.loadtxt('Models/paramvals/weight4_flat_no_comma.csv', delimiter=\",\")\n",
    "# bias4_flat      = np.loadtxt('Models/paramvals/bias4_flat_no_comma.csv', delimiter=\",\")\n",
    "\n",
    "# weights_and_biases_flat = [weight0_flat, bias0_flat, weight2_flat, bias2_flat, weight4_flat, bias4_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2157cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3647063672542572021484375\n"
     ]
    }
   ],
   "source": [
    "print('%.25f' % weight0[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e7a63e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weight0_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad63b8",
   "metadata": {},
   "source": [
    "(Below: old pickle version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "60288975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   reload pickled data from file\n",
    "# test_name = save_names[0]\n",
    "# with open(test_name, 'rb') as f:\n",
    "#     test_load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "788d1c48-f316-4c85-b069-e7d62cd6c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the loaded versions in the appropriate variables\n",
    "# with open('Models/paramvals/weight0.csv', 'rb') as f:\n",
    "#     weight0 = pickle.load(f)\n",
    "    \n",
    "# with open('Models/paramvals/bias0.csv', 'rb') as f:\n",
    "#     bias0 = pickle.load(f)\n",
    "#     s = np.shape(bias0)[0]\n",
    "#     bias0 = np.reshape(bias0, (s, 1))\n",
    "    \n",
    "# with open('Models/paramvals/weight2.csv', 'rb') as f:\n",
    "#     weight2 = pickle.load(f)\n",
    "    \n",
    "# with open('Models/paramvals/bias2.csv', 'rb') as f:\n",
    "#     bias2 = pickle.load(f)\n",
    "#     s = np.shape(bias2)[0]\n",
    "#     bias2 = np.reshape(bias2, (s, 1))\n",
    "    \n",
    "# with open('Models/paramvals/weight4.csv', 'rb') as f:\n",
    "#     weight4 = pickle.load(f)\n",
    "    \n",
    "# with open('Models/paramvals/bias4.csv', 'rb') as f:\n",
    "#     bias4 = pickle.load(f)\n",
    "#     s = np.shape(bias4)[0]\n",
    "#     bias4 = np.reshape(bias4, (s, 1))\n",
    "\n",
    "# # Gather together in a list of all variables\n",
    "# weights_and_biases = [weight0, bias0, weight2, bias2, weight4, bias4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b9a2b",
   "metadata": {},
   "source": [
    "Same for flattened arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4abc8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Models/paramvals/bias0_flat.csv', 'r') as file:\n",
    "#     csvreader = csv.reader(file)\n",
    "# #     for row in csvreader:\n",
    "# #         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5b4c157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the loaded versions in the appropriate variables\n",
    "# with open('Models/paramvals/weight0_flat.csv', 'rb') as f:\n",
    "#     weight0_flat = pickle.load(f)\n",
    "    \n",
    "# # with open('Models/paramvals/bias0_flat.csv', 'rb') as f:\n",
    "# #     bias0_flat = pickle.load(f)\n",
    "\n",
    "    \n",
    "# with open('Models/paramvals/weight2_flat.csv', 'rb') as f:\n",
    "#     weight2_flat = pickle.load(f)\n",
    "    \n",
    "# with open('Models/paramvals/bias2_flat.csv', 'rb') as f:\n",
    "#     bias2_flat = pickle.load(f)\n",
    "    \n",
    "# with open('Models/paramvals/weight4_flat.csv', 'rb') as f:\n",
    "#     weight4_flat = pickle.load(f)\n",
    "    \n",
    "# with open('Models/paramvals/bias4_flat.csv', 'rb') as f:\n",
    "#     bias4_flat = pickle.load(f)\n",
    "\n",
    "# # Gather together in a list of all variables\n",
    "# weights_and_biases_flat = [weight0_flat, bias0_flat, weight2_flat, bias2_flat, weight4_flat, bias4_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "85e7dfa7-7137-4a5f-9c90-b79ab3516a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the file:  weight0\n",
      "The shape is equal to  (600, 3)\n",
      "For the file:  bias0\n",
      "The shape is equal to  (600, 1)\n",
      "For the file:  weight2\n",
      "The shape is equal to  (200, 600)\n",
      "For the file:  bias2\n",
      "The shape is equal to  (200, 1)\n",
      "For the file:  weight4\n",
      "The shape is equal to  (1, 200)\n",
      "For the file:  bias4\n",
      "The shape is equal to  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape for each parameter:\n",
    "for i in range(len(weights_and_biases)):\n",
    "    print(\"For the file: \", file_names[i])\n",
    "    # Read the values\n",
    "    shape = np.shape(weights_and_biases[i])\n",
    "    print(\"The shape is equal to \", shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b07bec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the file:  Models/paramvals/weight0_flat.csv\n",
      "The shape is equal to  (1800,)\n",
      "For the file:  Models/paramvals/bias0_flat.csv\n",
      "The shape is equal to  (600,)\n",
      "For the file:  Models/paramvals/weight2_flat.csv\n",
      "The shape is equal to  (120000,)\n",
      "For the file:  Models/paramvals/bias2_flat.csv\n",
      "The shape is equal to  (200,)\n",
      "For the file:  Models/paramvals/weight4_flat.csv\n",
      "The shape is equal to  (200,)\n",
      "For the file:  Models/paramvals/bias4_flat.csv\n",
      "The shape is equal to  ()\n"
     ]
    }
   ],
   "source": [
    "# Same for their flattened versions:\n",
    "for i in range(len(weights_and_biases_flat)):\n",
    "    print(\"For the file: \", flat_save_names[i])\n",
    "    # Read the values\n",
    "    shape = np.shape(weights_and_biases_flat[i])\n",
    "    print(\"The shape is equal to \", shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd58f4",
   "metadata": {},
   "source": [
    "##### Play around with some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8c6a77e-6221-4158-8dc5-2de8cdddbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the example file\n",
    "# print(example)\n",
    "# print(np.shape(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cf45633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_load_value = test_load[0][0]\n",
    "# print('%.25f' % test_load_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a77502-f96b-4221-9878-4e50854bcfe2",
   "metadata": {},
   "source": [
    "## Predicting using the values in the arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9709a2-c540-4130-ac6b-3ada2552bc0b",
   "metadata": {},
   "source": [
    "When we are going to implement this in the Gmunu code, we can no longer use any of the built-in tools of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1e65ebe8-1c4f-4c4f-aced-77fadcd57267",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One specific test case for the data\n",
    "rho,eps,v,p,D,S,tau = 9.83632270803203,1.962038705851822,0.2660655147967911,12.866163917605371,10.204131145455385,12.026584842282125,22.131296926293793"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acabd1c-7014-4bae-9615-389bd9c251d5",
   "metadata": {},
   "source": [
    "This is how the PyTorch implementation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ffbcdea-ab47-41ce-a968-d53be5534c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact:\n",
      "12.866163917605371\n",
      "Pytorch prediction:\n",
      "12.866371154785156\n"
     ]
    }
   ],
   "source": [
    "input_test = torch.tensor([D, S, tau])\n",
    "exact_result = p\n",
    "print(\"Exact:\")\n",
    "print(exact_result)\n",
    "# print(input_test)\n",
    "with torch.no_grad():\n",
    "    pred = model(input_test).item()\n",
    "\n",
    "print(\"Pytorch prediction:\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e00da-5bd8-419c-806b-4b6ecf3f2dbf",
   "metadata": {},
   "source": [
    "Now, we have to try and get the same output, but by defining all intermediate steps ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "194e0737-14b7-49b7-b3af-89221678facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def compute_prediction(x):\n",
    "    \"\"\"Input is a np. array of size 1x3\"\"\"\n",
    "    x = np.matmul(weight0, x) + bias0\n",
    "    x = sigmoid(x)\n",
    "    x = np.matmul(weight2, x) + bias2\n",
    "    x = sigmoid(x)\n",
    "    x = np.matmul(weight4, x) + bias4\n",
    "    return x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5745fb74-6f68-4556-a8e0-c80628e93b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "input_test = np.array([[D, S, tau]])\n",
    "print(np.shape(input_test))\n",
    "input_test = np.transpose(input_test)\n",
    "print(np.shape(input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c422a5de-4b83-4f5f-b1e0-c6023c8cd65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.866371869133928\n",
      "12.866371154785156\n"
     ]
    }
   ],
   "source": [
    "our_prediction = compute_prediction(input_test)\n",
    "print(our_prediction)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbe512-19ff-4157-9a33-68899241991d",
   "metadata": {},
   "source": [
    "Now we compute rho and eps from this (see appendix A of central paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aee97e55-2be0-4bc1-8c31-62ad7b4db25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our calculations:\n",
      "9.836326155512264 1.9620391642983483\n",
      "Exact results:\n",
      "9.83632270803203 1.962038705851822\n"
     ]
    }
   ],
   "source": [
    "v_star = S/(tau + D + our_prediction)\n",
    "W_star = 1/np.sqrt(1-v_star**2)\n",
    "\n",
    "rho_star = D/W_star\n",
    "eps_star = (tau + D*(1 - W_star) + our_prediction*(1 - W_star**2))/(D*W_star)\n",
    "print(\"Our calculations:\")\n",
    "print(rho_star, eps_star)\n",
    "print(\"Exact results:\")\n",
    "print(rho, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff9646-60ef-402e-a56c-3eccee7478e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (to do) Save as hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba57a7f2-8af2-4ba6-a242-d19fbfbe034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open an HDF5 file for writing\n",
    "# with h5py.File(\"NNC2Pv0_params.h5\", \"w\") as f:\n",
    "#     # Save the weights and biases of the network to the HDF5 file\n",
    "#     f.create_dataset(\"NNC2Pv0_params\", data=NNC2P.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bc14b",
   "metadata": {},
   "source": [
    "# Using Torch script and tracing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983a5e9",
   "metadata": {},
   "source": [
    "There exist two ways of converting a PyTorch model to Torch Script. The first is known as tracing, a mechanism in which the structure of the model is captured by evaluating it once using example inputs, and recording the flow of those inputs through the model. This is suitable for models that make limited use of control flow. The second approach is to add explicit annotations to your model that inform the Torch Script compiler that it may directly parse and compile your model code, subject to the constraints imposed by the Torch Script language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa4c899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 0.5000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.tensor([1, 1, 0.5])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71837a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  original_name=NeuralNetwork\n",
       "  (stack): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): Linear(original_name=Linear)\n",
       "    (1): Sigmoid(original_name=Sigmoid)\n",
       "    (2): Linear(original_name=Linear)\n",
       "    (3): Sigmoid(original_name=Sigmoid)\n",
       "    (4): Linear(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_script_module = torch.jit.trace(model, example)\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28fafa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0598], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = traced_script_module(torch.tensor([1,1,0.5]))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8866ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module.save(\"NNC2Pv0t2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7817ec-e463-4ed5-baf0-1b887b960d0b",
   "metadata": {},
   "source": [
    "__To do: finish it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cc2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "Thibeau Wouters",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
