{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af563537",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c67ea",
   "metadata": {},
   "source": [
    "# First notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bb9f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn # pytorch neural networks\n",
    "from torch.utils.data import Dataset, DataLoader # pytorch dataset structures\n",
    "from torchvision.transforms import ToTensor # pytorch transformer\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb3c8c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efeafb",
   "metadata": {},
   "source": [
    "The conserved variables are $(D, S_i, \\tau)$ and they are related to primitive variables, $w = (\\rho, v^i, \\epsilon, p)$, defined in the local rest frame of the fluid through (in units of light speed $c = 1$). The P2C is explicitly given:\n",
    "\\begin{equation}\n",
    "D = \\rho W \\, , \\quad S_i = \\rho h W^2 v_i \\, , \\quad \\tau = \\rho h W^2 - p - D \\, ,\n",
    "\\end{equation}\n",
    "where we used\n",
    "\\begin{equation}\n",
    "W = (1 - v^2)^{-1/2} \\, , \\quad h = 1 + \\epsilon + \\frac{p}{\\rho} \\, .\n",
    "\\end{equation}\n",
    "\n",
    "Our first goal is to reproduce the results from [this paper](https://www.mdpi.com/2073-8994/13/11/2157). We first focus on what they call __NNEOS__ networks. These are networks which are trained to infer information on the equation of state (EOS). In its simplest form, the EOS is the thermodynamical relation connecting the pressure to the fluid's rest-mass density and internal energy $p = \\bar{p}(\\rho, \\epsilon)$. We consider an __analytical $\\Gamma$-law EOS__ as a benchmark:\n",
    "\\begin{equation}\n",
    "    \\bar{p}(\\rho, \\varepsilon) = (\\Gamma - 1)\\rho\\epsilon \\, ,\n",
    "\\end{equation}\n",
    "and we fix $\\Gamma = 5/3$ in order to fully mimic the situation of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca72213",
   "metadata": {},
   "source": [
    "## Generating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db09163",
   "metadata": {},
   "source": [
    "We generate training data for the NNEOS networks as follows. We create a training set by randomly sampling the EOS on a uniform distribution over $\\rho \\in (0, 10.1)$ and $\\epsilon \\in (0, 2.02)$. We then compute three quantities:\n",
    "\\begin{itemize}\n",
    "\\item $p$, using the EOS defined above\n",
    "\\item $\\chi := \\partial p/\\partial\\rho$, inferred from the EOS\n",
    "\\item $\\kappa := \\partial p/\\partial \\epsilon$, inferred from the EOS\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebd9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three functions determining the output\n",
    "def eos(rho, eps, Gamma = 5/3):\n",
    "    \"\"\"Computes the analytical gamma law EOS from rho and epsilon\"\"\"\n",
    "    return (Gamma - 1) * rho * eps\n",
    "\n",
    "def chi(rho, eps, Gamma = 5/3):\n",
    "    \"\"\"Computes dp/drho from EOS\"\"\"\n",
    "    return (Gamma - 1) * eps\n",
    "\n",
    "def kappa(rho, eps, Gamma = 5/3):\n",
    "    \"\"\"Computes dp/deps from EOS\"\"\"\n",
    "    return (Gamma - 1) * rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40409d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges of parameters to be sampled (see paper Section 2.1)\n",
    "rho_min = 0\n",
    "rho_max = 10.1\n",
    "eps_min = 0\n",
    "eps_max = 2.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1751eb",
   "metadata": {},
   "source": [
    "Note: the code is in comment, as the data has been generated already and we want to use the same dataset for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4df2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_datapoints = 10000 # 80 000 for train, 10 000 for test\n",
    "# data = []\n",
    "\n",
    "# for i in range(number_of_datapoints):\n",
    "#     rho = random.uniform(rho_min, rho_max)\n",
    "#     eps = random.uniform(eps_min, eps_max)\n",
    "    \n",
    "#     new_row = [rho, eps, eos(rho, eps), chi(rho, eps), kappa(rho, eps)]\n",
    "    \n",
    "#     data.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663bea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = ['rho', 'eps', 'p', 'chi', 'kappa']\n",
    "\n",
    "# with open('NNEOS_data_test.csv', 'w', newline = '') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     # write header\n",
    "#     writer.writerow(header)\n",
    "#     # write data\n",
    "#     writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ea690aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has 80000 instances\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>eps</th>\n",
       "      <th>p</th>\n",
       "      <th>chi</th>\n",
       "      <th>kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.770794</td>\n",
       "      <td>0.809768</td>\n",
       "      <td>5.274717</td>\n",
       "      <td>0.539845</td>\n",
       "      <td>6.513863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.093352</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>3.871421</td>\n",
       "      <td>0.383561</td>\n",
       "      <td>6.728901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.685186</td>\n",
       "      <td>1.647820</td>\n",
       "      <td>1.851255</td>\n",
       "      <td>1.098547</td>\n",
       "      <td>1.123457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.167718</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.317913</td>\n",
       "      <td>0.272251</td>\n",
       "      <td>0.778479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.750848</td>\n",
       "      <td>1.069954</td>\n",
       "      <td>5.528700</td>\n",
       "      <td>0.713303</td>\n",
       "      <td>5.167232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>3.985951</td>\n",
       "      <td>1.642317</td>\n",
       "      <td>4.364131</td>\n",
       "      <td>1.094878</td>\n",
       "      <td>2.657301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>6.948815</td>\n",
       "      <td>0.809021</td>\n",
       "      <td>3.747824</td>\n",
       "      <td>0.539347</td>\n",
       "      <td>4.632543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>8.423227</td>\n",
       "      <td>1.125142</td>\n",
       "      <td>6.318217</td>\n",
       "      <td>0.750095</td>\n",
       "      <td>5.615485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>4.748173</td>\n",
       "      <td>0.774870</td>\n",
       "      <td>2.452810</td>\n",
       "      <td>0.516580</td>\n",
       "      <td>3.165449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>2.927483</td>\n",
       "      <td>0.616751</td>\n",
       "      <td>1.203686</td>\n",
       "      <td>0.411167</td>\n",
       "      <td>1.951655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rho       eps         p       chi     kappa\n",
       "0       9.770794  0.809768  5.274717  0.539845  6.513863\n",
       "1      10.093352  0.575342  3.871421  0.383561  6.728901\n",
       "2       1.685186  1.647820  1.851255  1.098547  1.123457\n",
       "3       1.167718  0.408377  0.317913  0.272251  0.778479\n",
       "4       7.750848  1.069954  5.528700  0.713303  5.167232\n",
       "...          ...       ...       ...       ...       ...\n",
       "79995   3.985951  1.642317  4.364131  1.094878  2.657301\n",
       "79996   6.948815  0.809021  3.747824  0.539347  4.632543\n",
       "79997   8.423227  1.125142  6.318217  0.750095  5.615485\n",
       "79998   4.748173  0.774870  2.452810  0.516580  3.165449\n",
       "79999   2.927483  0.616751  1.203686  0.411167  1.951655\n",
       "\n",
       "[80000 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"data/NNEOS_data_train.csv\")\n",
    "print(\"The training data has \" + str(len(data_train)) + \" instances\")\n",
    "data_test = pd.read_csv(\"data/NNEOS_data_test.csv\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f30d9",
   "metadata": {},
   "source": [
    "In case we want to visualize the datapoints (not recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55cf2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho = data_train['rho']\n",
    "# eps = data_train['eps']\n",
    "\n",
    "# plt.figure(figsize = (12,10))\n",
    "# plt.plot(rho, eps, 'o', color = 'black', alpha = 0.005)\n",
    "# plt.grid()\n",
    "# plt.xlabel(r'$\\rho$')\n",
    "# plt.ylabel(r'$\\epsilon$')\n",
    "# plt.title('Training data')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b37904",
   "metadata": {},
   "source": [
    "## Getting data into PyTorch's DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "efc67934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"See PyTorch tutorial: the following three methods HAVE to be implemented\"\"\"\n",
    "    \n",
    "    def __init__(self, all_data, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Separate features (rho and eps) from the labels (p, chi, kappa)\n",
    "        # see above to get how data is organized\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(len(all_data)):\n",
    "            new_feature = np.array([all_data['rho'][i], all_data['eps'][i]])\n",
    "            features.append(torch.from_numpy(new_feature))\n",
    "            new_label = np.array([all_data['p'][i], all_data['chi'][i], all_data['kappa'][i]])\n",
    "            labels.append(torch.from_numpy(new_label))\n",
    "            \n",
    "            \n",
    "        # Save as instance variables to the dataloader\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "        ### TODO: I don't understand transform and target_transform\n",
    "        #self.features = torch.from_numpy(np.array(features))\n",
    "        #self.labels = torch.from_numpy(np.array(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        if self.transform:\n",
    "            feature = transform(feature)\n",
    "        label = self.labels[idx]\n",
    "        if self.target_transform:\n",
    "            feature = target_transform(label)\n",
    "            \n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cf73d",
   "metadata": {},
   "source": [
    "Note that this may be confusing. \"data_train\" refers to the data that was generatd above, see the pandas table. \"training_data\" is defined similarly as in the PyTorch tutorial, see [this page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) and is an instance of CustomDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bca918ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test data, as in the tutorial\n",
    "training_data = CustomDataset(data_train)\n",
    "test_data = CustomDataset(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "237da204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([9.7708, 0.8098], dtype=torch.float64), tensor([10.0934,  0.5753], dtype=torch.float64)]\n",
      "[tensor([5.2747, 0.5398, 6.5139], dtype=torch.float64), tensor([3.8714, 0.3836, 6.7289], dtype=torch.float64)]\n",
      "80000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Check if this is done correctly\n",
    "print(training_data.features[:2])\n",
    "print(training_data.labels[:2])\n",
    "print(training_data.__len__())\n",
    "print(test_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "50aecee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call DataLoader on the above CustomDataset instances:\n",
    "train_dataloader = DataLoader(training_data, batch_size=32)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411ed07",
   "metadata": {},
   "source": [
    "## Building the neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e7e9e",
   "metadata": {},
   "source": [
    "We will follow [this part of the PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html). For more information, see the [documentation page of torch.nn](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1115de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters of the model here. Will first of all put two hidden layers\n",
    "device = \"cpu\"\n",
    "size_HL_1 = 600\n",
    "size_HL_2 = 300\n",
    "\n",
    "# Implement neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, size_HL_1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(size_HL_1, size_HL_2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(size_HL_2, 3) ### Q: Does this have to become ReLU? Gives an error!\n",
    "            ###nn.ReLU() # ???\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x) ### no flatten needed, as our input and output are 1D?\n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d4c8763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=600, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=300, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f18c9",
   "metadata": {},
   "source": [
    "## Optimizing the neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335e8f0",
   "metadata": {},
   "source": [
    "Save hyperparameters and loss function - note that we follow the paper. I think that their loss function agrees with [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss). The paper uses the [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam). More details on optimizers can be found [here](https://pytorch.org/docs/stable/optim.html). Required argument `params` can be filled in by calling `model` which contains the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c7cf5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameters here --- see paper!!!\n",
    "learning_rate = 6e-4\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac8b43",
   "metadata": {},
   "source": [
    "The train and test loops are implemented below (copy pasted from [this part of the tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "af2b7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c61571",
   "metadata": {},
   "source": [
    "Now train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2c6425cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [107]\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#x = self.flatten(x) ### no flatten needed, as our input and output are 1D?\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1aa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
